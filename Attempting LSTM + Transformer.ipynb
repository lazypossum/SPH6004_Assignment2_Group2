{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "536a6ccd-e607-4f3d-b9c6-9a91d76a070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import subprocess\n",
    "import shlex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d36930c-3f24-4319-b444-efd046fec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('Timestep5_train.xlsx')\n",
    "test_df = pd.read_excel('Timestep5_test.xlsx')\n",
    "holdout_df = pd.read_excel('Timestep5_holdout.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5621f4aa-a661-4b4a-ad42-6650867204a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71445, 70)\n",
      "(20415, 70)\n",
      "(10210, 70)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(holdout_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a508378b-71a8-415c-aea2-2063a3769fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_step(df, id_list, steps):\n",
    "    result_df = pd.DataFrame()\n",
    "    for id in id_list:\n",
    "        id_df = df[df['id'] == id]\n",
    "        if id_df.shape[0] >= steps:\n",
    "            id_df = id_df.sort_values(by='hour', ascending=False)\n",
    "            counter = 0\n",
    "            for index, row in id_df.iterrows():\n",
    "                if counter < steps:\n",
    "                    row_dict = row.to_dict()\n",
    "                    result_df = result_df.append(row_dict, ignore_index=True)\n",
    "                    counter += 1\n",
    "        else:\n",
    "            id_df = id_df.sort_values(by='hour', ascending=False)\n",
    "            n = steps - id_df.shape[0]\n",
    "            for index, row in id_df.iterrows():\n",
    "                row_dict = row.to_dict()\n",
    "                result_df = result_df.append(row_dict, ignore_index=True)\n",
    "            for i in range(n):\n",
    "                result_df = result_df.append(row_dict, ignore_index=True)\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba14b943-f842-4b98-94f6-c3e777f7821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_df = get_time_step(merged_holdout_df, holdout_id_list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "730c0aa3-e726-4bbb-8263-b121b12e3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting los_icu into 3 categorical bins\n",
    "\n",
    "# low_thres = 2.5\n",
    "# medium_thres = 5\n",
    "\n",
    "# result_df['icu_cat'] = 10\n",
    "# result_df.loc[result_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# result_df.loc[((result_df['los_icu'] >= low_thres) & (result_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# result_df.loc[result_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "# test_df['icu_cat'] = 10\n",
    "# test_df.loc[test_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# test_df.loc[((test_df['los_icu'] >= low_thres) & (test_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# test_df.loc[test_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "# holdout_df['icu_cat'] = 10\n",
    "# holdout_df.loc[holdout_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# holdout_df.loc[((holdout_df['los_icu'] >= low_thres) & (holdout_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# holdout_df.loc[holdout_df['los_icu'] >= medium_thres, 'icu_cat'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5826b276-4a05-4947-a3e3-83bb7b15682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_step(df, id_list, steps):\n",
    "    result_df = pd.DataFrame()\n",
    "    for id in id_list:\n",
    "        id_df = df[df['id'] == id]\n",
    "        if id_df.shape[0] >= steps:\n",
    "            id_df = id_df.sort_values(by='hour', ascending=False)\n",
    "            counter = 0\n",
    "            for index, row in id_df.iterrows():\n",
    "                if counter < steps:\n",
    "                    row_dict = row.to_dict()\n",
    "                    result_df = result_df.append(row_dict, ignore_index=True)\n",
    "                    counter += 1\n",
    "        else:\n",
    "            id_df = id_df.sort_values(by='hour', ascending=False)\n",
    "            n = steps - id_df.shape[0]\n",
    "            for index, row in id_df.iterrows():\n",
    "                row_dict = row.to_dict()\n",
    "                result_df = result_df.append(row_dict, ignore_index=True)\n",
    "            for i in range(n):\n",
    "                result_df = result_df.append(row_dict, ignore_index=True)\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ab0f44e-bb6e-49ea-b3b2-bc4062977e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout_df = get_time_step(merged_holdout_df, holdout_id_list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa77b4ce-70db-439d-a0dc-18b40928dc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>...</th>\n",
       "      <th>first_careunit_Coronary Care Unit (CCU)</th>\n",
       "      <th>first_careunit_Medical Intensive Care Unit (MICU)</th>\n",
       "      <th>first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)</th>\n",
       "      <th>first_careunit_Neuro Intermediate</th>\n",
       "      <th>first_careunit_Neuro Stepdown</th>\n",
       "      <th>first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)</th>\n",
       "      <th>first_careunit_Surgical Intensive Care Unit (SICU)</th>\n",
       "      <th>first_careunit_Trauma SICU (TSICU)</th>\n",
       "      <th>hour</th>\n",
       "      <th>icu_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 08:20:00</td>\n",
       "      <td>15</td>\n",
       "      <td>23.0</td>\n",
       "      <td>47</td>\n",
       "      <td>11.4</td>\n",
       "      <td>108</td>\n",
       "      <td>0.8</td>\n",
       "      <td>154</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 08:20:00</td>\n",
       "      <td>15</td>\n",
       "      <td>23.0</td>\n",
       "      <td>47</td>\n",
       "      <td>11.4</td>\n",
       "      <td>108</td>\n",
       "      <td>0.8</td>\n",
       "      <td>154</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 08:20:00</td>\n",
       "      <td>15</td>\n",
       "      <td>23.0</td>\n",
       "      <td>47</td>\n",
       "      <td>11.4</td>\n",
       "      <td>108</td>\n",
       "      <td>0.8</td>\n",
       "      <td>154</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 13:45:00</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48</td>\n",
       "      <td>10.8</td>\n",
       "      <td>107</td>\n",
       "      <td>0.9</td>\n",
       "      <td>149</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 21:55:00</td>\n",
       "      <td>13</td>\n",
       "      <td>24.0</td>\n",
       "      <td>50</td>\n",
       "      <td>10.8</td>\n",
       "      <td>108</td>\n",
       "      <td>0.9</td>\n",
       "      <td>131</td>\n",
       "      <td>141</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.933333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            charttime  aniongap  bicarbonate  bun  calcium  \\\n",
       "0  20001305  1978-03-25 08:20:00        15         23.0   47     11.4   \n",
       "1  20001305  1978-03-25 08:20:00        15         23.0   47     11.4   \n",
       "2  20001305  1978-03-25 08:20:00        15         23.0   47     11.4   \n",
       "3  20001305  1978-03-25 13:45:00        13         25.0   48     10.8   \n",
       "4  20001305  1978-03-25 21:55:00        13         24.0   50     10.8   \n",
       "\n",
       "   chloride  creatinine  glucose  sodium  ...  \\\n",
       "0       108         0.8      154     142  ...   \n",
       "1       108         0.8      154     142  ...   \n",
       "2       108         0.8      154     142  ...   \n",
       "3       107         0.9      149     140  ...   \n",
       "4       108         0.9      131     141  ...   \n",
       "\n",
       "   first_careunit_Coronary Care Unit (CCU)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   first_careunit_Medical Intensive Care Unit (MICU)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "   first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)  \\\n",
       "0                                                  1                 \n",
       "1                                                  1                 \n",
       "2                                                  1                 \n",
       "3                                                  1                 \n",
       "4                                                  1                 \n",
       "\n",
       "   first_careunit_Neuro Intermediate  first_careunit_Neuro Stepdown  \\\n",
       "0                                  0                              0   \n",
       "1                                  0                              0   \n",
       "2                                  0                              0   \n",
       "3                                  0                              0   \n",
       "4                                  0                              0   \n",
       "\n",
       "   first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)  \\\n",
       "0                                                  0                \n",
       "1                                                  0                \n",
       "2                                                  0                \n",
       "3                                                  0                \n",
       "4                                                  0                \n",
       "\n",
       "   first_careunit_Surgical Intensive Care Unit (SICU)  \\\n",
       "0                                                  0    \n",
       "1                                                  0    \n",
       "2                                                  0    \n",
       "3                                                  0    \n",
       "4                                                  0    \n",
       "\n",
       "   first_careunit_Trauma SICU (TSICU)       hour  icu_cat  \n",
       "0                                   0   5.350000        1  \n",
       "1                                   0   5.350000        1  \n",
       "2                                   0   5.350000        1  \n",
       "3                                   0  10.766667        1  \n",
       "4                                   0  18.933333        1  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a545188-bcf0-4d18-9767-5dea310950b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'charttime',\n",
       " 'aniongap',\n",
       " 'bicarbonate',\n",
       " 'bun',\n",
       " 'calcium',\n",
       " 'chloride',\n",
       " 'creatinine',\n",
       " 'glucose',\n",
       " 'sodium',\n",
       " 'potassium',\n",
       " 'hematocrit',\n",
       " 'hemoglobin',\n",
       " 'mch',\n",
       " 'mchc',\n",
       " 'mcv',\n",
       " 'platelet',\n",
       " 'rbc',\n",
       " 'rdw',\n",
       " 'wbc',\n",
       " 'inr',\n",
       " 'pt',\n",
       " 'ptt',\n",
       " 'hosp_admittime',\n",
       " 'hosp_dischtime',\n",
       " 'icu_intime',\n",
       " 'icu_outtime',\n",
       " 'los_icu',\n",
       " 'icu_death',\n",
       " 'gender',\n",
       " 'race',\n",
       " 'admission_age',\n",
       " 'weight_admit',\n",
       " 'height',\n",
       " 'charlson_score',\n",
       " 'atrial_fibrillation',\n",
       " 'malignant_cancer',\n",
       " 'chf',\n",
       " 'ckd',\n",
       " 'cld',\n",
       " 'copd',\n",
       " 'diabetes',\n",
       " 'hypertension',\n",
       " 'ihd',\n",
       " 'stroke',\n",
       " 'race_encode_African',\n",
       " 'race_encode_Asian',\n",
       " 'race_encode_Caucasian',\n",
       " 'race_encode_Hispanic',\n",
       " 'race_encode_Not Specified',\n",
       " 'race_encode_South American',\n",
       " 'admission_type_DIRECT EMER.',\n",
       " 'admission_type_DIRECT OBSERVATION',\n",
       " 'admission_type_ELECTIVE',\n",
       " 'admission_type_EU OBSERVATION',\n",
       " 'admission_type_EW EMER.',\n",
       " 'admission_type_OBSERVATION ADMIT',\n",
       " 'admission_type_SURGICAL SAME DAY ADMISSION',\n",
       " 'admission_type_URGENT',\n",
       " 'first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)',\n",
       " 'first_careunit_Coronary Care Unit (CCU)',\n",
       " 'first_careunit_Medical Intensive Care Unit (MICU)',\n",
       " 'first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)',\n",
       " 'first_careunit_Neuro Intermediate',\n",
       " 'first_careunit_Neuro Stepdown',\n",
       " 'first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)',\n",
       " 'first_careunit_Surgical Intensive Care Unit (SICU)',\n",
       " 'first_careunit_Trauma SICU (TSICU)',\n",
       " 'hour',\n",
       " 'icu_cat']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c2fda0f-79cc-41f3-92ab-aae6683baabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                  int64\n",
       "charttime                                                          object\n",
       "aniongap                                                            int64\n",
       "bicarbonate                                                       float64\n",
       "bun                                                                 int64\n",
       "                                                                   ...   \n",
       "first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)      int64\n",
       "first_careunit_Surgical Intensive Care Unit (SICU)                  int64\n",
       "first_careunit_Trauma SICU (TSICU)                                  int64\n",
       "hour                                                              float64\n",
       "icu_cat                                                             int64\n",
       "Length: 70, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af9053a7-a592-4c69-b192-45265b0a6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71445, 59)\n",
      "(20415, 59)\n",
      "(10210, 59)\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['id',\n",
    "                   'charttime',\n",
    "                   'hour',\n",
    "                   'icu_cat',\n",
    "                   'hosp_admittime',\n",
    "                   'hosp_dischtime',\n",
    "                   'icu_intime',\n",
    "                   'icu_outtime',\n",
    "                   'los_icu',\n",
    "                   'icu_death',\n",
    "                   'race'\n",
    "                  ]\n",
    "\n",
    "X_train_df = train_df.drop(columns=columns_to_drop)\n",
    "X_test_df = test_df.drop(columns=columns_to_drop)\n",
    "X_holdout_df = holdout_df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(X_train_df.shape)\n",
    "print(X_test_df.shape)\n",
    "print(X_holdout_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a496d235-b35c-46d2-9b4c-e4844c15cd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aniongap                                                             int64\n",
       "bicarbonate                                                        float64\n",
       "bun                                                                  int64\n",
       "calcium                                                            float64\n",
       "chloride                                                             int64\n",
       "creatinine                                                         float64\n",
       "glucose                                                              int64\n",
       "sodium                                                               int64\n",
       "potassium                                                          float64\n",
       "hematocrit                                                         float64\n",
       "hemoglobin                                                         float64\n",
       "mch                                                                float64\n",
       "mchc                                                               float64\n",
       "mcv                                                                float64\n",
       "platelet                                                             int64\n",
       "rbc                                                                float64\n",
       "rdw                                                                float64\n",
       "wbc                                                                float64\n",
       "inr                                                                float64\n",
       "pt                                                                 float64\n",
       "ptt                                                                float64\n",
       "gender                                                               int64\n",
       "admission_age                                                      float64\n",
       "weight_admit                                                       float64\n",
       "height                                                             float64\n",
       "charlson_score                                                       int64\n",
       "atrial_fibrillation                                                  int64\n",
       "malignant_cancer                                                     int64\n",
       "chf                                                                  int64\n",
       "ckd                                                                  int64\n",
       "cld                                                                  int64\n",
       "copd                                                                 int64\n",
       "diabetes                                                             int64\n",
       "hypertension                                                         int64\n",
       "ihd                                                                  int64\n",
       "stroke                                                               int64\n",
       "race_encode_African                                                  int64\n",
       "race_encode_Asian                                                    int64\n",
       "race_encode_Caucasian                                                int64\n",
       "race_encode_Hispanic                                                 int64\n",
       "race_encode_Not Specified                                            int64\n",
       "race_encode_South American                                           int64\n",
       "admission_type_DIRECT EMER.                                          int64\n",
       "admission_type_DIRECT OBSERVATION                                    int64\n",
       "admission_type_ELECTIVE                                              int64\n",
       "admission_type_EU OBSERVATION                                        int64\n",
       "admission_type_EW EMER.                                              int64\n",
       "admission_type_OBSERVATION ADMIT                                     int64\n",
       "admission_type_SURGICAL SAME DAY ADMISSION                           int64\n",
       "admission_type_URGENT                                                int64\n",
       "first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)          int64\n",
       "first_careunit_Coronary Care Unit (CCU)                              int64\n",
       "first_careunit_Medical Intensive Care Unit (MICU)                    int64\n",
       "first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)      int64\n",
       "first_careunit_Neuro Intermediate                                    int64\n",
       "first_careunit_Neuro Stepdown                                        int64\n",
       "first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)       int64\n",
       "first_careunit_Surgical Intensive Care Unit (SICU)                   int64\n",
       "first_careunit_Trauma SICU (TSICU)                                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5f0cc5c-b3b9-470a-8991-e958c69e44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization using data from X_train to transform X_test, X_holdout\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = X_train_df.columns[X_train_df.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "# print(num_cols)\n",
    "scaler = StandardScaler()\n",
    "X_train_df[num_cols] = scaler.fit_transform(X_train_df[num_cols])\n",
    "X_test_df[num_cols] = scaler.transform(X_test_df[num_cols])\n",
    "X_holdout_df[num_cols] = scaler.transform(X_holdout_df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a42b7c28-ae75-400b-864e-c676de3d3b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aniongap</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>bun</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>glucose</th>\n",
       "      <th>sodium</th>\n",
       "      <th>potassium</th>\n",
       "      <th>hematocrit</th>\n",
       "      <th>...</th>\n",
       "      <th>admission_type_URGENT</th>\n",
       "      <th>first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)</th>\n",
       "      <th>first_careunit_Coronary Care Unit (CCU)</th>\n",
       "      <th>first_careunit_Medical Intensive Care Unit (MICU)</th>\n",
       "      <th>first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)</th>\n",
       "      <th>first_careunit_Neuro Intermediate</th>\n",
       "      <th>first_careunit_Neuro Stepdown</th>\n",
       "      <th>first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)</th>\n",
       "      <th>first_careunit_Surgical Intensive Care Unit (SICU)</th>\n",
       "      <th>first_careunit_Trauma SICU (TSICU)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "      <td>10210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.034285</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-0.022060</td>\n",
       "      <td>-0.023968</td>\n",
       "      <td>-0.013462</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.001639</td>\n",
       "      <td>-0.037537</td>\n",
       "      <td>-0.014542</td>\n",
       "      <td>-0.011621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011461</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.015107</td>\n",
       "      <td>0.013644</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>-0.036203</td>\n",
       "      <td>-0.017820</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>-0.017278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.982303</td>\n",
       "      <td>0.991229</td>\n",
       "      <td>0.971259</td>\n",
       "      <td>0.976816</td>\n",
       "      <td>0.995206</td>\n",
       "      <td>0.980604</td>\n",
       "      <td>0.965954</td>\n",
       "      <td>0.965095</td>\n",
       "      <td>0.983992</td>\n",
       "      <td>0.978542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990376</td>\n",
       "      <td>1.034467</td>\n",
       "      <td>0.988041</td>\n",
       "      <td>0.991868</td>\n",
       "      <td>1.010491</td>\n",
       "      <td>1.063012</td>\n",
       "      <td>0.577845</td>\n",
       "      <td>0.939553</td>\n",
       "      <td>1.011773</td>\n",
       "      <td>0.979637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.148475</td>\n",
       "      <td>-3.511353</td>\n",
       "      <td>-1.257441</td>\n",
       "      <td>-7.334448</td>\n",
       "      <td>-5.686783</td>\n",
       "      <td>-0.890567</td>\n",
       "      <td>-1.815757</td>\n",
       "      <td>-5.962329</td>\n",
       "      <td>-3.688452</td>\n",
       "      <td>-3.349954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468075</td>\n",
       "      <td>-0.363662</td>\n",
       "      <td>-0.370780</td>\n",
       "      <td>-0.600822</td>\n",
       "      <td>-0.489721</td>\n",
       "      <td>-0.083527</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>-0.148916</td>\n",
       "      <td>-0.421541</td>\n",
       "      <td>-0.371391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.546588</td>\n",
       "      <td>-0.600528</td>\n",
       "      <td>-0.687793</td>\n",
       "      <td>-0.549026</td>\n",
       "      <td>-0.575844</td>\n",
       "      <td>-0.546173</td>\n",
       "      <td>-0.548309</td>\n",
       "      <td>-0.580979</td>\n",
       "      <td>-0.650235</td>\n",
       "      <td>-0.592042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468075</td>\n",
       "      <td>-0.363662</td>\n",
       "      <td>-0.370780</td>\n",
       "      <td>-0.600822</td>\n",
       "      <td>-0.489721</td>\n",
       "      <td>-0.083527</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>-0.148916</td>\n",
       "      <td>-0.421541</td>\n",
       "      <td>-0.371391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.096352</td>\n",
       "      <td>-0.018363</td>\n",
       "      <td>-0.260556</td>\n",
       "      <td>0.056815</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>-0.300177</td>\n",
       "      <td>-0.262111</td>\n",
       "      <td>-0.024287</td>\n",
       "      <td>-0.121850</td>\n",
       "      <td>-0.251559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468075</td>\n",
       "      <td>-0.363662</td>\n",
       "      <td>-0.370780</td>\n",
       "      <td>-0.600822</td>\n",
       "      <td>-0.489721</td>\n",
       "      <td>-0.083527</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>-0.148916</td>\n",
       "      <td>-0.421541</td>\n",
       "      <td>-0.371391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.353884</td>\n",
       "      <td>0.563802</td>\n",
       "      <td>0.288748</td>\n",
       "      <td>0.420320</td>\n",
       "      <td>0.592371</td>\n",
       "      <td>0.093416</td>\n",
       "      <td>0.255771</td>\n",
       "      <td>0.532404</td>\n",
       "      <td>0.505608</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468075</td>\n",
       "      <td>-0.363662</td>\n",
       "      <td>-0.370780</td>\n",
       "      <td>1.664387</td>\n",
       "      <td>-0.489721</td>\n",
       "      <td>-0.083527</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>-0.148916</td>\n",
       "      <td>-0.421541</td>\n",
       "      <td>-0.371391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.233011</td>\n",
       "      <td>5.027067</td>\n",
       "      <td>6.676951</td>\n",
       "      <td>14.354667</td>\n",
       "      <td>4.681122</td>\n",
       "      <td>14.951563</td>\n",
       "      <td>17.850131</td>\n",
       "      <td>6.099319</td>\n",
       "      <td>7.539740</td>\n",
       "      <td>4.651395</td>\n",
       "      <td>...</td>\n",
       "      <td>2.136411</td>\n",
       "      <td>2.749803</td>\n",
       "      <td>2.697014</td>\n",
       "      <td>1.664387</td>\n",
       "      <td>2.041980</td>\n",
       "      <td>11.972190</td>\n",
       "      <td>18.417771</td>\n",
       "      <td>6.715173</td>\n",
       "      <td>2.372246</td>\n",
       "      <td>2.692582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aniongap   bicarbonate           bun       calcium      chloride  \\\n",
       "count  10210.000000  10210.000000  10210.000000  10210.000000  10210.000000   \n",
       "mean      -0.034285      0.003143     -0.022060     -0.023968     -0.013462   \n",
       "std        0.982303      0.991229      0.971259      0.976816      0.995206   \n",
       "min       -4.148475     -3.511353     -1.257441     -7.334448     -5.686783   \n",
       "25%       -0.546588     -0.600528     -0.687793     -0.549026     -0.575844   \n",
       "50%       -0.096352     -0.018363     -0.260556      0.056815      0.008264   \n",
       "75%        0.353884      0.563802      0.288748      0.420320      0.592371   \n",
       "max        8.233011      5.027067      6.676951     14.354667      4.681122   \n",
       "\n",
       "         creatinine       glucose        sodium     potassium    hematocrit  \\\n",
       "count  10210.000000  10210.000000  10210.000000  10210.000000  10210.000000   \n",
       "mean       0.002053     -0.001639     -0.037537     -0.014542     -0.011621   \n",
       "std        0.980604      0.965954      0.965095      0.983992      0.978542   \n",
       "min       -0.890567     -1.815757     -5.962329     -3.688452     -3.349954   \n",
       "25%       -0.546173     -0.548309     -0.580979     -0.650235     -0.592042   \n",
       "50%       -0.300177     -0.262111     -0.024287     -0.121850     -0.251559   \n",
       "75%        0.093416      0.255771      0.532404      0.505608      0.565600   \n",
       "max       14.951563     17.850131      6.099319      7.539740      4.651395   \n",
       "\n",
       "       ...  admission_type_URGENT  \\\n",
       "count  ...           10210.000000   \n",
       "mean   ...              -0.011461   \n",
       "std    ...               0.990376   \n",
       "min    ...              -0.468075   \n",
       "25%    ...              -0.468075   \n",
       "50%    ...              -0.468075   \n",
       "75%    ...              -0.468075   \n",
       "max    ...               2.136411   \n",
       "\n",
       "       first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)  \\\n",
       "count                                       10210.000000             \n",
       "mean                                            0.029714             \n",
       "std                                             1.034467             \n",
       "min                                            -0.363662             \n",
       "25%                                            -0.363662             \n",
       "50%                                            -0.363662             \n",
       "75%                                            -0.363662             \n",
       "max                                             2.749803             \n",
       "\n",
       "       first_careunit_Coronary Care Unit (CCU)  \\\n",
       "count                             10210.000000   \n",
       "mean                                 -0.010217   \n",
       "std                                   0.988041   \n",
       "min                                  -0.370780   \n",
       "25%                                  -0.370780   \n",
       "50%                                  -0.370780   \n",
       "75%                                  -0.370780   \n",
       "max                                   2.697014   \n",
       "\n",
       "       first_careunit_Medical Intensive Care Unit (MICU)  \\\n",
       "count                                       10210.000000   \n",
       "mean                                           -0.015107   \n",
       "std                                             0.991868   \n",
       "min                                            -0.600822   \n",
       "25%                                            -0.600822   \n",
       "50%                                            -0.600822   \n",
       "75%                                             1.664387   \n",
       "max                                             1.664387   \n",
       "\n",
       "       first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)  \\\n",
       "count                                       10210.000000                 \n",
       "mean                                            0.013644                 \n",
       "std                                             1.010491                 \n",
       "min                                            -0.489721                 \n",
       "25%                                            -0.489721                 \n",
       "50%                                            -0.489721                 \n",
       "75%                                            -0.489721                 \n",
       "max                                             2.041980                 \n",
       "\n",
       "       first_careunit_Neuro Intermediate  first_careunit_Neuro Stepdown  \\\n",
       "count                       10210.000000                   10210.000000   \n",
       "mean                            0.010935                      -0.036203   \n",
       "std                             1.063012                       0.577845   \n",
       "min                            -0.083527                      -0.054295   \n",
       "25%                            -0.083527                      -0.054295   \n",
       "50%                            -0.083527                      -0.054295   \n",
       "75%                            -0.083527                      -0.054295   \n",
       "max                            11.972190                      18.417771   \n",
       "\n",
       "       first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)  \\\n",
       "count                                       10210.000000                \n",
       "mean                                           -0.017820                \n",
       "std                                             0.939553                \n",
       "min                                            -0.148916                \n",
       "25%                                            -0.148916                \n",
       "50%                                            -0.148916                \n",
       "75%                                            -0.148916                \n",
       "max                                             6.715173                \n",
       "\n",
       "       first_careunit_Surgical Intensive Care Unit (SICU)  \\\n",
       "count                                       10210.000000    \n",
       "mean                                            0.012166    \n",
       "std                                             1.011773    \n",
       "min                                            -0.421541    \n",
       "25%                                            -0.421541    \n",
       "50%                                            -0.421541    \n",
       "75%                                            -0.421541    \n",
       "max                                             2.372246    \n",
       "\n",
       "       first_careunit_Trauma SICU (TSICU)  \n",
       "count                        10210.000000  \n",
       "mean                            -0.017278  \n",
       "std                              0.979637  \n",
       "min                             -0.371391  \n",
       "25%                             -0.371391  \n",
       "50%                             -0.371391  \n",
       "75%                             -0.371391  \n",
       "max                              2.692582  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8738a4-0c77-4f24-a1c5-fb7c06bf6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting los_icu into 3 categorical bins\n",
    "\n",
    "# low_thres = 2.5\n",
    "# medium_thres = 5\n",
    "\n",
    "# result_df['icu_cat'] = 10\n",
    "# result_df.loc[result_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# result_df.loc[((result_df['los_icu'] >= low_thres) & (result_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# result_df.loc[result_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "# test_df['icu_cat'] = 10\n",
    "# test_df.loc[test_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# test_df.loc[((test_df['los_icu'] >= low_thres) & (test_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# test_df.loc[test_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "# holdout_df['icu_cat'] = 10\n",
    "# holdout_df.loc[holdout_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "# holdout_df.loc[((holdout_df['los_icu'] >= low_thres) & (holdout_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "# holdout_df.loc[holdout_df['los_icu'] >= medium_thres, 'icu_cat'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbbc2bc4-deee-4f2e-bb21-061095cc0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sorting hours to ascending order\n",
    "\n",
    "# result_df = result_df.sort_values(by=['id', 'hour'], ascending=[True, True])\n",
    "# test_df = test_df.sort_values(by=['id', 'hour'], ascending=[True, True])\n",
    "# holdout_df = holdout_df.sort_values(by=['id', 'hour'], ascending=[True, True])\n",
    "\n",
    "# result_df.reset_index(drop=True, inplace=True)\n",
    "# test_df.reset_index(drop=True, inplace=True)\n",
    "# holdout_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c32536f-bed5-4991-b1fc-6fc5e1d9daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming X data into LSTM input format of [num_samples, time_step, num_features]\n",
    "\n",
    "\n",
    "steps = 5\n",
    "\n",
    "numpy_train_data = X_train_df.values\n",
    "numpy_test_data = X_test_df.values\n",
    "numpy_holdout_data = X_holdout_df.values\n",
    "\n",
    "# print(len(numpy_data))\n",
    "X_train_input = []\n",
    "y_train = []\n",
    "X_test_input = []\n",
    "y_test = []\n",
    "X_holdout_input = []\n",
    "y_holdout = []\n",
    "\n",
    "\n",
    "for i in range(int(len(numpy_train_data)/steps)):\n",
    "    sample = X_train_df.iloc[i*steps:i*steps+steps]\n",
    "    label = train_df.iloc[i*steps][-1]\n",
    "    X_train_input.append(sample)\n",
    "    y_train.append(label)\n",
    "\n",
    "for i in range(int(len(numpy_test_data)/steps)):\n",
    "    sample = X_test_df.iloc[i*steps:i*steps+steps]\n",
    "    label = test_df.iloc[i*steps][-1]\n",
    "    X_test_input.append(sample)\n",
    "    y_test.append(label)\n",
    "\n",
    "for i in range(int(len(numpy_holdout_data)/steps)):\n",
    "    sample = X_holdout_df.iloc[i*steps:i*steps+steps]\n",
    "    label = holdout_df.iloc[i*steps][-1]\n",
    "    X_holdout_input.append(sample)\n",
    "    y_holdout.append(label)\n",
    "\n",
    "X_train_input = np.array(X_train_input)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test_input = np.array(X_test_input)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_holdout_input = np.array(X_holdout_input)\n",
    "y_holdout = np.array(y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3315026-7c32-44e9-9a9a-4d57a56179b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14289, 5, 59)\n",
      "(4083, 5, 59)\n",
      "(2042, 5, 59)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_input.shape)\n",
    "print(X_test_input.shape)\n",
    "print(X_holdout_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d222a0b-0ba4-4607-9f09-423602046782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 14289, 59)\n",
      "(5, 4083, 59)\n",
      "(5, 2042, 59)\n"
     ]
    }
   ],
   "source": [
    "# converting into transformer input format of [time_step, num_samples, num_features]\n",
    "\n",
    "X_train_trans = np.transpose(X_train_input, (1, 0, 2))\n",
    "X_test_trans = np.transpose(X_test_input, (1, 0, 2))\n",
    "X_holdout_trans = np.transpose(X_holdout_input, (1, 0, 2))\n",
    "\n",
    "print(X_train_trans.shape)\n",
    "print(X_test_trans.shape)\n",
    "print(X_holdout_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bea3d3c4-0497-4f8d-be5f-21d339bd72ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 14289, 59])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "X_train_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccca31d5-fde5-4dd8-bc80-d17a2490dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_input.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa39137d-ed63-4ab5-83a2-b1ffc9b595cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.09635218 -0.01836304  0.57357206 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.01836304  0.57357206 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.01836304  0.57357206 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.54658805  0.36974698  0.61426125 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.54658805  0.17569197  0.69563963 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]]\n",
      "\n",
      " [[-0.32147012 -0.21241805 -0.19952254 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.60052807 -0.03676578 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.01836304  0.12599098 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.01836304 -0.24021173 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.57900163 -0.01836304  0.28874774 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]]\n",
      "\n",
      " [[ 0.12876576 -0.21241805 -0.80986039 ...  6.715173   -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.12876576 -0.21241805 -0.80986039 ...  6.715173   -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.12876576 -0.21241805 -0.80986039 ...  6.715173   -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.09635218 -0.01836304 -0.24021173 ...  6.715173   -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.32147012 -0.21241805 -0.97261715 ...  6.715173   -0.42154144\n",
      "   -0.37139068]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.77170599  0.36974698 -0.7691712  ... -0.14891649 -0.42154144\n",
      "    2.6925824 ]\n",
      "  [-0.77170599  0.36974698 -0.7691712  ... -0.14891649 -0.42154144\n",
      "    2.6925824 ]\n",
      "  [-0.77170599  0.36974698 -0.7691712  ... -0.14891649 -0.42154144\n",
      "    2.6925824 ]\n",
      "  [-0.77170599  0.36974698 -0.7691712  ... -0.14891649 -0.42154144\n",
      "    2.6925824 ]\n",
      "  [-0.77170599  0.36974698 -0.7691712  ... -0.14891649 -0.42154144\n",
      "    2.6925824 ]]\n",
      "\n",
      " [[-0.54658805 -0.60052807 -0.3622793  ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-0.54658805 -0.60052807 -0.3622793  ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.3538837  -1.18269311 -0.64710363 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.57900163 -1.76485814 -0.64710363 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [-1.4470598  -0.01836304 -0.56572525 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]]\n",
      "\n",
      " [[ 0.3538837   1.34002203  0.37012611 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 1.02923751  1.34002203  0.65495044 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 1.70459132  0.757857    0.37012611 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 0.57900163  1.14596702  0.57357206 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]\n",
      "  [ 1.47947338  0.56380199  1.06184234 ... -0.14891649 -0.42154144\n",
      "   -0.37139068]]]\n",
      "[1 2 2 1 1 0 0 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_input[:10])\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6619d99-e3f1-439c-b2fe-b93c28799052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f25c276d-24ae-4938-bead-0b06ba0daec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.0822\n",
      "Epoch [2/200], Loss: 1.0797\n",
      "Epoch [3/200], Loss: 1.0778\n",
      "Epoch [4/200], Loss: 1.0761\n",
      "Epoch [5/200], Loss: 1.0749\n",
      "Epoch [6/200], Loss: 1.0740\n",
      "Epoch [7/200], Loss: 1.0734\n",
      "Epoch [8/200], Loss: 1.0735\n",
      "Epoch [9/200], Loss: 1.0733\n",
      "Epoch [10/200], Loss: 1.0721\n",
      "Epoch [11/200], Loss: 1.0710\n",
      "Epoch [12/200], Loss: 1.0690\n",
      "Epoch [13/200], Loss: 1.0670\n",
      "Epoch [14/200], Loss: 1.0641\n",
      "Epoch [15/200], Loss: 1.0615\n",
      "Epoch [16/200], Loss: 1.0588\n",
      "Epoch [17/200], Loss: 1.0555\n",
      "Epoch [18/200], Loss: 1.0531\n",
      "Epoch [19/200], Loss: 1.0513\n",
      "Epoch [20/200], Loss: 1.0498\n",
      "Epoch [21/200], Loss: 1.0482\n",
      "Epoch [22/200], Loss: 1.0460\n",
      "Epoch [23/200], Loss: 1.0448\n",
      "Epoch [24/200], Loss: 1.0440\n",
      "Epoch [25/200], Loss: 1.0426\n",
      "Epoch [26/200], Loss: 1.0410\n",
      "Epoch [27/200], Loss: 1.0384\n",
      "Epoch [28/200], Loss: 1.0368\n",
      "Epoch [29/200], Loss: 1.0350\n",
      "Epoch [30/200], Loss: 1.0341\n",
      "Epoch [31/200], Loss: 1.0319\n",
      "Epoch [32/200], Loss: 1.0303\n",
      "Epoch [33/200], Loss: 1.0289\n",
      "Epoch [34/200], Loss: 1.0275\n",
      "Epoch [35/200], Loss: 1.0251\n",
      "Epoch [36/200], Loss: 1.0229\n",
      "Epoch [37/200], Loss: 1.0208\n",
      "Epoch [38/200], Loss: 1.0177\n",
      "Epoch [39/200], Loss: 1.0158\n",
      "Epoch [40/200], Loss: 1.0133\n",
      "Epoch [41/200], Loss: 1.0117\n",
      "Epoch [42/200], Loss: 1.0088\n",
      "Epoch [43/200], Loss: 1.0061\n",
      "Epoch [44/200], Loss: 1.0037\n",
      "Epoch [45/200], Loss: 1.0013\n",
      "Epoch [46/200], Loss: 0.9990\n",
      "Epoch [47/200], Loss: 0.9968\n",
      "Epoch [48/200], Loss: 0.9924\n",
      "Epoch [49/200], Loss: 0.9902\n",
      "Epoch [50/200], Loss: 0.9880\n",
      "Epoch [51/200], Loss: 0.9853\n",
      "Epoch [52/200], Loss: 0.9835\n",
      "Epoch [53/200], Loss: 0.9799\n",
      "Epoch [54/200], Loss: 0.9777\n",
      "Epoch [55/200], Loss: 0.9747\n",
      "Epoch [56/200], Loss: 0.9729\n",
      "Epoch [57/200], Loss: 0.9699\n",
      "Epoch [58/200], Loss: 0.9677\n",
      "Epoch [59/200], Loss: 0.9657\n",
      "Epoch [60/200], Loss: 0.9624\n",
      "Epoch [61/200], Loss: 0.9606\n",
      "Epoch [62/200], Loss: 0.9573\n",
      "Epoch [63/200], Loss: 0.9546\n",
      "Epoch [64/200], Loss: 0.9517\n",
      "Epoch [65/200], Loss: 0.9482\n",
      "Epoch [66/200], Loss: 0.9455\n",
      "Epoch [67/200], Loss: 0.9424\n",
      "Epoch [68/200], Loss: 0.9386\n",
      "Epoch [69/200], Loss: 0.9351\n",
      "Epoch [70/200], Loss: 0.9320\n",
      "Epoch [71/200], Loss: 0.9274\n",
      "Epoch [72/200], Loss: 0.9247\n",
      "Epoch [73/200], Loss: 0.9222\n",
      "Epoch [74/200], Loss: 0.9173\n",
      "Epoch [75/200], Loss: 0.9120\n",
      "Epoch [76/200], Loss: 0.9119\n",
      "Epoch [77/200], Loss: 0.9043\n",
      "Epoch [78/200], Loss: 0.9024\n",
      "Epoch [79/200], Loss: 0.8994\n",
      "Epoch [80/200], Loss: 0.8925\n",
      "Epoch [81/200], Loss: 0.8896\n",
      "Epoch [82/200], Loss: 0.8851\n",
      "Epoch [83/200], Loss: 0.8793\n",
      "Epoch [84/200], Loss: 0.8761\n",
      "Epoch [85/200], Loss: 0.8719\n",
      "Epoch [86/200], Loss: 0.8659\n",
      "Epoch [87/200], Loss: 0.8608\n",
      "Epoch [88/200], Loss: 0.8547\n",
      "Epoch [89/200], Loss: 0.8521\n",
      "Epoch [90/200], Loss: 0.8503\n",
      "Epoch [91/200], Loss: 0.8501\n",
      "Epoch [92/200], Loss: 0.8340\n",
      "Epoch [93/200], Loss: 0.8341\n",
      "Epoch [94/200], Loss: 0.8346\n",
      "Epoch [95/200], Loss: 0.8201\n",
      "Epoch [96/200], Loss: 0.8233\n",
      "Epoch [97/200], Loss: 0.8144\n",
      "Epoch [98/200], Loss: 0.8079\n",
      "Epoch [99/200], Loss: 0.8072\n",
      "Epoch [100/200], Loss: 0.7970\n",
      "Epoch [101/200], Loss: 0.7930\n",
      "Epoch [102/200], Loss: 0.7886\n",
      "Epoch [103/200], Loss: 0.7833\n",
      "Epoch [104/200], Loss: 0.7745\n",
      "Epoch [105/200], Loss: 0.7728\n",
      "Epoch [106/200], Loss: 0.7688\n",
      "Epoch [107/200], Loss: 0.7596\n",
      "Epoch [108/200], Loss: 0.7571\n",
      "Epoch [109/200], Loss: 0.7493\n",
      "Epoch [110/200], Loss: 0.7479\n",
      "Epoch [111/200], Loss: 0.7449\n",
      "Epoch [112/200], Loss: 0.7444\n",
      "Epoch [113/200], Loss: 0.7510\n",
      "Epoch [114/200], Loss: 0.7328\n",
      "Epoch [115/200], Loss: 0.7255\n",
      "Epoch [116/200], Loss: 0.7215\n",
      "Epoch [117/200], Loss: 0.7246\n",
      "Epoch [118/200], Loss: 0.7049\n",
      "Epoch [119/200], Loss: 0.7091\n",
      "Epoch [120/200], Loss: 0.7008\n",
      "Epoch [121/200], Loss: 0.6941\n",
      "Epoch [122/200], Loss: 0.6883\n",
      "Epoch [123/200], Loss: 0.6886\n",
      "Epoch [124/200], Loss: 0.6805\n",
      "Epoch [125/200], Loss: 0.6745\n",
      "Epoch [126/200], Loss: 0.6701\n",
      "Epoch [127/200], Loss: 0.6679\n",
      "Epoch [128/200], Loss: 0.6606\n",
      "Epoch [129/200], Loss: 0.6541\n",
      "Epoch [130/200], Loss: 0.6497\n",
      "Epoch [131/200], Loss: 0.6469\n",
      "Epoch [132/200], Loss: 0.6464\n",
      "Epoch [133/200], Loss: 0.6457\n",
      "Epoch [134/200], Loss: 0.6504\n",
      "Epoch [135/200], Loss: 0.6425\n",
      "Epoch [136/200], Loss: 0.6314\n",
      "Epoch [137/200], Loss: 0.6194\n",
      "Epoch [138/200], Loss: 0.6227\n",
      "Epoch [139/200], Loss: 0.6196\n",
      "Epoch [140/200], Loss: 0.6073\n",
      "Epoch [141/200], Loss: 0.6105\n",
      "Epoch [142/200], Loss: 0.6024\n",
      "Epoch [143/200], Loss: 0.5987\n",
      "Epoch [144/200], Loss: 0.5944\n",
      "Epoch [145/200], Loss: 0.5884\n",
      "Epoch [146/200], Loss: 0.5843\n",
      "Epoch [147/200], Loss: 0.5796\n",
      "Epoch [148/200], Loss: 0.5745\n",
      "Epoch [149/200], Loss: 0.5711\n",
      "Epoch [150/200], Loss: 0.5658\n",
      "Epoch [151/200], Loss: 0.5623\n",
      "Epoch [152/200], Loss: 0.5563\n",
      "Epoch [153/200], Loss: 0.5520\n",
      "Epoch [154/200], Loss: 0.5478\n",
      "Epoch [155/200], Loss: 0.5439\n",
      "Epoch [156/200], Loss: 0.5394\n",
      "Epoch [157/200], Loss: 0.5357\n",
      "Epoch [158/200], Loss: 0.5321\n",
      "Epoch [159/200], Loss: 0.5480\n",
      "Epoch [160/200], Loss: 0.7320\n",
      "Epoch [161/200], Loss: 0.9635\n",
      "Epoch [162/200], Loss: 0.6444\n",
      "Epoch [163/200], Loss: 0.8624\n",
      "Epoch [164/200], Loss: 0.6644\n",
      "Epoch [165/200], Loss: 0.6640\n",
      "Epoch [166/200], Loss: 0.7503\n",
      "Epoch [167/200], Loss: 0.6463\n",
      "Epoch [168/200], Loss: 0.6301\n",
      "Epoch [169/200], Loss: 0.6701\n",
      "Epoch [170/200], Loss: 0.6304\n",
      "Epoch [171/200], Loss: 0.6022\n",
      "Epoch [172/200], Loss: 0.6302\n",
      "Epoch [173/200], Loss: 0.6121\n",
      "Epoch [174/200], Loss: 0.5782\n",
      "Epoch [175/200], Loss: 0.5855\n",
      "Epoch [176/200], Loss: 0.5952\n",
      "Epoch [177/200], Loss: 0.5771\n",
      "Epoch [178/200], Loss: 0.5581\n",
      "Epoch [179/200], Loss: 0.5571\n",
      "Epoch [180/200], Loss: 0.5629\n",
      "Epoch [181/200], Loss: 0.5512\n",
      "Epoch [182/200], Loss: 0.5385\n",
      "Epoch [183/200], Loss: 0.5380\n",
      "Epoch [184/200], Loss: 0.5357\n",
      "Epoch [185/200], Loss: 0.5270\n",
      "Epoch [186/200], Loss: 0.5254\n",
      "Epoch [187/200], Loss: 0.5194\n",
      "Epoch [188/200], Loss: 0.5119\n",
      "Epoch [189/200], Loss: 0.5107\n",
      "Epoch [190/200], Loss: 0.5106\n",
      "Epoch [191/200], Loss: 0.5038\n",
      "Epoch [192/200], Loss: 0.4997\n",
      "Epoch [193/200], Loss: 0.4986\n",
      "Epoch [194/200], Loss: 0.4953\n",
      "Epoch [195/200], Loss: 0.4920\n",
      "Epoch [196/200], Loss: 0.4896\n",
      "Epoch [197/200], Loss: 0.4859\n",
      "Epoch [198/200], Loss: 0.4835\n",
      "Epoch [199/200], Loss: 0.4806\n",
      "Epoch [200/200], Loss: 0.4779\n"
     ]
    }
   ],
   "source": [
    "# Example sequential measurement data\n",
    "# Assuming each sequence has 5 time steps and 59 features\n",
    "# X = np.random.randn(14289, 5, 59)\n",
    "X_train = torch.tensor(X_train_input, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_input, dtype=torch.float32)\n",
    "X_holdout = torch.tensor(X_holdout_input, dtype=torch.float32)\n",
    "\n",
    "# Example classification labels\n",
    "# y = np.random.randint(0, 3, 14289)  # Assuming three classes\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "y_holdout = torch.tensor(y_holdout, dtype=torch.long)\n",
    "\n",
    "# Define LSTM model for classification\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate=0.0):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Defining dropout layer with specified dropout rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Applying dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instantiate the classification model\n",
    "input_size = 59  # Number of features in each time step\n",
    "hidden_size = 100  # Number of LSTM units\n",
    "num_layers = 4  # Number of LSTM layers\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_rate = 0.3  # Example dropout rate\n",
    "classification_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, dropout_rate)\n",
    "# classification_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classification model\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classification_model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad4b42bc-faa9-403e-b537-e8fcc5d3213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      6248\n",
      "           1       0.82      0.63      0.71      4198\n",
      "           2       0.74      0.93      0.83      3843\n",
      "\n",
      "    accuracy                           0.81     14289\n",
      "   macro avg       0.81      0.81      0.80     14289\n",
      "weighted avg       0.82      0.81      0.81     14289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_train)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_train.numpy(), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4265f04-23dd-4f3d-b0fe-f7d0ab62c544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_train)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(y_train.numpy(), predicted)\n",
    "\n",
    "# # Print confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "cm = confusion_matrix(y_train.numpy(), predicted)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Training Data Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbe45195-b1cc-4566-832b-46c7633b9f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52      1794\n",
      "           1       0.29      0.24      0.27      1150\n",
      "           2       0.40      0.47      0.43      1139\n",
      "\n",
      "    accuracy                           0.43      4083\n",
      "   macro avg       0.41      0.41      0.41      4083\n",
      "weighted avg       0.42      0.43      0.42      4083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test.numpy(), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "643a9788-2db3-4066-953b-37673857acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(y_train.numpy(), predicted)\n",
    "\n",
    "# # Print confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "cm = confusion_matrix(y_test.numpy(), predicted)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Testing Data Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd04eb6-b910-4c55-aa69-8d65a4feaab3",
   "metadata": {},
   "source": [
    "# Attempting Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0544713-0ef4-47b0-877a-29cb85e5b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = torch.tensor(X_train_trans, dtype=torch.float32)\n",
    "X_test_trans = torch.tensor(X_test_trans, dtype=torch.float32)\n",
    "X_holdout_trans = torch.tensor(X_holdout_trans, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a5af6c8c-75c4-491e-bcb7-a03a6712615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 14289, 59])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "329d2da8-05f5-473d-85b8-601de55b2da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14289])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99c2e11c-d946-426b-a617-be4ccd7e8551",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_trans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[0;32m     62\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[70], line 15\u001b[0m, in \u001b[0;36mTransformerClassifier.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# src: [seq_length, batch_size, input_size]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# tgt: [seq_length, batch_size, input_size] (optional)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transformer_output, \u001b[38;5;28mtuple\u001b[39m):  \u001b[38;5;66;03m# If tuple, extract the output tensor\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         transformer_output \u001b[38;5;241m=\u001b[39m transformer_output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:198\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Take in and process masked source/target sequences.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=input_size, nhead=4, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        # src: [seq_length, batch_size, input_size]\n",
    "        # tgt: [seq_length, batch_size, input_size] (optional)\n",
    "        transformer_output = self.transformer(src, tgt)\n",
    "        if isinstance(transformer_output, tuple):  # If tuple, extract the output tensor\n",
    "            transformer_output = transformer_output[0]\n",
    "        last_output = transformer_output[-1, :, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "    # def forward(self, src, tgt=None):\n",
    "    #     # src: [seq_length, batch_size, input_size]\n",
    "    #     # tgt: [seq_length, batch_size, input_size] (optional)\n",
    "    #     transformer_output = self.transformer(src, tgt)\n",
    "    #     last_output = transformer_output[-1, :, :]\n",
    "    #     output = self.fc(last_output)\n",
    "    #     return output\n",
    "    # def forward(self, x):\n",
    "    #     # x: [seq_length, batch_size, input_size]\n",
    "    #     transformer_output = self.transformer(x)\n",
    "    #     # Take only the output of the last time step\n",
    "    #     last_output = transformer_output[-1, :, :]\n",
    "    #     # Feed the output of the last time step to the final linear layer\n",
    "    #     output = self.fc(last_output)\n",
    "    #     return output\n",
    "\n",
    "# Example usage\n",
    "# Define hyperparameters\n",
    "input_size = 60  # Number of features\n",
    "seq_length = 5  # Length of each sequence\n",
    "hidden_size = 64  # Hidden size of the transformer\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_classes = 3  # Number of output classes\n",
    "\n",
    "# Create an instance of the model\n",
    "model = TransformerClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy input data (replace with your actual data)\n",
    "# X_train = torch.randn(seq_length, 32, input_size)\n",
    "# y_train = torch.randint(0, num_classes, (32,))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_trans)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, you can use the model for inference\n",
    "# For example:\n",
    "# X_test = torch.randn(seq_length, 10, input_size)\n",
    "# outputs = model(X_test)\n",
    "# predicted_labels = torch.argmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e9afe88-eb9c-4fe8-b9a5-3751da9aacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\AppData\\Local\\Temp\\ipykernel_17144\\3752788572.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_trans_try = torch.tensor(X_train_trans)\n",
      "C:\\Users\\ngqin\\AppData\\Local\\Temp\\ipykernel_17144\\3752788572.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_try = torch.tensor(y_train)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_trans_try\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_try)\n\u001b[0;32m     66\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 19\u001b[0m, in \u001b[0;36mTransformerClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Add embedding and positional encoding\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:seq_len, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_encoding = self.positional_encoding(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len = inputs.size(0)\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        # Add embedding and positional encoding\n",
    "        x = self.embedding(inputs)\n",
    "        x = x + self.pos_encoding[:seq_len, :].unsqueeze(1)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.dropout(x)\n",
    "        transformer_output = self.transformer(x)\n",
    "        \n",
    "        # Take only the output of the last time step\n",
    "        last_output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Feed the output of the last time step to the final linear layer\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "    def positional_encoding(self, input_size, hidden_size):\n",
    "        position = torch.arange(input_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_size))\n",
    "        pe = torch.zeros(input_size, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 1000  # Vocabulary size\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "num_classes = 10\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create an instance of the model\n",
    "model = TransformerClassifier(input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert X_train_trans and y_train to PyTorch tensors if not already\n",
    "X_train_trans_try = torch.tensor(X_train_trans)\n",
    "y_train_try = torch.tensor(y_train)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_trans_try)\n",
    "    loss = criterion(outputs, y_train_try)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73c1863a-f632-4900-a47a-f6b4185da2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m transformer_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_trans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert X_train to integer type\u001b[39;00m\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[0;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[77], line 19\u001b[0m, in \u001b[0;36mTransformerClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Add embedding and positional encoding\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:seq_len, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Transformer encoder\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_encoding = self.positional_encoding(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        seq_len = inputs.size(0)\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        # Add embedding and positional encoding\n",
    "        x = self.embedding(inputs)\n",
    "        x = x + self.pos_encoding[:seq_len, :].unsqueeze(1)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.dropout(x)\n",
    "        transformer_output = self.transformer(x)\n",
    "        \n",
    "        # Take only the output of the last time step\n",
    "        last_output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Feed the output of the last time step to the final linear layer\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "    def positional_encoding(self, input_size, hidden_size):\n",
    "        position = torch.arange(input_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_size))\n",
    "        pe = torch.zeros(input_size, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "# Instantiate the Transformer classifier model\n",
    "input_size = 59  # Number of features in each time step\n",
    "hidden_size = 100  # Hidden size of the transformer\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_rate = 0.3  # Example dropout rate\n",
    "transformer_model = TransformerClassifier(input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Transformer classifier model\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer_model(X_train_trans.long())  # Convert X_train to integer type\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c786251c-ed1d-4ab9-93f7-587a18209280",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerClassifier.__init__() takes from 4 to 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Create the model instance\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer (replace with your choices if needed)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerClassifier.__init__() takes from 4 to 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "  def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
    "    super(TransformerClassifier, self).__init__()\n",
    "    self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout), num_layers)\n",
    "    self.linear = nn.Linear(d_model, 3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # No embedding layer, directly feed the input to the transformer\n",
    "    # x = x.permute(1, 0, 2)  # [batch_size, seq_len, feature_dim] -> (seq_len, batch_size, feature_dim)\n",
    "\n",
    "    # Pass the input through the transformer\n",
    "    x = self.transformer(x)\n",
    "\n",
    "    # Extract the final hidden representation for each token (last layer, output token)\n",
    "    pooled_output = x[:, -1, :]  # [seq_len, d_model] -> [batch_size, d_model]\n",
    "\n",
    "    # Classify using a linear layer\n",
    "    logits = self.linear(pooled_output)\n",
    "    predictions = F.log_softmax(logits, dim=1)  # [batch_size, num_classes]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# # Example usage (assuming your input x has shape [batch_size, seq_len, feature_dim])\n",
    "# model = TransformerClassifier(d_model=256, nhead=4, num_layers=2)\n",
    "# output = model(x)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "batch_size = 14289\n",
    "seq_len = 5\n",
    "feature_dim = 78  # Assuming your features have dimension 78\n",
    "\n",
    "# x = torch.randn(batch_size, seq_len, feature_dim)  # Random input data\n",
    "\n",
    "# Define model hyperparameters\n",
    "vocab_size = 10000  # Adjust based on your vocabulary size\n",
    "embedding_dim = 128\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Create the model instance\n",
    "model = TransformerClassifier(vocab_size, embedding_dim, d_model, nhead, num_layers)\n",
    "\n",
    "# Define loss function and optimizer (replace with your choices if needed)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())  # Adjust optimizer hyperparameters if needed\n",
    "\n",
    "# Train the model for 200 epochs\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "  # Forward pass\n",
    "  output = model(X_train_trans)\n",
    "\n",
    "  # Generate random labels for demonstration (replace with your actual labels)\n",
    "  # labels = y_train\n",
    "\n",
    "  # Calculate loss\n",
    "  loss = criterion(output, y_train)\n",
    "\n",
    "  # Backward pass and update weights\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Print training progress (optional)\n",
    "  if (epoch+1) % 10 == 0:\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6d3dd0e-0837-4a61-a2b0-2217973db40c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 12 (1559865555.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[88], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    def train_epoch(self, optimizer, criterion, X_train_trans, y_train, device, batch_size=32):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout), num_layers)\n",
    "        self.linear = nn.Linear(d_model, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ... (same forward method as before)\n",
    "\n",
    "    def train_epoch(self, optimizer, criterion, X_train_trans, y_train, device, batch_size=32):\n",
    "        self.train()  # Set the model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        # Create a temporary DataLoader from the training data\n",
    "        train_dataset = TensorDataset(X_train_trans, y_train)\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "          # ... (same training loop logic as before)\n",
    "\n",
    "        return total_loss / len(train_data_loader)\n",
    "\n",
    "# Example usage\n",
    "model = TransformerClassifier(d_model=256, nhead=4, num_layers=2)\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_train = # Assuming you have your training labels (shape [batch_size])\n",
    "\n",
    "# Train for 10 epochs\n",
    "for epoch in range(10):\n",
    "  loss = model.train_epoch(optimizer, criterion, X_train_trans, y_train, device)\n",
    "  print(f'Average training loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0fb1dde7-603c-497b-b658-33013786cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans_new = torch.zeros((5, 14289, 60))\n",
    "X_train_trans_new[:, :, :59] = X_train_trans\n",
    "\n",
    "X_test_trans_new = torch.zeros((5, 4083, 60))\n",
    "X_test_trans_new[:, :, :59] = X_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09cde6de-bdb7-42b2-91a5-59455602081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1515\n",
      "Epoch [2/100], Loss: 1.1119\n",
      "Epoch [3/100], Loss: 1.7659\n",
      "Epoch [4/100], Loss: 1.1689\n",
      "Epoch [5/100], Loss: 1.1292\n",
      "Epoch [6/100], Loss: 1.1623\n",
      "Epoch [7/100], Loss: 1.1076\n",
      "Epoch [8/100], Loss: 1.0792\n",
      "Epoch [9/100], Loss: 1.0876\n",
      "Epoch [10/100], Loss: 1.0958\n",
      "Epoch [11/100], Loss: 1.0935\n",
      "Epoch [12/100], Loss: 1.0855\n",
      "Epoch [13/100], Loss: 1.0787\n",
      "Epoch [14/100], Loss: 1.0775\n",
      "Epoch [15/100], Loss: 1.0790\n",
      "Epoch [16/100], Loss: 1.0819\n",
      "Epoch [17/100], Loss: 1.0833\n",
      "Epoch [18/100], Loss: 1.0837\n",
      "Epoch [19/100], Loss: 1.0810\n",
      "Epoch [20/100], Loss: 1.0805\n",
      "Epoch [21/100], Loss: 1.0783\n",
      "Epoch [22/100], Loss: 1.0770\n",
      "Epoch [23/100], Loss: 1.0768\n",
      "Epoch [24/100], Loss: 1.0791\n",
      "Epoch [25/100], Loss: 1.0785\n",
      "Epoch [26/100], Loss: 1.0798\n",
      "Epoch [27/100], Loss: 1.0796\n",
      "Epoch [28/100], Loss: 1.0796\n",
      "Epoch [29/100], Loss: 1.0791\n",
      "Epoch [30/100], Loss: 1.0766\n",
      "Epoch [31/100], Loss: 1.0774\n",
      "Epoch [32/100], Loss: 1.0766\n",
      "Epoch [33/100], Loss: 1.0774\n",
      "Epoch [34/100], Loss: 1.0770\n",
      "Epoch [35/100], Loss: 1.0780\n",
      "Epoch [36/100], Loss: 1.0778\n",
      "Epoch [37/100], Loss: 1.0777\n",
      "Epoch [38/100], Loss: 1.0784\n",
      "Epoch [39/100], Loss: 1.0764\n",
      "Epoch [40/100], Loss: 1.0761\n",
      "Epoch [41/100], Loss: 1.0764\n",
      "Epoch [42/100], Loss: 1.0768\n",
      "Epoch [43/100], Loss: 1.0762\n",
      "Epoch [44/100], Loss: 1.0779\n",
      "Epoch [45/100], Loss: 1.0779\n",
      "Epoch [46/100], Loss: 1.0773\n",
      "Epoch [47/100], Loss: 1.0768\n",
      "Epoch [48/100], Loss: 1.0765\n",
      "Epoch [49/100], Loss: 1.0774\n",
      "Epoch [50/100], Loss: 1.0762\n",
      "Epoch [51/100], Loss: 1.0767\n",
      "Epoch [52/100], Loss: 1.0762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m transformer_model(X_train_trans_new)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads), num_layers)\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_length, batch_size, input_size]\n",
    "        transformer_output = self.transformer(x)\n",
    "        # Take only the output of the last time step\n",
    "        last_output = transformer_output[-1, :, :]\n",
    "        # Feed the output of the last time step to the final linear layer\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "# Instantiate the Transformer classifier model\n",
    "input_size = 60  # Number of features in each time step\n",
    "hidden_size = 100  # Hidden size of the transformer\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_rate = 0.0  # Example dropout rate\n",
    "transformer_model = TransformerClassifier(input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.003)\n",
    "\n",
    "# Train the Transformer classifier model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer_model(X_train_trans_new)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb741d3d-a334-42d8-970a-498720e48e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      1.00      0.61      1794\n",
      "           1       0.00      0.00      0.00      1150\n",
      "           2       0.00      0.00      0.00      1139\n",
      "\n",
      "    accuracy                           0.44      4083\n",
      "   macro avg       0.15      0.33      0.20      4083\n",
      "weighted avg       0.19      0.44      0.27      4083\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the classification model\n",
    "transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = transformer_model(X_test_trans_new)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test.numpy(), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92705e32-5a89-4f88-95f5-118fe515d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerClassifier.call().\n\n\u001b[1mDimensions must be equal, but are 59 and 400 for '{{node transformer_classifier_4_1/add_1}} = AddV2[T=DT_FLOAT](transformer_classifier_4_1/layer_normalization_12_1/add_2, transformer_classifier_4_1/dense_15_1/Relu)' with input shapes: [?,5,59], [?,5,400].\u001b[0m\n\nArguments received by TransformerClassifier.call():\n  • x=tf.Tensor(shape=(None, 5, 59), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Train the Transformer classifier model\u001b[39;00m\n\u001b[0;32m     51\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[108], line 27\u001b[0m, in \u001b[0;36mTransformerClassifier.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m     feedforward_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layers[i](feedforward_output)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Add and normalize\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization_layers[i](\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeedforward_output\u001b[49m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Take only the output of the last time step\u001b[39;00m\n\u001b[0;32m     29\u001b[0m last_output \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TransformerClassifier.call().\n\n\u001b[1mDimensions must be equal, but are 59 and 400 for '{{node transformer_classifier_4_1/add_1}} = AddV2[T=DT_FLOAT](transformer_classifier_4_1/layer_normalization_12_1/add_2, transformer_classifier_4_1/dense_15_1/Relu)' with input shapes: [?,5,59], [?,5,400].\u001b[0m\n\nArguments received by TransformerClassifier.call():\n  • x=tf.Tensor(shape=(None, 5, 59), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.attention_layers = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size) for _ in range(num_layers)]\n",
    "        self.dropout_layers = [layers.Dropout(rate=dropout_rate) for _ in range(num_layers)]\n",
    "        self.normalization_layers = [layers.LayerNormalization(epsilon=1e-6) for _ in range(num_layers)]\n",
    "        self.feedforward_layers = [layers.Dense(units=hidden_size*4, activation='relu') for _ in range(num_layers)]\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "    def call(self, x):\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            # Multi-head self-attention\n",
    "            attention_output = self.attention_layers[i](x, x)\n",
    "            # Apply dropout\n",
    "            attention_output = self.dropout_layers[i](attention_output)\n",
    "            # Add and normalize\n",
    "            x = self.normalization_layers[i](x + attention_output)\n",
    "            # Feedforward network\n",
    "            feedforward_output = self.feedforward_layers[i](x)\n",
    "            # Apply dropout\n",
    "            feedforward_output = self.dropout_layers[i](feedforward_output)\n",
    "            # Add and normalize\n",
    "            x = self.normalization_layers[i](x + feedforward_output)\n",
    "        # Take only the output of the last time step\n",
    "        last_output = x[:, -1, :]\n",
    "        # Feed the output of the last time step to the final dense layer\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "# Instantiate the Transformer classifier model\n",
    "input_size = 59  # Number of features in each time step\n",
    "hidden_size = 100  # Hidden size of the transformer\n",
    "num_layers = 4  # Number of transformer layers\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_rate = 0.3  # Example dropout rate\n",
    "transformer_model = TransformerClassifier(input_size, hidden_size, num_layers, num_heads, num_classes, dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "\n",
    "# Compile the model\n",
    "transformer_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train the Transformer classifier model\n",
    "num_epochs = 100\n",
    "transformer_model.fit(X_train, y_train, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b136d-5cf2-44ef-81bd-2c4eb945c3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51baee-3bba-4f36-a506-63dcc467d50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be01cd-14ad-4b81-ac7b-170c8dc27dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc32f3-5e09-42a7-aafa-b29a6b5b60a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b386c-18d4-48c9-93bc-e95158dee670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbfeac-7f8e-48f7-8d7b-7a2bc3600b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82c1b9-5556-4188-aa06-0af00d1d5314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998b34d-4765-446e-821e-3e8ea6c67a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307768a-6f3e-4f96-8290-5c4dd3116a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293e67-f287-4dc8-afe2-de439c483064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b44490-2cc3-4ba3-b39b-f62c9ddc1adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04b4b6-0057-424c-b5b6-d338fb80f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac637c-1837-4c2b-904a-7056a3a551c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c4496-069d-4abd-90b5-7342720fa23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9082c74-1024-4c6e-a250-6751d162b051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c58c67f2-3473-4263-a289-9b95e1122c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 1.1235\n",
      "Epoch [2/300], Loss: 1.1199\n",
      "Epoch [3/300], Loss: 1.1164\n",
      "Epoch [4/300], Loss: 1.1131\n",
      "Epoch [5/300], Loss: 1.1098\n",
      "Epoch [6/300], Loss: 1.1067\n",
      "Epoch [7/300], Loss: 1.1036\n",
      "Epoch [8/300], Loss: 1.1005\n",
      "Epoch [9/300], Loss: 1.0975\n",
      "Epoch [10/300], Loss: 1.0944\n",
      "Epoch [11/300], Loss: 1.0913\n",
      "Epoch [12/300], Loss: 1.0882\n",
      "Epoch [13/300], Loss: 1.0850\n",
      "Epoch [14/300], Loss: 1.0818\n",
      "Epoch [15/300], Loss: 1.0786\n",
      "Epoch [16/300], Loss: 1.0753\n",
      "Epoch [17/300], Loss: 1.0720\n",
      "Epoch [18/300], Loss: 1.0688\n",
      "Epoch [19/300], Loss: 1.0657\n",
      "Epoch [20/300], Loss: 1.0629\n",
      "Epoch [21/300], Loss: 1.0603\n",
      "Epoch [22/300], Loss: 1.0581\n",
      "Epoch [23/300], Loss: 1.0565\n",
      "Epoch [24/300], Loss: 1.0553\n",
      "Epoch [25/300], Loss: 1.0546\n",
      "Epoch [26/300], Loss: 1.0540\n",
      "Epoch [27/300], Loss: 1.0532\n",
      "Epoch [28/300], Loss: 1.0522\n",
      "Epoch [29/300], Loss: 1.0507\n",
      "Epoch [30/300], Loss: 1.0489\n",
      "Epoch [31/300], Loss: 1.0470\n",
      "Epoch [32/300], Loss: 1.0451\n",
      "Epoch [33/300], Loss: 1.0434\n",
      "Epoch [34/300], Loss: 1.0418\n",
      "Epoch [35/300], Loss: 1.0404\n",
      "Epoch [36/300], Loss: 1.0392\n",
      "Epoch [37/300], Loss: 1.0381\n",
      "Epoch [38/300], Loss: 1.0371\n",
      "Epoch [39/300], Loss: 1.0361\n",
      "Epoch [40/300], Loss: 1.0351\n",
      "Epoch [41/300], Loss: 1.0341\n",
      "Epoch [42/300], Loss: 1.0330\n",
      "Epoch [43/300], Loss: 1.0319\n",
      "Epoch [44/300], Loss: 1.0307\n",
      "Epoch [45/300], Loss: 1.0295\n",
      "Epoch [46/300], Loss: 1.0282\n",
      "Epoch [47/300], Loss: 1.0270\n",
      "Epoch [48/300], Loss: 1.0258\n",
      "Epoch [49/300], Loss: 1.0245\n",
      "Epoch [50/300], Loss: 1.0233\n",
      "Epoch [51/300], Loss: 1.0220\n",
      "Epoch [52/300], Loss: 1.0207\n",
      "Epoch [53/300], Loss: 1.0193\n",
      "Epoch [54/300], Loss: 1.0179\n",
      "Epoch [55/300], Loss: 1.0165\n",
      "Epoch [56/300], Loss: 1.0151\n",
      "Epoch [57/300], Loss: 1.0136\n",
      "Epoch [58/300], Loss: 1.0121\n",
      "Epoch [59/300], Loss: 1.0106\n",
      "Epoch [60/300], Loss: 1.0090\n",
      "Epoch [61/300], Loss: 1.0074\n",
      "Epoch [62/300], Loss: 1.0057\n",
      "Epoch [63/300], Loss: 1.0039\n",
      "Epoch [64/300], Loss: 1.0022\n",
      "Epoch [65/300], Loss: 1.0003\n",
      "Epoch [66/300], Loss: 0.9984\n",
      "Epoch [67/300], Loss: 0.9965\n",
      "Epoch [68/300], Loss: 0.9945\n",
      "Epoch [69/300], Loss: 0.9925\n",
      "Epoch [70/300], Loss: 0.9905\n",
      "Epoch [71/300], Loss: 0.9884\n",
      "Epoch [72/300], Loss: 0.9863\n",
      "Epoch [73/300], Loss: 0.9841\n",
      "Epoch [74/300], Loss: 0.9819\n",
      "Epoch [75/300], Loss: 0.9797\n",
      "Epoch [76/300], Loss: 0.9775\n",
      "Epoch [77/300], Loss: 0.9752\n",
      "Epoch [78/300], Loss: 0.9729\n",
      "Epoch [79/300], Loss: 0.9706\n",
      "Epoch [80/300], Loss: 0.9683\n",
      "Epoch [81/300], Loss: 0.9659\n",
      "Epoch [82/300], Loss: 0.9635\n",
      "Epoch [83/300], Loss: 0.9611\n",
      "Epoch [84/300], Loss: 0.9586\n",
      "Epoch [85/300], Loss: 0.9562\n",
      "Epoch [86/300], Loss: 0.9537\n",
      "Epoch [87/300], Loss: 0.9512\n",
      "Epoch [88/300], Loss: 0.9487\n",
      "Epoch [89/300], Loss: 0.9461\n",
      "Epoch [90/300], Loss: 0.9435\n",
      "Epoch [91/300], Loss: 0.9409\n",
      "Epoch [92/300], Loss: 0.9383\n",
      "Epoch [93/300], Loss: 0.9356\n",
      "Epoch [94/300], Loss: 0.9329\n",
      "Epoch [95/300], Loss: 0.9302\n",
      "Epoch [96/300], Loss: 0.9274\n",
      "Epoch [97/300], Loss: 0.9246\n",
      "Epoch [98/300], Loss: 0.9217\n",
      "Epoch [99/300], Loss: 0.9188\n",
      "Epoch [100/300], Loss: 0.9159\n",
      "Epoch [101/300], Loss: 0.9129\n",
      "Epoch [102/300], Loss: 0.9099\n",
      "Epoch [103/300], Loss: 0.9068\n",
      "Epoch [104/300], Loss: 0.9038\n",
      "Epoch [105/300], Loss: 0.9006\n",
      "Epoch [106/300], Loss: 0.8975\n",
      "Epoch [107/300], Loss: 0.8943\n",
      "Epoch [108/300], Loss: 0.8910\n",
      "Epoch [109/300], Loss: 0.8877\n",
      "Epoch [110/300], Loss: 0.8844\n",
      "Epoch [111/300], Loss: 0.8810\n",
      "Epoch [112/300], Loss: 0.8775\n",
      "Epoch [113/300], Loss: 0.8740\n",
      "Epoch [114/300], Loss: 0.8705\n",
      "Epoch [115/300], Loss: 0.8668\n",
      "Epoch [116/300], Loss: 0.8631\n",
      "Epoch [117/300], Loss: 0.8594\n",
      "Epoch [118/300], Loss: 0.8555\n",
      "Epoch [119/300], Loss: 0.8516\n",
      "Epoch [120/300], Loss: 0.8477\n",
      "Epoch [121/300], Loss: 0.8437\n",
      "Epoch [122/300], Loss: 0.8397\n",
      "Epoch [123/300], Loss: 0.8356\n",
      "Epoch [124/300], Loss: 0.8314\n",
      "Epoch [125/300], Loss: 0.8272\n",
      "Epoch [126/300], Loss: 0.8230\n",
      "Epoch [127/300], Loss: 0.8188\n",
      "Epoch [128/300], Loss: 0.8147\n",
      "Epoch [129/300], Loss: 0.8106\n",
      "Epoch [130/300], Loss: 0.8063\n",
      "Epoch [131/300], Loss: 0.8018\n",
      "Epoch [132/300], Loss: 0.7976\n",
      "Epoch [133/300], Loss: 0.7935\n",
      "Epoch [134/300], Loss: 0.7892\n",
      "Epoch [135/300], Loss: 0.7848\n",
      "Epoch [136/300], Loss: 0.7804\n",
      "Epoch [137/300], Loss: 0.7762\n",
      "Epoch [138/300], Loss: 0.7720\n",
      "Epoch [139/300], Loss: 0.7676\n",
      "Epoch [140/300], Loss: 0.7631\n",
      "Epoch [141/300], Loss: 0.7587\n",
      "Epoch [142/300], Loss: 0.7544\n",
      "Epoch [143/300], Loss: 0.7502\n",
      "Epoch [144/300], Loss: 0.7460\n",
      "Epoch [145/300], Loss: 0.7417\n",
      "Epoch [146/300], Loss: 0.7371\n",
      "Epoch [147/300], Loss: 0.7326\n",
      "Epoch [148/300], Loss: 0.7283\n",
      "Epoch [149/300], Loss: 0.7242\n",
      "Epoch [150/300], Loss: 0.7202\n",
      "Epoch [151/300], Loss: 0.7162\n",
      "Epoch [152/300], Loss: 0.7117\n",
      "Epoch [153/300], Loss: 0.7070\n",
      "Epoch [154/300], Loss: 0.7030\n",
      "Epoch [155/300], Loss: 0.6993\n",
      "Epoch [156/300], Loss: 0.6951\n",
      "Epoch [157/300], Loss: 0.6907\n",
      "Epoch [158/300], Loss: 0.6868\n",
      "Epoch [159/300], Loss: 0.6836\n",
      "Epoch [160/300], Loss: 0.6793\n",
      "Epoch [161/300], Loss: 0.6743\n",
      "Epoch [162/300], Loss: 0.6710\n",
      "Epoch [163/300], Loss: 0.6670\n",
      "Epoch [164/300], Loss: 0.6622\n",
      "Epoch [165/300], Loss: 0.6588\n",
      "Epoch [166/300], Loss: 0.6549\n",
      "Epoch [167/300], Loss: 0.6503\n",
      "Epoch [168/300], Loss: 0.6468\n",
      "Epoch [169/300], Loss: 0.6431\n",
      "Epoch [170/300], Loss: 0.6388\n",
      "Epoch [171/300], Loss: 0.6357\n",
      "Epoch [172/300], Loss: 0.6328\n",
      "Epoch [173/300], Loss: 0.6281\n",
      "Epoch [174/300], Loss: 0.6239\n",
      "Epoch [175/300], Loss: 0.6197\n",
      "Epoch [176/300], Loss: 0.6159\n",
      "Epoch [177/300], Loss: 0.6133\n",
      "Epoch [178/300], Loss: 0.6089\n",
      "Epoch [179/300], Loss: 0.6048\n",
      "Epoch [180/300], Loss: 0.6014\n",
      "Epoch [181/300], Loss: 0.5975\n",
      "Epoch [182/300], Loss: 0.5943\n",
      "Epoch [183/300], Loss: 0.5908\n",
      "Epoch [184/300], Loss: 0.5868\n",
      "Epoch [185/300], Loss: 0.5830\n",
      "Epoch [186/300], Loss: 0.5793\n",
      "Epoch [187/300], Loss: 0.5754\n",
      "Epoch [188/300], Loss: 0.5718\n",
      "Epoch [189/300], Loss: 0.5683\n",
      "Epoch [190/300], Loss: 0.5647\n",
      "Epoch [191/300], Loss: 0.5615\n",
      "Epoch [192/300], Loss: 0.5595\n",
      "Epoch [193/300], Loss: 0.5594\n",
      "Epoch [194/300], Loss: 0.5600\n",
      "Epoch [195/300], Loss: 0.5524\n",
      "Epoch [196/300], Loss: 0.5463\n",
      "Epoch [197/300], Loss: 0.5471\n",
      "Epoch [198/300], Loss: 0.5399\n",
      "Epoch [199/300], Loss: 0.5389\n",
      "Epoch [200/300], Loss: 0.5353\n",
      "Epoch [201/300], Loss: 0.5311\n",
      "Epoch [202/300], Loss: 0.5301\n",
      "Epoch [203/300], Loss: 0.5245\n",
      "Epoch [204/300], Loss: 0.5238\n",
      "Epoch [205/300], Loss: 0.5191\n",
      "Epoch [206/300], Loss: 0.5170\n",
      "Epoch [207/300], Loss: 0.5141\n",
      "Epoch [208/300], Loss: 0.5103\n",
      "Epoch [209/300], Loss: 0.5087\n",
      "Epoch [210/300], Loss: 0.5043\n",
      "Epoch [211/300], Loss: 0.5025\n",
      "Epoch [212/300], Loss: 0.4991\n",
      "Epoch [213/300], Loss: 0.4959\n",
      "Epoch [214/300], Loss: 0.4937\n",
      "Epoch [215/300], Loss: 0.4902\n",
      "Epoch [216/300], Loss: 0.4874\n",
      "Epoch [217/300], Loss: 0.4850\n",
      "Epoch [218/300], Loss: 0.4815\n",
      "Epoch [219/300], Loss: 0.4786\n",
      "Epoch [220/300], Loss: 0.4763\n",
      "Epoch [221/300], Loss: 0.4728\n",
      "Epoch [222/300], Loss: 0.4700\n",
      "Epoch [223/300], Loss: 0.4673\n",
      "Epoch [224/300], Loss: 0.4644\n",
      "Epoch [225/300], Loss: 0.4613\n",
      "Epoch [226/300], Loss: 0.4584\n",
      "Epoch [227/300], Loss: 0.4559\n",
      "Epoch [228/300], Loss: 0.4535\n",
      "Epoch [229/300], Loss: 0.4525\n",
      "Epoch [230/300], Loss: 0.4576\n",
      "Epoch [231/300], Loss: 0.4840\n",
      "Epoch [232/300], Loss: 0.4744\n",
      "Epoch [233/300], Loss: 0.4460\n",
      "Epoch [234/300], Loss: 0.4609\n",
      "Epoch [235/300], Loss: 0.4441\n",
      "Epoch [236/300], Loss: 0.4473\n",
      "Epoch [237/300], Loss: 0.4433\n",
      "Epoch [238/300], Loss: 0.4379\n",
      "Epoch [239/300], Loss: 0.4414\n",
      "Epoch [240/300], Loss: 0.4331\n",
      "Epoch [241/300], Loss: 0.4350\n",
      "Epoch [242/300], Loss: 0.4290\n",
      "Epoch [243/300], Loss: 0.4289\n",
      "Epoch [244/300], Loss: 0.4260\n",
      "Epoch [245/300], Loss: 0.4230\n",
      "Epoch [246/300], Loss: 0.4225\n",
      "Epoch [247/300], Loss: 0.4182\n",
      "Epoch [248/300], Loss: 0.4179\n",
      "Epoch [249/300], Loss: 0.4148\n",
      "Epoch [250/300], Loss: 0.4127\n",
      "Epoch [251/300], Loss: 0.4114\n",
      "Epoch [252/300], Loss: 0.4084\n",
      "Epoch [253/300], Loss: 0.4076\n",
      "Epoch [254/300], Loss: 0.4046\n",
      "Epoch [255/300], Loss: 0.4034\n",
      "Epoch [256/300], Loss: 0.4011\n",
      "Epoch [257/300], Loss: 0.3993\n",
      "Epoch [258/300], Loss: 0.3974\n",
      "Epoch [259/300], Loss: 0.3953\n",
      "Epoch [260/300], Loss: 0.3938\n",
      "Epoch [261/300], Loss: 0.3914\n",
      "Epoch [262/300], Loss: 0.3900\n",
      "Epoch [263/300], Loss: 0.3879\n",
      "Epoch [264/300], Loss: 0.3861\n",
      "Epoch [265/300], Loss: 0.3841\n",
      "Epoch [266/300], Loss: 0.3825\n",
      "Epoch [267/300], Loss: 0.3805\n",
      "Epoch [268/300], Loss: 0.3788\n",
      "Epoch [269/300], Loss: 0.3768\n",
      "Epoch [270/300], Loss: 0.3751\n",
      "Epoch [271/300], Loss: 0.3731\n",
      "Epoch [272/300], Loss: 0.3714\n",
      "Epoch [273/300], Loss: 0.3695\n",
      "Epoch [274/300], Loss: 0.3677\n",
      "Epoch [275/300], Loss: 0.3658\n",
      "Epoch [276/300], Loss: 0.3640\n",
      "Epoch [277/300], Loss: 0.3622\n",
      "Epoch [278/300], Loss: 0.3604\n",
      "Epoch [279/300], Loss: 0.3585\n",
      "Epoch [280/300], Loss: 0.3567\n",
      "Epoch [281/300], Loss: 0.3549\n",
      "Epoch [282/300], Loss: 0.3530\n",
      "Epoch [283/300], Loss: 0.3512\n",
      "Epoch [284/300], Loss: 0.3494\n",
      "Epoch [285/300], Loss: 0.3476\n",
      "Epoch [286/300], Loss: 0.3458\n",
      "Epoch [287/300], Loss: 0.3440\n",
      "Epoch [288/300], Loss: 0.3422\n",
      "Epoch [289/300], Loss: 0.3404\n",
      "Epoch [290/300], Loss: 0.3387\n",
      "Epoch [291/300], Loss: 0.3370\n",
      "Epoch [292/300], Loss: 0.3355\n",
      "Epoch [293/300], Loss: 0.3343\n",
      "Epoch [294/300], Loss: 0.3343\n",
      "Epoch [295/300], Loss: 0.3398\n",
      "Epoch [296/300], Loss: 0.3761\n",
      "Epoch [297/300], Loss: 0.4224\n",
      "Epoch [298/300], Loss: 0.3438\n",
      "Epoch [299/300], Loss: 0.3819\n",
      "Epoch [300/300], Loss: 0.3520\n"
     ]
    }
   ],
   "source": [
    "# Example sequential measurement data\n",
    "# Assuming each sequence has 5 time steps and 59 features\n",
    "# X = np.random.randn(14289, 5, 59)\n",
    "X_train = torch.tensor(X_train_input, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_input, dtype=torch.float32)\n",
    "\n",
    "# Example classification labels\n",
    "# y = np.random.randint(0, 3, 14289)  # Assuming three classes\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define LSTM model for classification\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instantiate the classification model\n",
    "input_size = 59  # Number of features in each time step\n",
    "hidden_size = 50  # Number of LSTM units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 3  # Number of output classes\n",
    "classification_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classification model\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classification_model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a885de0-f6f1-4d93-90b7-65726e8c35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\AppData\\Local\\Temp\\ipykernel_16240\\1965860993.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
      "C:\\Users\\ngqin\\AppData\\Local\\Temp\\ipykernel_16240\\1965860993.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1044\n",
      "Epoch [2/200], Loss: 1.1021\n",
      "Epoch [3/200], Loss: 1.0995\n",
      "Epoch [4/200], Loss: 1.0975\n",
      "Epoch [5/200], Loss: 1.0949\n",
      "Epoch [6/200], Loss: 1.0929\n",
      "Epoch [7/200], Loss: 1.0905\n",
      "Epoch [8/200], Loss: 1.0886\n",
      "Epoch [9/200], Loss: 1.0867\n",
      "Epoch [10/200], Loss: 1.0839\n",
      "Epoch [11/200], Loss: 1.0829\n",
      "Epoch [12/200], Loss: 1.0802\n",
      "Epoch [13/200], Loss: 1.0785\n",
      "Epoch [14/200], Loss: 1.0769\n",
      "Epoch [15/200], Loss: 1.0750\n",
      "Epoch [16/200], Loss: 1.0737\n",
      "Epoch [17/200], Loss: 1.0733\n",
      "Epoch [18/200], Loss: 1.0723\n",
      "Epoch [19/200], Loss: 1.0731\n",
      "Epoch [20/200], Loss: 1.0735\n",
      "Epoch [21/200], Loss: 1.0731\n",
      "Epoch [22/200], Loss: 1.0722\n",
      "Epoch [23/200], Loss: 1.0701\n",
      "Epoch [24/200], Loss: 1.0685\n",
      "Epoch [25/200], Loss: 1.0662\n",
      "Epoch [26/200], Loss: 1.0650\n",
      "Epoch [27/200], Loss: 1.0643\n",
      "Epoch [28/200], Loss: 1.0629\n",
      "Epoch [29/200], Loss: 1.0608\n",
      "Epoch [30/200], Loss: 1.0591\n",
      "Epoch [31/200], Loss: 1.0574\n",
      "Epoch [32/200], Loss: 1.0570\n",
      "Epoch [33/200], Loss: 1.0542\n",
      "Epoch [34/200], Loss: 1.0530\n",
      "Epoch [35/200], Loss: 1.0522\n",
      "Epoch [36/200], Loss: 1.0518\n",
      "Epoch [37/200], Loss: 1.0497\n",
      "Epoch [38/200], Loss: 1.0483\n",
      "Epoch [39/200], Loss: 1.0469\n",
      "Epoch [40/200], Loss: 1.0458\n",
      "Epoch [41/200], Loss: 1.0452\n",
      "Epoch [42/200], Loss: 1.0441\n",
      "Epoch [43/200], Loss: 1.0420\n",
      "Epoch [44/200], Loss: 1.0422\n",
      "Epoch [45/200], Loss: 1.0397\n",
      "Epoch [46/200], Loss: 1.0393\n",
      "Epoch [47/200], Loss: 1.0376\n",
      "Epoch [48/200], Loss: 1.0370\n",
      "Epoch [49/200], Loss: 1.0338\n",
      "Epoch [50/200], Loss: 1.0326\n",
      "Epoch [51/200], Loss: 1.0316\n",
      "Epoch [52/200], Loss: 1.0300\n",
      "Epoch [53/200], Loss: 1.0279\n",
      "Epoch [54/200], Loss: 1.0262\n",
      "Epoch [55/200], Loss: 1.0250\n",
      "Epoch [56/200], Loss: 1.0232\n",
      "Epoch [57/200], Loss: 1.0221\n",
      "Epoch [58/200], Loss: 1.0202\n",
      "Epoch [59/200], Loss: 1.0190\n",
      "Epoch [60/200], Loss: 1.0162\n",
      "Epoch [61/200], Loss: 1.0145\n",
      "Epoch [62/200], Loss: 1.0130\n",
      "Epoch [63/200], Loss: 1.0105\n",
      "Epoch [64/200], Loss: 1.0088\n",
      "Epoch [65/200], Loss: 1.0067\n",
      "Epoch [66/200], Loss: 1.0040\n",
      "Epoch [67/200], Loss: 1.0026\n",
      "Epoch [68/200], Loss: 1.0005\n",
      "Epoch [69/200], Loss: 0.9990\n",
      "Epoch [70/200], Loss: 0.9964\n",
      "Epoch [71/200], Loss: 0.9950\n",
      "Epoch [72/200], Loss: 0.9927\n",
      "Epoch [73/200], Loss: 0.9895\n",
      "Epoch [74/200], Loss: 0.9875\n",
      "Epoch [75/200], Loss: 0.9845\n",
      "Epoch [76/200], Loss: 0.9816\n",
      "Epoch [77/200], Loss: 0.9787\n",
      "Epoch [78/200], Loss: 0.9766\n",
      "Epoch [79/200], Loss: 0.9745\n",
      "Epoch [80/200], Loss: 0.9712\n",
      "Epoch [81/200], Loss: 0.9694\n",
      "Epoch [82/200], Loss: 0.9675\n",
      "Epoch [83/200], Loss: 0.9639\n",
      "Epoch [84/200], Loss: 0.9616\n",
      "Epoch [85/200], Loss: 0.9613\n",
      "Epoch [86/200], Loss: 0.9568\n",
      "Epoch [87/200], Loss: 0.9554\n",
      "Epoch [88/200], Loss: 0.9525\n",
      "Epoch [89/200], Loss: 0.9503\n",
      "Epoch [90/200], Loss: 0.9465\n",
      "Epoch [91/200], Loss: 0.9455\n",
      "Epoch [92/200], Loss: 0.9431\n",
      "Epoch [93/200], Loss: 0.9398\n",
      "Epoch [94/200], Loss: 0.9361\n",
      "Epoch [95/200], Loss: 0.9332\n",
      "Epoch [96/200], Loss: 0.9309\n",
      "Epoch [97/200], Loss: 0.9288\n",
      "Epoch [98/200], Loss: 0.9258\n",
      "Epoch [99/200], Loss: 0.9224\n",
      "Epoch [100/200], Loss: 0.9205\n",
      "Epoch [101/200], Loss: 0.9173\n",
      "Epoch [102/200], Loss: 0.9118\n",
      "Epoch [103/200], Loss: 0.9095\n",
      "Epoch [104/200], Loss: 0.9067\n",
      "Epoch [105/200], Loss: 0.9031\n",
      "Epoch [106/200], Loss: 0.9011\n",
      "Epoch [107/200], Loss: 0.8975\n",
      "Epoch [108/200], Loss: 0.8937\n",
      "Epoch [109/200], Loss: 0.8905\n",
      "Epoch [110/200], Loss: 0.8872\n",
      "Epoch [111/200], Loss: 0.8846\n",
      "Epoch [112/200], Loss: 0.8799\n",
      "Epoch [113/200], Loss: 0.8779\n",
      "Epoch [114/200], Loss: 0.8756\n",
      "Epoch [115/200], Loss: 0.8709\n",
      "Epoch [116/200], Loss: 0.8677\n",
      "Epoch [117/200], Loss: 0.8632\n",
      "Epoch [118/200], Loss: 0.8615\n",
      "Epoch [119/200], Loss: 0.8592\n",
      "Epoch [120/200], Loss: 0.8536\n",
      "Epoch [121/200], Loss: 0.8509\n",
      "Epoch [122/200], Loss: 0.8464\n",
      "Epoch [123/200], Loss: 0.8421\n",
      "Epoch [124/200], Loss: 0.8394\n",
      "Epoch [125/200], Loss: 0.8358\n",
      "Epoch [126/200], Loss: 0.8353\n",
      "Epoch [127/200], Loss: 0.8360\n",
      "Epoch [128/200], Loss: 0.8336\n",
      "Epoch [129/200], Loss: 0.8222\n",
      "Epoch [130/200], Loss: 0.8240\n",
      "Epoch [131/200], Loss: 0.8199\n",
      "Epoch [132/200], Loss: 0.8139\n",
      "Epoch [133/200], Loss: 0.8134\n",
      "Epoch [134/200], Loss: 0.8090\n",
      "Epoch [135/200], Loss: 0.8036\n",
      "Epoch [136/200], Loss: 0.8031\n",
      "Epoch [137/200], Loss: 0.7996\n",
      "Epoch [138/200], Loss: 0.7962\n",
      "Epoch [139/200], Loss: 0.7920\n",
      "Epoch [140/200], Loss: 0.7898\n",
      "Epoch [141/200], Loss: 0.7834\n",
      "Epoch [142/200], Loss: 0.7811\n",
      "Epoch [143/200], Loss: 0.7770\n",
      "Epoch [144/200], Loss: 0.7740\n",
      "Epoch [145/200], Loss: 0.7693\n",
      "Epoch [146/200], Loss: 0.7687\n",
      "Epoch [147/200], Loss: 0.7670\n",
      "Epoch [148/200], Loss: 0.7670\n",
      "Epoch [149/200], Loss: 0.7687\n",
      "Epoch [150/200], Loss: 0.7640\n",
      "Epoch [151/200], Loss: 0.7510\n",
      "Epoch [152/200], Loss: 0.7515\n",
      "Epoch [153/200], Loss: 0.7503\n",
      "Epoch [154/200], Loss: 0.7434\n",
      "Epoch [155/200], Loss: 0.7431\n",
      "Epoch [156/200], Loss: 0.7372\n",
      "Epoch [157/200], Loss: 0.7323\n",
      "Epoch [158/200], Loss: 0.7326\n",
      "Epoch [159/200], Loss: 0.7282\n",
      "Epoch [160/200], Loss: 0.7240\n",
      "Epoch [161/200], Loss: 0.7210\n",
      "Epoch [162/200], Loss: 0.7187\n",
      "Epoch [163/200], Loss: 0.7148\n",
      "Epoch [164/200], Loss: 0.7112\n",
      "Epoch [165/200], Loss: 0.7109\n",
      "Epoch [166/200], Loss: 0.7038\n",
      "Epoch [167/200], Loss: 0.7014\n",
      "Epoch [168/200], Loss: 0.7002\n",
      "Epoch [169/200], Loss: 0.6949\n",
      "Epoch [170/200], Loss: 0.6943\n",
      "Epoch [171/200], Loss: 0.6887\n",
      "Epoch [172/200], Loss: 0.6883\n",
      "Epoch [173/200], Loss: 0.6850\n",
      "Epoch [174/200], Loss: 0.6859\n",
      "Epoch [175/200], Loss: 0.6878\n",
      "Epoch [176/200], Loss: 0.7017\n",
      "Epoch [177/200], Loss: 0.7007\n",
      "Epoch [178/200], Loss: 0.7015\n",
      "Epoch [179/200], Loss: 0.6778\n",
      "Epoch [180/200], Loss: 0.6810\n",
      "Epoch [181/200], Loss: 0.6833\n",
      "Epoch [182/200], Loss: 0.6747\n",
      "Epoch [183/200], Loss: 0.6725\n",
      "Epoch [184/200], Loss: 0.6667\n",
      "Epoch [185/200], Loss: 0.6644\n",
      "Epoch [186/200], Loss: 0.6657\n",
      "Epoch [187/200], Loss: 0.6576\n",
      "Epoch [188/200], Loss: 0.6623\n",
      "Epoch [189/200], Loss: 0.6539\n",
      "Epoch [190/200], Loss: 0.6582\n",
      "Epoch [191/200], Loss: 0.6498\n",
      "Epoch [192/200], Loss: 0.6477\n",
      "Epoch [193/200], Loss: 0.6483\n",
      "Epoch [194/200], Loss: 0.6426\n",
      "Epoch [195/200], Loss: 0.6426\n",
      "Epoch [196/200], Loss: 0.6387\n",
      "Epoch [197/200], Loss: 0.6386\n",
      "Epoch [198/200], Loss: 0.6341\n",
      "Epoch [199/200], Loss: 0.6327\n",
      "Epoch [200/200], Loss: 0.6319\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc142982-bf5b-495e-ba65-6923a4611604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[858 355 581]\n",
      " [451 252 447]\n",
      " [326 202 611]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test.numpy(), predicted)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "636ccb24-5ec5-4b69-9e69-64a8971935d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4664  674  910]\n",
      " [ 770 2176 1252]\n",
      " [  76  319 3448]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_train)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train.numpy(), predicted)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66c61b12-1b01-4cda-8d11-7c1a8e81d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1140\n",
      "Epoch [2/200], Loss: 1.0965\n",
      "Epoch [3/200], Loss: 1.0841\n",
      "Epoch [4/200], Loss: 1.0766\n",
      "Epoch [5/200], Loss: 1.0736\n",
      "Epoch [6/200], Loss: 1.0737\n",
      "Epoch [7/200], Loss: 1.0729\n",
      "Epoch [8/200], Loss: 1.0702\n",
      "Epoch [9/200], Loss: 1.0673\n",
      "Epoch [10/200], Loss: 1.0645\n",
      "Epoch [11/200], Loss: 1.0623\n",
      "Epoch [12/200], Loss: 1.0595\n",
      "Epoch [13/200], Loss: 1.0573\n",
      "Epoch [14/200], Loss: 1.0583\n",
      "Epoch [15/200], Loss: 1.0556\n",
      "Epoch [16/200], Loss: 1.0535\n",
      "Epoch [17/200], Loss: 1.0536\n",
      "Epoch [18/200], Loss: 1.0509\n",
      "Epoch [19/200], Loss: 1.0521\n",
      "Epoch [20/200], Loss: 1.0513\n",
      "Epoch [21/200], Loss: 1.0507\n",
      "Epoch [22/200], Loss: 1.0503\n",
      "Epoch [23/200], Loss: 1.0482\n",
      "Epoch [24/200], Loss: 1.0488\n",
      "Epoch [25/200], Loss: 1.0480\n",
      "Epoch [26/200], Loss: 1.0497\n",
      "Epoch [27/200], Loss: 1.0473\n",
      "Epoch [28/200], Loss: 1.0453\n",
      "Epoch [29/200], Loss: 1.0462\n",
      "Epoch [30/200], Loss: 1.0452\n",
      "Epoch [31/200], Loss: 1.0451\n",
      "Epoch [32/200], Loss: 1.0440\n",
      "Epoch [33/200], Loss: 1.0427\n",
      "Epoch [34/200], Loss: 1.0439\n",
      "Epoch [35/200], Loss: 1.0422\n",
      "Epoch [36/200], Loss: 1.0415\n",
      "Epoch [37/200], Loss: 1.0415\n",
      "Epoch [38/200], Loss: 1.0399\n",
      "Epoch [39/200], Loss: 1.0403\n",
      "Epoch [40/200], Loss: 1.0388\n",
      "Epoch [41/200], Loss: 1.0381\n",
      "Epoch [42/200], Loss: 1.0372\n",
      "Epoch [43/200], Loss: 1.0366\n",
      "Epoch [44/200], Loss: 1.0355\n",
      "Epoch [45/200], Loss: 1.0362\n",
      "Epoch [46/200], Loss: 1.0339\n",
      "Epoch [47/200], Loss: 1.0320\n",
      "Epoch [48/200], Loss: 1.0310\n",
      "Epoch [49/200], Loss: 1.0314\n",
      "Epoch [50/200], Loss: 1.0302\n",
      "Epoch [51/200], Loss: 1.0286\n",
      "Epoch [52/200], Loss: 1.0270\n",
      "Epoch [53/200], Loss: 1.0264\n",
      "Epoch [54/200], Loss: 1.0250\n",
      "Epoch [55/200], Loss: 1.0243\n",
      "Epoch [56/200], Loss: 1.0215\n",
      "Epoch [57/200], Loss: 1.0200\n",
      "Epoch [58/200], Loss: 1.0185\n",
      "Epoch [59/200], Loss: 1.0166\n",
      "Epoch [60/200], Loss: 1.0133\n",
      "Epoch [61/200], Loss: 1.0108\n",
      "Epoch [62/200], Loss: 1.0078\n",
      "Epoch [63/200], Loss: 1.0081\n",
      "Epoch [64/200], Loss: 1.0056\n",
      "Epoch [65/200], Loss: 1.0031\n",
      "Epoch [66/200], Loss: 1.0006\n",
      "Epoch [67/200], Loss: 1.0003\n",
      "Epoch [68/200], Loss: 0.9950\n",
      "Epoch [69/200], Loss: 0.9946\n",
      "Epoch [70/200], Loss: 0.9943\n",
      "Epoch [71/200], Loss: 0.9912\n",
      "Epoch [72/200], Loss: 0.9879\n",
      "Epoch [73/200], Loss: 0.9877\n",
      "Epoch [74/200], Loss: 0.9834\n",
      "Epoch [75/200], Loss: 0.9825\n",
      "Epoch [76/200], Loss: 0.9822\n",
      "Epoch [77/200], Loss: 0.9792\n",
      "Epoch [78/200], Loss: 0.9758\n",
      "Epoch [79/200], Loss: 0.9758\n",
      "Epoch [80/200], Loss: 0.9731\n",
      "Epoch [81/200], Loss: 0.9717\n",
      "Epoch [82/200], Loss: 0.9684\n",
      "Epoch [83/200], Loss: 0.9684\n",
      "Epoch [84/200], Loss: 0.9659\n",
      "Epoch [85/200], Loss: 0.9638\n",
      "Epoch [86/200], Loss: 0.9629\n",
      "Epoch [87/200], Loss: 0.9608\n",
      "Epoch [88/200], Loss: 0.9600\n",
      "Epoch [89/200], Loss: 0.9560\n",
      "Epoch [90/200], Loss: 0.9555\n",
      "Epoch [91/200], Loss: 0.9517\n",
      "Epoch [92/200], Loss: 0.9525\n",
      "Epoch [93/200], Loss: 0.9505\n",
      "Epoch [94/200], Loss: 0.9491\n",
      "Epoch [95/200], Loss: 0.9467\n",
      "Epoch [96/200], Loss: 0.9463\n",
      "Epoch [97/200], Loss: 0.9445\n",
      "Epoch [98/200], Loss: 0.9428\n",
      "Epoch [99/200], Loss: 0.9397\n",
      "Epoch [100/200], Loss: 0.9397\n",
      "Epoch [101/200], Loss: 0.9384\n",
      "Epoch [102/200], Loss: 0.9366\n",
      "Epoch [103/200], Loss: 0.9332\n",
      "Epoch [104/200], Loss: 0.9327\n",
      "Epoch [105/200], Loss: 0.9316\n",
      "Epoch [106/200], Loss: 0.9301\n",
      "Epoch [107/200], Loss: 0.9276\n",
      "Epoch [108/200], Loss: 0.9249\n",
      "Epoch [109/200], Loss: 0.9251\n",
      "Epoch [110/200], Loss: 0.9231\n",
      "Epoch [111/200], Loss: 0.9222\n",
      "Epoch [112/200], Loss: 0.9199\n",
      "Epoch [113/200], Loss: 0.9184\n",
      "Epoch [114/200], Loss: 0.9168\n",
      "Epoch [115/200], Loss: 0.9156\n",
      "Epoch [116/200], Loss: 0.9146\n",
      "Epoch [117/200], Loss: 0.9136\n",
      "Epoch [118/200], Loss: 0.9118\n",
      "Epoch [119/200], Loss: 0.9094\n",
      "Epoch [120/200], Loss: 0.9054\n",
      "Epoch [121/200], Loss: 0.9051\n",
      "Epoch [122/200], Loss: 0.9031\n",
      "Epoch [123/200], Loss: 0.9047\n",
      "Epoch [124/200], Loss: 0.9004\n",
      "Epoch [125/200], Loss: 0.8994\n",
      "Epoch [126/200], Loss: 0.8976\n",
      "Epoch [127/200], Loss: 0.8957\n",
      "Epoch [128/200], Loss: 0.8948\n",
      "Epoch [129/200], Loss: 0.8940\n",
      "Epoch [130/200], Loss: 0.8909\n",
      "Epoch [131/200], Loss: 0.8890\n",
      "Epoch [132/200], Loss: 0.8897\n",
      "Epoch [133/200], Loss: 0.8878\n",
      "Epoch [134/200], Loss: 0.8855\n",
      "Epoch [135/200], Loss: 0.8851\n",
      "Epoch [136/200], Loss: 0.8803\n",
      "Epoch [137/200], Loss: 0.8807\n",
      "Epoch [138/200], Loss: 0.8801\n",
      "Epoch [139/200], Loss: 0.8826\n",
      "Epoch [140/200], Loss: 0.8793\n",
      "Epoch [141/200], Loss: 0.8812\n",
      "Epoch [142/200], Loss: 0.8830\n",
      "Epoch [143/200], Loss: 0.8756\n",
      "Epoch [144/200], Loss: 0.8711\n",
      "Epoch [145/200], Loss: 0.8761\n",
      "Epoch [146/200], Loss: 0.8681\n",
      "Epoch [147/200], Loss: 0.8678\n",
      "Epoch [148/200], Loss: 0.8688\n",
      "Epoch [149/200], Loss: 0.8642\n",
      "Epoch [150/200], Loss: 0.8642\n",
      "Epoch [151/200], Loss: 0.8642\n",
      "Epoch [152/200], Loss: 0.8611\n",
      "Epoch [153/200], Loss: 0.8624\n",
      "Epoch [154/200], Loss: 0.8599\n",
      "Epoch [155/200], Loss: 0.8593\n",
      "Epoch [156/200], Loss: 0.8565\n",
      "Epoch [157/200], Loss: 0.8567\n",
      "Epoch [158/200], Loss: 0.8545\n",
      "Epoch [159/200], Loss: 0.8508\n",
      "Epoch [160/200], Loss: 0.8522\n",
      "Epoch [161/200], Loss: 0.8489\n",
      "Epoch [162/200], Loss: 0.8500\n",
      "Epoch [163/200], Loss: 0.8465\n",
      "Epoch [164/200], Loss: 0.8439\n",
      "Epoch [165/200], Loss: 0.8461\n",
      "Epoch [166/200], Loss: 0.8422\n",
      "Epoch [167/200], Loss: 0.8428\n",
      "Epoch [168/200], Loss: 0.8408\n",
      "Epoch [169/200], Loss: 0.8400\n",
      "Epoch [170/200], Loss: 0.8411\n",
      "Epoch [171/200], Loss: 0.8366\n",
      "Epoch [172/200], Loss: 0.8350\n",
      "Epoch [173/200], Loss: 0.8351\n",
      "Epoch [174/200], Loss: 0.8316\n",
      "Epoch [175/200], Loss: 0.8302\n",
      "Epoch [176/200], Loss: 0.8315\n",
      "Epoch [177/200], Loss: 0.8303\n",
      "Epoch [178/200], Loss: 0.8283\n",
      "Epoch [179/200], Loss: 0.8281\n",
      "Epoch [180/200], Loss: 0.8269\n",
      "Epoch [181/200], Loss: 0.8255\n",
      "Epoch [182/200], Loss: 0.8255\n",
      "Epoch [183/200], Loss: 0.8251\n",
      "Epoch [184/200], Loss: 0.8226\n",
      "Epoch [185/200], Loss: 0.8220\n",
      "Epoch [186/200], Loss: 0.8236\n",
      "Epoch [187/200], Loss: 0.8226\n",
      "Epoch [188/200], Loss: 0.8211\n",
      "Epoch [189/200], Loss: 0.8177\n",
      "Epoch [190/200], Loss: 0.8149\n",
      "Epoch [191/200], Loss: 0.8176\n",
      "Epoch [192/200], Loss: 0.8165\n",
      "Epoch [193/200], Loss: 0.8106\n",
      "Epoch [194/200], Loss: 0.8126\n",
      "Epoch [195/200], Loss: 0.8118\n",
      "Epoch [196/200], Loss: 0.8100\n",
      "Epoch [197/200], Loss: 0.8058\n",
      "Epoch [198/200], Loss: 0.8087\n",
      "Epoch [199/200], Loss: 0.8079\n",
      "Epoch [200/200], Loss: 0.8044\n"
     ]
    }
   ],
   "source": [
    "# Define RNN model for classification\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate=0.0):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Defining dropout layer with specified dropout rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Applying dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instantiate the classification model\n",
    "input_size = 59  # Number of features in each time step\n",
    "hidden_size = 50  # Number of RNN units\n",
    "num_layers = 4  # Number of RNN layers\n",
    "num_classes = 3  # Number of output classes\n",
    "dropout_rate = 0.5  # Example dropout rate\n",
    "classification_model = RNNClassifier(input_size, hidden_size, num_layers, num_classes, dropout_rate)\n",
    "# classification_model = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classification model\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classification_model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2304899-26d7-4a52-807b-8e776704b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.60      0.56      1794\n",
      "           1       0.30      0.19      0.23      1150\n",
      "           2       0.42      0.47      0.44      1139\n",
      "\n",
      "    accuracy                           0.45      4083\n",
      "   macro avg       0.41      0.42      0.41      4083\n",
      "weighted avg       0.43      0.45      0.43      4083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test.numpy(), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc9930e7-d4d1-4ca6-a6e1-4b765969cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1080  331  383]\n",
      " [ 580  220  350]\n",
      " [ 413  193  533]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the classification model\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classification_model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted tensor to numpy array\n",
    "predicted = predicted.numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test.numpy(), predicted)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2b6e6f44-1861-46f3-a0a8-b17c35415811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2430170774459839\n",
      "Epoch [2/10], Loss: 1.2445777654647827\n",
      "Epoch [3/10], Loss: 1.2082958221435547\n",
      "Epoch [4/10], Loss: 1.2565702199935913\n",
      "Epoch [5/10], Loss: 1.2094172239303589\n",
      "Epoch [6/10], Loss: 1.1871225833892822\n",
      "Epoch [7/10], Loss: 1.215932846069336\n",
      "Epoch [8/10], Loss: 1.208574891090393\n",
      "Epoch [9/10], Loss: 1.1900966167449951\n",
      "Epoch [10/10], Loss: 1.2072184085845947\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 58\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mX_test\u001b[49m))\n\u001b[0;32m     59\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     60\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_test, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m y_test\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 59\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.tensor(time_input, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        inputs = X_train_tensor[i:i+batch_size]\n",
    "        targets = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor(X_test))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == torch.argmax(y_test, dim=1)).sum().item() / y_test.size(0)\n",
    "    print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0f619-94db-4e4d-8cd3-eca024db5592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "be623a2e-9cf4-44af-8607-88359e6c6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14289,)\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randint(0, 3, 14289)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4c418752-2c74-422d-9825-fec41b2f96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.backend' has no attribute 'convert_to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\keras\\src\\utils\\progbar.py:162\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    160\u001b[0m info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_numpy\u001b[49m(\n\u001b[0;32m    163\u001b[0m         backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m    164\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    167\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(avg)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(avg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-3\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.backend' has no attribute 'convert_to_numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Example input and output shapes\n",
    "# input_shape = (14289, 5, 60)\n",
    "# output_shape = (14289,)\n",
    "\n",
    "# # Generate example data\n",
    "# X = np.random.random(input_shape)\n",
    "# y = np.random.randint(0, 3, output_shape)\n",
    "\n",
    "# # Convert labels to one-hot encoding\n",
    "# y = to_categorical(y, num_classes=3)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(5, 59)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(time_input, labels, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95be201-c00f-499a-aefc-9d0d3d7b67be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0d66b-56e3-4016-beec-91fa7f48241c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f14512-a9b8-48f7-997f-09dffd59f2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9056a-fa4b-4eee-a2e9-f20e7007dd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6035b-4aac-4869-9276-b265d1ac85c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1b8a0e-53d6-4efb-ad67-30810f9d4064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.2754\n",
      "Epoch [2/50], Loss: 1.2713\n",
      "Epoch [3/50], Loss: 1.2676\n",
      "Epoch [4/50], Loss: 1.2642\n",
      "Epoch [5/50], Loss: 1.2611\n",
      "Epoch [6/50], Loss: 1.2582\n",
      "Epoch [7/50], Loss: 1.2556\n",
      "Epoch [8/50], Loss: 1.2532\n",
      "Epoch [9/50], Loss: 1.2510\n",
      "Epoch [10/50], Loss: 1.2489\n",
      "Epoch [11/50], Loss: 1.2470\n",
      "Epoch [12/50], Loss: 1.2452\n",
      "Epoch [13/50], Loss: 1.2433\n",
      "Epoch [14/50], Loss: 1.2415\n",
      "Epoch [15/50], Loss: 1.2396\n",
      "Epoch [16/50], Loss: 1.2375\n",
      "Epoch [17/50], Loss: 1.2354\n",
      "Epoch [18/50], Loss: 1.2332\n",
      "Epoch [19/50], Loss: 1.2309\n",
      "Epoch [20/50], Loss: 1.2284\n",
      "Epoch [21/50], Loss: 1.2259\n",
      "Epoch [22/50], Loss: 1.2234\n",
      "Epoch [23/50], Loss: 1.2207\n",
      "Epoch [24/50], Loss: 1.2180\n",
      "Epoch [25/50], Loss: 1.2152\n",
      "Epoch [26/50], Loss: 1.2123\n",
      "Epoch [27/50], Loss: 1.2093\n",
      "Epoch [28/50], Loss: 1.2063\n",
      "Epoch [29/50], Loss: 1.2031\n",
      "Epoch [30/50], Loss: 1.1998\n",
      "Epoch [31/50], Loss: 1.1964\n",
      "Epoch [32/50], Loss: 1.1929\n",
      "Epoch [33/50], Loss: 1.1893\n",
      "Epoch [34/50], Loss: 1.1858\n",
      "Epoch [35/50], Loss: 1.1822\n",
      "Epoch [36/50], Loss: 1.1787\n",
      "Epoch [37/50], Loss: 1.1752\n",
      "Epoch [38/50], Loss: 1.1718\n",
      "Epoch [39/50], Loss: 1.1685\n",
      "Epoch [40/50], Loss: 1.1651\n",
      "Epoch [41/50], Loss: 1.1618\n",
      "Epoch [42/50], Loss: 1.1585\n",
      "Epoch [43/50], Loss: 1.1552\n",
      "Epoch [44/50], Loss: 1.1517\n",
      "Epoch [45/50], Loss: 1.1481\n",
      "Epoch [46/50], Loss: 1.1442\n",
      "Epoch [47/50], Loss: 1.1402\n",
      "Epoch [48/50], Loss: 1.1362\n",
      "Epoch [49/50], Loss: 1.1321\n",
      "Epoch [50/50], Loss: 1.1280\n",
      "Mean Squared Error on Test Data: 1.1239392757415771\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Example sequential measurement data\n",
    "# Assuming each sequence has 10 time steps and 3 features\n",
    "X = np.random.randn(100, 10, 3)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Example regression labels\n",
    "y = np.random.randn(100, 1)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Using only the last time step's output\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 3  # Number of features in each time step\n",
    "hidden_size = 50  # Number of LSTM units\n",
    "output_size = 1  # Output size (single regression value)\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    print(\"Mean Squared Error on Test Data:\", loss.item())\n",
    "\n",
    "# Make predictions\n",
    "predictions = outputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f291c55d-a01c-4a08-8a35-f0fa22d39e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53983414],\n",
       "       [ 0.02595963],\n",
       "       [ 0.4001707 ],\n",
       "       [ 0.35539943],\n",
       "       [ 0.69263095],\n",
       "       [ 0.458923  ],\n",
       "       [ 0.45358214],\n",
       "       [ 0.6972333 ],\n",
       "       [ 0.01918528],\n",
       "       [-0.64251935],\n",
       "       [-0.04402913],\n",
       "       [-0.0785234 ],\n",
       "       [-0.08128691],\n",
       "       [ 0.5547164 ],\n",
       "       [ 0.09887768],\n",
       "       [ 0.4697313 ],\n",
       "       [ 0.26392022],\n",
       "       [ 0.0141659 ],\n",
       "       [-0.01234652],\n",
       "       [ 0.3830874 ],\n",
       "       [-0.3867787 ],\n",
       "       [ 0.52545273],\n",
       "       [ 0.43124145],\n",
       "       [ 0.0906188 ],\n",
       "       [-0.23529367],\n",
       "       [-0.1234112 ],\n",
       "       [-0.27464283],\n",
       "       [ 0.10087299],\n",
       "       [ 0.4842308 ],\n",
       "       [ 0.1294104 ],\n",
       "       [-0.07540329],\n",
       "       [ 0.20075747],\n",
       "       [-0.47554547],\n",
       "       [ 0.11421708],\n",
       "       [ 0.6245119 ],\n",
       "       [ 0.5454791 ],\n",
       "       [-0.3282947 ],\n",
       "       [ 0.05051087],\n",
       "       [ 0.39321065],\n",
       "       [-0.03295558],\n",
       "       [ 0.33816934],\n",
       "       [ 0.47598463],\n",
       "       [ 0.09506223],\n",
       "       [-0.00884948],\n",
       "       [ 0.41988903],\n",
       "       [-0.04274533],\n",
       "       [-0.06416123],\n",
       "       [ 0.24925269],\n",
       "       [-0.06592648],\n",
       "       [-0.09784995],\n",
       "       [ 0.13916703],\n",
       "       [-0.01446067],\n",
       "       [-0.3062598 ],\n",
       "       [-0.06122676],\n",
       "       [-0.05221102],\n",
       "       [-0.25737226],\n",
       "       [ 0.6151144 ],\n",
       "       [ 0.11623336],\n",
       "       [-0.35694999],\n",
       "       [ 0.08804549],\n",
       "       [ 0.20186214],\n",
       "       [ 0.02900726],\n",
       "       [-0.30973735],\n",
       "       [-0.27547792],\n",
       "       [ 0.76891214],\n",
       "       [-0.05324687],\n",
       "       [-0.18026069],\n",
       "       [ 0.33550993],\n",
       "       [ 0.22156158],\n",
       "       [ 0.1643571 ],\n",
       "       [ 0.4491331 ],\n",
       "       [ 0.29800576],\n",
       "       [ 0.5758192 ],\n",
       "       [ 0.30821404],\n",
       "       [-0.16636178],\n",
       "       [ 0.21758932],\n",
       "       [-0.25346422],\n",
       "       [ 0.31862968],\n",
       "       [ 0.92048335],\n",
       "       [ 0.10142978],\n",
       "       [ 0.40439895],\n",
       "       [ 0.24616249],\n",
       "       [ 0.07599353],\n",
       "       [ 0.6595602 ],\n",
       "       [-0.30560613],\n",
       "       [ 0.02939192],\n",
       "       [-0.3559599 ],\n",
       "       [ 0.12465762],\n",
       "       [-0.2394654 ],\n",
       "       [-0.35690302],\n",
       "       [ 0.00124365],\n",
       "       [ 0.263992  ],\n",
       "       [-0.12047873],\n",
       "       [ 0.07604867],\n",
       "       [-0.10386565],\n",
       "       [-0.29493976],\n",
       "       [ 0.4638098 ],\n",
       "       [ 0.44033763],\n",
       "       [-0.23341312],\n",
       "       [ 0.00368519]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bdc9c6-b810-4262-ac26-ccdf52159a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5af84a-9930-4561-974f-bd0237da28a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
