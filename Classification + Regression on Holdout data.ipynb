{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34696db2-db1d-4f51-a184-e031a1bcfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import subprocess\n",
    "import shlex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1df020a-1efa-4fc6-857a-dc996beef8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14283, 67)\n",
      "(4080, 67)\n",
      "(2041, 67)\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.read_csv('median_icumean_train.csv')\n",
    "X_test_df = pd.read_csv('median_icumean_test.csv')\n",
    "X_holdout_df = pd.read_csv('median_icumean_holdout.csv')\n",
    "\n",
    "print(X_train_df.shape)\n",
    "print(X_test_df.shape)\n",
    "print(X_holdout_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3436d63-7cab-42ef-a62d-ff3f96d57dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hosp_admittime</th>\n",
       "      <th>hosp_dischtime</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>icu_outtime</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 02:58:00</td>\n",
       "      <td>1978-03-27 19:23:00</td>\n",
       "      <td>1978-03-25 02:59:00</td>\n",
       "      <td>1978-03-27 21:46:00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84.227760</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>32.70</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>32.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001361</td>\n",
       "      <td>2043-05-04 14:55:00</td>\n",
       "      <td>2043-05-18 16:58:00</td>\n",
       "      <td>2043-05-04 16:52:00</td>\n",
       "      <td>2043-05-10 17:59:00</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.338465</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.320000</td>\n",
       "      <td>33.56</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>209.600000</td>\n",
       "      <td>3.684000</td>\n",
       "      <td>14.220000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>31.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002506</td>\n",
       "      <td>2032-03-19 05:42:00</td>\n",
       "      <td>2032-03-28 16:09:00</td>\n",
       "      <td>2032-03-19 05:50:00</td>\n",
       "      <td>2032-03-25 19:23:00</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.214207</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>33.50</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>193.333333</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>13.133333</td>\n",
       "      <td>9.633333</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>31.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003425</td>\n",
       "      <td>2055-07-21 10:00:00</td>\n",
       "      <td>2055-07-29 14:40:00</td>\n",
       "      <td>2055-07-22 17:13:00</td>\n",
       "      <td>2055-07-26 17:11:00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.551461</td>\n",
       "      <td>72.7</td>\n",
       "      <td>...</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>31.05</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20003491</td>\n",
       "      <td>1997-12-18 04:50:00</td>\n",
       "      <td>1997-12-28 17:29:00</td>\n",
       "      <td>1997-12-18 06:10:00</td>\n",
       "      <td>1997-12-20 19:02:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.963058</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.966667</td>\n",
       "      <td>32.70</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>169.666667</td>\n",
       "      <td>3.523333</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       hosp_admittime       hosp_dischtime           icu_intime  \\\n",
       "0  20001305  1978-03-25 02:58:00  1978-03-27 19:23:00  1978-03-25 02:59:00   \n",
       "1  20001361  2043-05-04 14:55:00  2043-05-18 16:58:00  2043-05-04 16:52:00   \n",
       "2  20002506  2032-03-19 05:42:00  2032-03-28 16:09:00  2032-03-19 05:50:00   \n",
       "3  20003425  2055-07-21 10:00:00  2055-07-29 14:40:00  2055-07-22 17:13:00   \n",
       "4  20003491  1997-12-18 04:50:00  1997-12-28 17:29:00  1997-12-18 06:10:00   \n",
       "\n",
       "           icu_outtime  los_icu  icu_death  gender  admission_age  \\\n",
       "0  1978-03-27 21:46:00     2.78          1       0      84.227760   \n",
       "1  2043-05-10 17:59:00     6.05          0       1      30.338465   \n",
       "2  2032-03-25 19:23:00     6.56          0       1      24.214207   \n",
       "3  2055-07-26 17:11:00     4.00          0       1      76.551461   \n",
       "4  1997-12-20 19:02:00     2.54          0       1      56.963058   \n",
       "\n",
       "   weight_admit  ...        mch   mchc        mcv    platelet       rbc  \\\n",
       "0          44.0  ...  30.100000  32.70  91.000000  182.000000  3.300000   \n",
       "1         102.0  ...  30.320000  33.56  90.000000  209.600000  3.684000   \n",
       "2          60.0  ...  30.100000  33.50  89.333333  193.333333  4.020000   \n",
       "3          72.7  ...  27.900000  31.05  90.000000  214.000000  2.720000   \n",
       "4          60.0  ...  29.966667  32.70  91.000000  169.666667  3.523333   \n",
       "\n",
       "         rdw        wbc       inr         pt        ptt  \n",
       "0  15.600000   9.900000  1.333333  14.333333  32.766667  \n",
       "1  14.220000  15.260000  1.600000  17.380000  31.240000  \n",
       "2  13.133333   9.633333  1.266667  13.600000  31.033333  \n",
       "3  15.650000  10.100000  1.400000  15.000000  33.500000  \n",
       "4  15.700000   8.600000  1.533333  16.500000  36.500000  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07aec4e4-0874-4504-a473-11404016b26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.00000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "      <td>14283.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.499301e+07</td>\n",
       "      <td>4.862968</td>\n",
       "      <td>0.10691</td>\n",
       "      <td>0.562977</td>\n",
       "      <td>67.230634</td>\n",
       "      <td>85.348792</td>\n",
       "      <td>169.749871</td>\n",
       "      <td>5.810334</td>\n",
       "      <td>0.267591</td>\n",
       "      <td>0.129105</td>\n",
       "      <td>...</td>\n",
       "      <td>29.923418</td>\n",
       "      <td>32.661421</td>\n",
       "      <td>91.450866</td>\n",
       "      <td>199.855313</td>\n",
       "      <td>3.422641</td>\n",
       "      <td>15.751874</td>\n",
       "      <td>11.883691</td>\n",
       "      <td>1.531261</td>\n",
       "      <td>16.551365</td>\n",
       "      <td>36.770599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.871706e+06</td>\n",
       "      <td>6.045947</td>\n",
       "      <td>0.30901</td>\n",
       "      <td>0.496035</td>\n",
       "      <td>15.602344</td>\n",
       "      <td>28.642186</td>\n",
       "      <td>7.923268</td>\n",
       "      <td>2.924375</td>\n",
       "      <td>0.442719</td>\n",
       "      <td>0.335327</td>\n",
       "      <td>...</td>\n",
       "      <td>2.295107</td>\n",
       "      <td>1.373935</td>\n",
       "      <td>6.139115</td>\n",
       "      <td>95.897348</td>\n",
       "      <td>0.588463</td>\n",
       "      <td>2.025444</td>\n",
       "      <td>8.359243</td>\n",
       "      <td>0.634117</td>\n",
       "      <td>6.614060</td>\n",
       "      <td>12.993901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000130e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.009528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>53.400000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>18.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.251859e+07</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.416664</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.850000</td>\n",
       "      <td>31.866667</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>3.045000</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>8.350000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>13.933333</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.500208e+07</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.533893</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.050000</td>\n",
       "      <td>32.700000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>185.666667</td>\n",
       "      <td>3.370000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.746099e+07</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78.917610</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>31.126786</td>\n",
       "      <td>33.475000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>16.633333</td>\n",
       "      <td>13.733333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>16.233333</td>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.999962e+07</td>\n",
       "      <td>101.730000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.058421</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.075000</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>130.750000</td>\n",
       "      <td>1285.500000</td>\n",
       "      <td>6.770000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>309.450000</td>\n",
       "      <td>12.550000</td>\n",
       "      <td>136.475000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       los_icu    icu_death        gender  admission_age  \\\n",
       "count  1.428300e+04  14283.000000  14283.00000  14283.000000   14283.000000   \n",
       "mean   2.499301e+07      4.862968      0.10691      0.562977      67.230634   \n",
       "std    2.871706e+06      6.045947      0.30901      0.496035      15.602344   \n",
       "min    2.000130e+07      1.000000      0.00000      0.000000      18.009528   \n",
       "25%    2.251859e+07      1.800000      0.00000      0.000000      57.416664   \n",
       "50%    2.500208e+07      2.880000      0.00000      1.000000      68.533893   \n",
       "75%    2.746099e+07      5.270000      0.00000      1.000000      78.917610   \n",
       "max    2.999962e+07    101.730000      1.00000      1.000000     100.058421   \n",
       "\n",
       "       weight_admit        height  charlson_score  atrial_fibrillation  \\\n",
       "count  14283.000000  14283.000000    14283.000000         14283.000000   \n",
       "mean      85.348792    169.749871        5.810334             0.267591   \n",
       "std       28.642186      7.923268        2.924375             0.442719   \n",
       "min        1.000000    122.000000        0.000000             0.000000   \n",
       "25%       68.500000    168.000000        4.000000             0.000000   \n",
       "50%       81.500000    170.000000        6.000000             0.000000   \n",
       "75%       97.600000    170.000000        8.000000             1.000000   \n",
       "max     1010.000000    203.000000       17.000000             1.000000   \n",
       "\n",
       "       malignant_cancer  ...           mch          mchc           mcv  \\\n",
       "count      14283.000000  ...  14283.000000  14283.000000  14283.000000   \n",
       "mean           0.129105  ...     29.923418     32.661421     91.450866   \n",
       "std            0.335327  ...      2.295107      1.373935      6.139115   \n",
       "min            0.000000  ...     16.700000     25.500000     53.400000   \n",
       "25%            0.000000  ...     28.850000     31.866667     88.000000   \n",
       "50%            0.000000  ...     30.050000     32.700000     91.000000   \n",
       "75%            0.000000  ...     31.126786     33.475000     94.500000   \n",
       "max            1.000000  ...     43.075000     39.800000    130.750000   \n",
       "\n",
       "           platelet           rbc           rdw           wbc           inr  \\\n",
       "count  14283.000000  14283.000000  14283.000000  14283.000000  14283.000000   \n",
       "mean     199.855313      3.422641     15.751874     11.883691      1.531261   \n",
       "std       95.897348      0.588463      2.025444      8.359243      0.634117   \n",
       "min        8.000000      1.360000     11.300000      0.100000      0.800000   \n",
       "25%      145.000000      3.045000     14.450000      8.350000      1.275000   \n",
       "50%      185.666667      3.370000     15.400000     10.500000      1.400000   \n",
       "75%      235.000000      3.760000     16.633333     13.733333      1.500000   \n",
       "max     1285.500000      6.770000     34.600000    309.450000     12.550000   \n",
       "\n",
       "                 pt           ptt  \n",
       "count  14283.000000  14283.000000  \n",
       "mean      16.551365     36.770599  \n",
       "std        6.614060     12.993901  \n",
       "min        8.700000     18.250000  \n",
       "25%       13.933333     31.000000  \n",
       "50%       15.000000     33.500000  \n",
       "75%       16.233333     36.500000  \n",
       "max      136.475000    150.000000  \n",
       "\n",
       "[8 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1927f09-8bbd-4ffd-ab7a-5b4dc893ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting los_icu into 3 categorical bins\n",
    "\n",
    "low_thres = 2.5\n",
    "medium_thres = 5\n",
    "\n",
    "X_train_df['icu_cat'] = 10\n",
    "X_train_df.loc[X_train_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "X_train_df.loc[((X_train_df['los_icu'] >= low_thres) & (X_train_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "X_train_df.loc[X_train_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "X_test_df['icu_cat'] = 10\n",
    "X_test_df.loc[X_test_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "X_test_df.loc[((X_test_df['los_icu'] >= low_thres) & (X_test_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "X_test_df.loc[X_test_df['los_icu'] >= medium_thres, 'icu_cat'] = 2\n",
    "\n",
    "X_holdout_df['icu_cat'] = 10\n",
    "X_holdout_df.loc[X_holdout_df['los_icu'] < low_thres, 'icu_cat'] = 0\n",
    "X_holdout_df.loc[((X_holdout_df['los_icu'] >= low_thres) & (X_holdout_df['los_icu'] < medium_thres)), 'icu_cat'] = 1\n",
    "X_holdout_df.loc[X_holdout_df['los_icu'] >= medium_thres, 'icu_cat'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9ac25f-ece9-4be8-9d2b-c7b4c649740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14283, 68)\n",
      "(4080, 68)\n",
      "(2041, 68)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_df.shape)\n",
    "print(X_test_df.shape)\n",
    "print(X_holdout_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e1301c-1863-42b5-84d9-a2f2a47c26ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hosp_admittime</th>\n",
       "      <th>hosp_dischtime</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>icu_outtime</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>...</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "      <th>icu_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001305</td>\n",
       "      <td>1978-03-25 02:58:00</td>\n",
       "      <td>1978-03-27 19:23:00</td>\n",
       "      <td>1978-03-25 02:59:00</td>\n",
       "      <td>1978-03-27 21:46:00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84.227760</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.70</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>32.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001361</td>\n",
       "      <td>2043-05-04 14:55:00</td>\n",
       "      <td>2043-05-18 16:58:00</td>\n",
       "      <td>2043-05-04 16:52:00</td>\n",
       "      <td>2043-05-10 17:59:00</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.338465</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.56</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>209.600000</td>\n",
       "      <td>3.684000</td>\n",
       "      <td>14.220000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>31.240000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002506</td>\n",
       "      <td>2032-03-19 05:42:00</td>\n",
       "      <td>2032-03-28 16:09:00</td>\n",
       "      <td>2032-03-19 05:50:00</td>\n",
       "      <td>2032-03-25 19:23:00</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.214207</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.50</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>193.333333</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>13.133333</td>\n",
       "      <td>9.633333</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>31.033333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003425</td>\n",
       "      <td>2055-07-21 10:00:00</td>\n",
       "      <td>2055-07-29 14:40:00</td>\n",
       "      <td>2055-07-22 17:13:00</td>\n",
       "      <td>2055-07-26 17:11:00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.551461</td>\n",
       "      <td>72.7</td>\n",
       "      <td>...</td>\n",
       "      <td>31.05</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20003491</td>\n",
       "      <td>1997-12-18 04:50:00</td>\n",
       "      <td>1997-12-28 17:29:00</td>\n",
       "      <td>1997-12-18 06:10:00</td>\n",
       "      <td>1997-12-20 19:02:00</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56.963058</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.70</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>169.666667</td>\n",
       "      <td>3.523333</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       hosp_admittime       hosp_dischtime           icu_intime  \\\n",
       "0  20001305  1978-03-25 02:58:00  1978-03-27 19:23:00  1978-03-25 02:59:00   \n",
       "1  20001361  2043-05-04 14:55:00  2043-05-18 16:58:00  2043-05-04 16:52:00   \n",
       "2  20002506  2032-03-19 05:42:00  2032-03-28 16:09:00  2032-03-19 05:50:00   \n",
       "3  20003425  2055-07-21 10:00:00  2055-07-29 14:40:00  2055-07-22 17:13:00   \n",
       "4  20003491  1997-12-18 04:50:00  1997-12-28 17:29:00  1997-12-18 06:10:00   \n",
       "\n",
       "           icu_outtime  los_icu  icu_death  gender  admission_age  \\\n",
       "0  1978-03-27 21:46:00     2.78          1       0      84.227760   \n",
       "1  2043-05-10 17:59:00     6.05          0       1      30.338465   \n",
       "2  2032-03-25 19:23:00     6.56          0       1      24.214207   \n",
       "3  2055-07-26 17:11:00     4.00          0       1      76.551461   \n",
       "4  1997-12-20 19:02:00     2.54          0       1      56.963058   \n",
       "\n",
       "   weight_admit  ...   mchc        mcv    platelet       rbc        rdw  \\\n",
       "0          44.0  ...  32.70  91.000000  182.000000  3.300000  15.600000   \n",
       "1         102.0  ...  33.56  90.000000  209.600000  3.684000  14.220000   \n",
       "2          60.0  ...  33.50  89.333333  193.333333  4.020000  13.133333   \n",
       "3          72.7  ...  31.05  90.000000  214.000000  2.720000  15.650000   \n",
       "4          60.0  ...  32.70  91.000000  169.666667  3.523333  15.700000   \n",
       "\n",
       "         wbc       inr         pt        ptt  icu_cat  \n",
       "0   9.900000  1.333333  14.333333  32.766667        1  \n",
       "1  15.260000  1.600000  17.380000  31.240000        2  \n",
       "2   9.633333  1.266667  13.600000  31.033333        2  \n",
       "3  10.100000  1.400000  15.000000  33.500000        1  \n",
       "4   8.600000  1.533333  16.500000  36.500000        1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde195df-eb4c-4a4d-b353-908521c48aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into X and Y\n",
    "\n",
    "X_features_to_drop = ['id', 'los_icu', 'icu_death', 'hosp_admittime', 'hosp_dischtime', 'icu_intime', 'icu_outtime', 'icu_cat', 'icu_outcome']\n",
    "\n",
    "y_train = X_train_df.icu_cat\n",
    "X_train = X_train_df.drop(columns=X_features_to_drop)\n",
    "\n",
    "y_test = X_test_df.icu_cat\n",
    "X_test = X_test_df.drop(columns=X_features_to_drop)\n",
    "\n",
    "y_holdout = X_holdout_df.icu_cat\n",
    "X_holdout = X_holdout_df.drop(columns=X_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5f608e-0794-405c-b1c9-9b85f1375575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14283, 59)\n",
      "(4080, 59)\n",
      "(2041, 59)\n",
      "(14283,)\n",
      "(4080,)\n",
      "(2041,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_holdout.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697f766e-5872-41b5-b885-ac35be810f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                                                               int64\n",
       "admission_age                                                      float64\n",
       "weight_admit                                                       float64\n",
       "height                                                             float64\n",
       "charlson_score                                                       int64\n",
       "atrial_fibrillation                                                  int64\n",
       "malignant_cancer                                                     int64\n",
       "chf                                                                  int64\n",
       "ckd                                                                  int64\n",
       "cld                                                                  int64\n",
       "copd                                                                 int64\n",
       "diabetes                                                             int64\n",
       "hypertension                                                         int64\n",
       "ihd                                                                  int64\n",
       "stroke                                                               int64\n",
       "race_encode_African                                                  int64\n",
       "race_encode_Asian                                                    int64\n",
       "race_encode_Caucasian                                                int64\n",
       "race_encode_Hispanic                                                 int64\n",
       "race_encode_Not Specified                                            int64\n",
       "race_encode_South American                                           int64\n",
       "admission_type_DIRECT EMER.                                          int64\n",
       "admission_type_DIRECT OBSERVATION                                    int64\n",
       "admission_type_ELECTIVE                                              int64\n",
       "admission_type_EU OBSERVATION                                        int64\n",
       "admission_type_EW EMER.                                              int64\n",
       "admission_type_OBSERVATION ADMIT                                     int64\n",
       "admission_type_SURGICAL SAME DAY ADMISSION                           int64\n",
       "admission_type_URGENT                                                int64\n",
       "first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)          int64\n",
       "first_careunit_Coronary Care Unit (CCU)                              int64\n",
       "first_careunit_Medical Intensive Care Unit (MICU)                    int64\n",
       "first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)      int64\n",
       "first_careunit_Neuro Intermediate                                    int64\n",
       "first_careunit_Neuro Stepdown                                        int64\n",
       "first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)       int64\n",
       "first_careunit_Surgical Intensive Care Unit (SICU)                   int64\n",
       "first_careunit_Trauma SICU (TSICU)                                   int64\n",
       "aniongap                                                           float64\n",
       "bicarbonate                                                        float64\n",
       "bun                                                                float64\n",
       "calcium                                                            float64\n",
       "chloride                                                           float64\n",
       "creatinine                                                         float64\n",
       "glucose                                                            float64\n",
       "sodium                                                             float64\n",
       "potassium                                                          float64\n",
       "hematocrit                                                         float64\n",
       "hemoglobin                                                         float64\n",
       "mch                                                                float64\n",
       "mchc                                                               float64\n",
       "mcv                                                                float64\n",
       "platelet                                                           float64\n",
       "rbc                                                                float64\n",
       "rdw                                                                float64\n",
       "wbc                                                                float64\n",
       "inr                                                                float64\n",
       "pt                                                                 float64\n",
       "ptt                                                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787bbf0b-5dc4-4822-a7df-9d4d6d7eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization using data from X_train to transform X_test\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = X_train.columns[X_train.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "# print(num_cols)\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "X_holdout[num_cols] = scaler.transform(X_holdout[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52a3282-aaca-4a37-a2ab-798babc8d537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "      <td>1.428300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.333231e-16</td>\n",
       "      <td>3.900200e-16</td>\n",
       "      <td>-1.349399e-16</td>\n",
       "      <td>-2.243112e-15</td>\n",
       "      <td>9.452014e-18</td>\n",
       "      <td>1.079520e-16</td>\n",
       "      <td>-5.248355e-17</td>\n",
       "      <td>8.606308e-17</td>\n",
       "      <td>-3.830553e-17</td>\n",
       "      <td>-1.144191e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>3.733546e-16</td>\n",
       "      <td>1.294926e-15</td>\n",
       "      <td>2.666463e-15</td>\n",
       "      <td>-2.168989e-16</td>\n",
       "      <td>4.716058e-16</td>\n",
       "      <td>-1.412827e-16</td>\n",
       "      <td>-8.457065e-17</td>\n",
       "      <td>-7.959591e-18</td>\n",
       "      <td>-1.223787e-16</td>\n",
       "      <td>3.487296e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "      <td>1.000035e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.134993e+00</td>\n",
       "      <td>-3.154836e+00</td>\n",
       "      <td>-2.945018e+00</td>\n",
       "      <td>-6.026749e+00</td>\n",
       "      <td>-1.986933e+00</td>\n",
       "      <td>-6.044477e-01</td>\n",
       "      <td>-3.850239e-01</td>\n",
       "      <td>-7.246701e-01</td>\n",
       "      <td>-8.617885e-01</td>\n",
       "      <td>-3.565539e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.761772e+00</td>\n",
       "      <td>-5.212526e+00</td>\n",
       "      <td>-6.198320e+00</td>\n",
       "      <td>-2.000702e+00</td>\n",
       "      <td>-3.505255e+00</td>\n",
       "      <td>-2.198051e+00</td>\n",
       "      <td>-1.409709e+00</td>\n",
       "      <td>-1.153236e+00</td>\n",
       "      <td>-1.187113e+00</td>\n",
       "      <td>-1.425380e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.134993e+00</td>\n",
       "      <td>-6.290281e-01</td>\n",
       "      <td>-5.882715e-01</td>\n",
       "      <td>-2.208599e-01</td>\n",
       "      <td>-6.190716e-01</td>\n",
       "      <td>-6.044477e-01</td>\n",
       "      <td>-3.850239e-01</td>\n",
       "      <td>-7.246701e-01</td>\n",
       "      <td>-8.617885e-01</td>\n",
       "      <td>-3.565539e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.677149e-01</td>\n",
       "      <td>-5.784714e-01</td>\n",
       "      <td>-5.621310e-01</td>\n",
       "      <td>-5.720412e-01</td>\n",
       "      <td>-6.417632e-01</td>\n",
       "      <td>-6.427822e-01</td>\n",
       "      <td>-4.227434e-01</td>\n",
       "      <td>-4.041372e-01</td>\n",
       "      <td>-3.958421e-01</td>\n",
       "      <td>-4.441161e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.810628e-01</td>\n",
       "      <td>8.353260e-02</td>\n",
       "      <td>-1.343797e-01</td>\n",
       "      <td>3.157000e-02</td>\n",
       "      <td>6.485923e-02</td>\n",
       "      <td>-6.044477e-01</td>\n",
       "      <td>-3.850239e-01</td>\n",
       "      <td>-7.246701e-01</td>\n",
       "      <td>-8.617885e-01</td>\n",
       "      <td>-3.565539e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.515494e-02</td>\n",
       "      <td>2.808018e-02</td>\n",
       "      <td>-7.344411e-02</td>\n",
       "      <td>-1.479618e-01</td>\n",
       "      <td>-8.945758e-02</td>\n",
       "      <td>-1.737328e-01</td>\n",
       "      <td>-1.655340e-01</td>\n",
       "      <td>-2.070059e-01</td>\n",
       "      <td>-2.345638e-01</td>\n",
       "      <td>-2.517114e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.810628e-01</td>\n",
       "      <td>7.490788e-01</td>\n",
       "      <td>4.277480e-01</td>\n",
       "      <td>3.157000e-02</td>\n",
       "      <td>7.487901e-01</td>\n",
       "      <td>1.654403e+00</td>\n",
       "      <td>-3.850239e-01</td>\n",
       "      <td>1.379938e+00</td>\n",
       "      <td>1.160378e+00</td>\n",
       "      <td>-3.565539e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.243373e-01</td>\n",
       "      <td>5.921732e-01</td>\n",
       "      <td>4.966906e-01</td>\n",
       "      <td>3.664952e-01</td>\n",
       "      <td>5.733092e-01</td>\n",
       "      <td>4.352086e-01</td>\n",
       "      <td>2.212769e-01</td>\n",
       "      <td>-4.930083e-02</td>\n",
       "      <td>-4.808586e-02</td>\n",
       "      <td>-2.082578e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.810628e-01</td>\n",
       "      <td>2.104103e+00</td>\n",
       "      <td>3.228397e+01</td>\n",
       "      <td>4.196664e+00</td>\n",
       "      <td>3.826479e+00</td>\n",
       "      <td>1.654403e+00</td>\n",
       "      <td>2.597241e+00</td>\n",
       "      <td>1.379938e+00</td>\n",
       "      <td>1.160378e+00</td>\n",
       "      <td>2.804625e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.730471e+00</td>\n",
       "      <td>5.195900e+00</td>\n",
       "      <td>6.401657e+00</td>\n",
       "      <td>1.132130e+01</td>\n",
       "      <td>5.688509e+00</td>\n",
       "      <td>9.306003e+00</td>\n",
       "      <td>3.559853e+01</td>\n",
       "      <td>1.737711e+01</td>\n",
       "      <td>1.813226e+01</td>\n",
       "      <td>8.714347e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             gender  admission_age  weight_admit        height  \\\n",
       "count  1.428300e+04   1.428300e+04  1.428300e+04  1.428300e+04   \n",
       "mean   1.333231e-16   3.900200e-16 -1.349399e-16 -2.243112e-15   \n",
       "std    1.000035e+00   1.000035e+00  1.000035e+00  1.000035e+00   \n",
       "min   -1.134993e+00  -3.154836e+00 -2.945018e+00 -6.026749e+00   \n",
       "25%   -1.134993e+00  -6.290281e-01 -5.882715e-01 -2.208599e-01   \n",
       "50%    8.810628e-01   8.353260e-02 -1.343797e-01  3.157000e-02   \n",
       "75%    8.810628e-01   7.490788e-01  4.277480e-01  3.157000e-02   \n",
       "max    8.810628e-01   2.104103e+00  3.228397e+01  4.196664e+00   \n",
       "\n",
       "       charlson_score  atrial_fibrillation  malignant_cancer           chf  \\\n",
       "count    1.428300e+04         1.428300e+04      1.428300e+04  1.428300e+04   \n",
       "mean     9.452014e-18         1.079520e-16     -5.248355e-17  8.606308e-17   \n",
       "std      1.000035e+00         1.000035e+00      1.000035e+00  1.000035e+00   \n",
       "min     -1.986933e+00        -6.044477e-01     -3.850239e-01 -7.246701e-01   \n",
       "25%     -6.190716e-01        -6.044477e-01     -3.850239e-01 -7.246701e-01   \n",
       "50%      6.485923e-02        -6.044477e-01     -3.850239e-01 -7.246701e-01   \n",
       "75%      7.487901e-01         1.654403e+00     -3.850239e-01  1.379938e+00   \n",
       "max      3.826479e+00         1.654403e+00      2.597241e+00  1.379938e+00   \n",
       "\n",
       "                ckd           cld  ...           mch          mchc  \\\n",
       "count  1.428300e+04  1.428300e+04  ...  1.428300e+04  1.428300e+04   \n",
       "mean  -3.830553e-17 -1.144191e-17  ...  3.733546e-16  1.294926e-15   \n",
       "std    1.000035e+00  1.000035e+00  ...  1.000035e+00  1.000035e+00   \n",
       "min   -8.617885e-01 -3.565539e-01  ... -5.761772e+00 -5.212526e+00   \n",
       "25%   -8.617885e-01 -3.565539e-01  ... -4.677149e-01 -5.784714e-01   \n",
       "50%   -8.617885e-01 -3.565539e-01  ...  5.515494e-02  2.808018e-02   \n",
       "75%    1.160378e+00 -3.565539e-01  ...  5.243373e-01  5.921732e-01   \n",
       "max    1.160378e+00  2.804625e+00  ...  5.730471e+00  5.195900e+00   \n",
       "\n",
       "                mcv      platelet           rbc           rdw           wbc  \\\n",
       "count  1.428300e+04  1.428300e+04  1.428300e+04  1.428300e+04  1.428300e+04   \n",
       "mean   2.666463e-15 -2.168989e-16  4.716058e-16 -1.412827e-16 -8.457065e-17   \n",
       "std    1.000035e+00  1.000035e+00  1.000035e+00  1.000035e+00  1.000035e+00   \n",
       "min   -6.198320e+00 -2.000702e+00 -3.505255e+00 -2.198051e+00 -1.409709e+00   \n",
       "25%   -5.621310e-01 -5.720412e-01 -6.417632e-01 -6.427822e-01 -4.227434e-01   \n",
       "50%   -7.344411e-02 -1.479618e-01 -8.945758e-02 -1.737328e-01 -1.655340e-01   \n",
       "75%    4.966906e-01  3.664952e-01  5.733092e-01  4.352086e-01  2.212769e-01   \n",
       "max    6.401657e+00  1.132130e+01  5.688509e+00  9.306003e+00  3.559853e+01   \n",
       "\n",
       "                inr            pt           ptt  \n",
       "count  1.428300e+04  1.428300e+04  1.428300e+04  \n",
       "mean  -7.959591e-18 -1.223787e-16  3.487296e-16  \n",
       "std    1.000035e+00  1.000035e+00  1.000035e+00  \n",
       "min   -1.153236e+00 -1.187113e+00 -1.425380e+00  \n",
       "25%   -4.041372e-01 -3.958421e-01 -4.441161e-01  \n",
       "50%   -2.070059e-01 -2.345638e-01 -2.517114e-01  \n",
       "75%   -4.930083e-02 -4.808586e-02 -2.082578e-02  \n",
       "max    1.737711e+01  1.813226e+01  8.714347e+00  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbf5706-598e-4b3f-9ff5-f475061fa57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.010844</td>\n",
       "      <td>-0.010256</td>\n",
       "      <td>-0.005270</td>\n",
       "      <td>-0.018058</td>\n",
       "      <td>-0.030941</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>-0.010048</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.010023</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013606</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.019893</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>-0.020015</td>\n",
       "      <td>0.030381</td>\n",
       "      <td>0.035645</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.026353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001440</td>\n",
       "      <td>0.993147</td>\n",
       "      <td>0.931224</td>\n",
       "      <td>1.001911</td>\n",
       "      <td>0.985703</td>\n",
       "      <td>1.003648</td>\n",
       "      <td>0.988894</td>\n",
       "      <td>1.000148</td>\n",
       "      <td>1.001568</td>\n",
       "      <td>1.001838</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030755</td>\n",
       "      <td>1.012629</td>\n",
       "      <td>1.033956</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>1.020633</td>\n",
       "      <td>1.008651</td>\n",
       "      <td>1.106378</td>\n",
       "      <td>1.093530</td>\n",
       "      <td>1.060812</td>\n",
       "      <td>1.046085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.134993</td>\n",
       "      <td>-3.110516</td>\n",
       "      <td>-2.945018</td>\n",
       "      <td>-5.395674</td>\n",
       "      <td>-1.986933</td>\n",
       "      <td>-0.604448</td>\n",
       "      <td>-0.385024</td>\n",
       "      <td>-0.724670</td>\n",
       "      <td>-0.861788</td>\n",
       "      <td>-0.356554</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.979635</td>\n",
       "      <td>-4.703022</td>\n",
       "      <td>-5.123209</td>\n",
       "      <td>-1.979846</td>\n",
       "      <td>-3.006764</td>\n",
       "      <td>-2.074617</td>\n",
       "      <td>-1.409709</td>\n",
       "      <td>-1.153236</td>\n",
       "      <td>-1.182793</td>\n",
       "      <td>-1.436924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.134993</td>\n",
       "      <td>-0.651561</td>\n",
       "      <td>-0.606602</td>\n",
       "      <td>-0.220860</td>\n",
       "      <td>-0.619072</td>\n",
       "      <td>-0.604448</td>\n",
       "      <td>-0.385024</td>\n",
       "      <td>-0.724670</td>\n",
       "      <td>-0.861788</td>\n",
       "      <td>-0.356554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457730</td>\n",
       "      <td>-0.572406</td>\n",
       "      <td>-0.562131</td>\n",
       "      <td>-0.547709</td>\n",
       "      <td>-0.633266</td>\n",
       "      <td>-0.655743</td>\n",
       "      <td>-0.412774</td>\n",
       "      <td>-0.364711</td>\n",
       "      <td>-0.385762</td>\n",
       "      <td>-0.436420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.881063</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>-0.134380</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.064859</td>\n",
       "      <td>-0.604448</td>\n",
       "      <td>-0.385024</td>\n",
       "      <td>-0.724670</td>\n",
       "      <td>-0.861788</td>\n",
       "      <td>-0.356554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.028080</td>\n",
       "      <td>-0.073444</td>\n",
       "      <td>-0.139272</td>\n",
       "      <td>-0.089458</td>\n",
       "      <td>-0.198420</td>\n",
       "      <td>-0.145097</td>\n",
       "      <td>-0.207006</td>\n",
       "      <td>-0.234564</td>\n",
       "      <td>-0.251711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.881063</td>\n",
       "      <td>0.739906</td>\n",
       "      <td>0.441714</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.748790</td>\n",
       "      <td>1.654403</td>\n",
       "      <td>-0.385024</td>\n",
       "      <td>1.379938</td>\n",
       "      <td>1.160378</td>\n",
       "      <td>-0.356554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556239</td>\n",
       "      <td>0.628566</td>\n",
       "      <td>0.578138</td>\n",
       "      <td>0.413422</td>\n",
       "      <td>0.590303</td>\n",
       "      <td>0.399310</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>-0.049301</td>\n",
       "      <td>-0.022886</td>\n",
       "      <td>0.009959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.881063</td>\n",
       "      <td>2.091408</td>\n",
       "      <td>14.477447</td>\n",
       "      <td>4.827739</td>\n",
       "      <td>3.484513</td>\n",
       "      <td>1.654403</td>\n",
       "      <td>2.597241</td>\n",
       "      <td>1.379938</td>\n",
       "      <td>1.160378</td>\n",
       "      <td>2.804625</td>\n",
       "      <td>...</td>\n",
       "      <td>5.596123</td>\n",
       "      <td>4.176893</td>\n",
       "      <td>6.523829</td>\n",
       "      <td>7.707935</td>\n",
       "      <td>5.127706</td>\n",
       "      <td>9.009762</td>\n",
       "      <td>44.840118</td>\n",
       "      <td>14.774972</td>\n",
       "      <td>15.059149</td>\n",
       "      <td>8.714347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender  admission_age  weight_admit       height  charlson_score  \\\n",
       "count  4080.000000    4080.000000   4080.000000  4080.000000     4080.000000   \n",
       "mean     -0.010844      -0.010256     -0.005270    -0.018058       -0.030941   \n",
       "std       1.001440       0.993147      0.931224     1.001911        0.985703   \n",
       "min      -1.134993      -3.110516     -2.945018    -5.395674       -1.986933   \n",
       "25%      -1.134993      -0.651561     -0.606602    -0.220860       -0.619072   \n",
       "50%       0.881063       0.070605     -0.134380     0.031570        0.064859   \n",
       "75%       0.881063       0.739906      0.441714     0.031570        0.748790   \n",
       "max       0.881063       2.091408     14.477447     4.827739        3.484513   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer          chf          ckd  \\\n",
       "count          4080.000000       4080.000000  4080.000000  4080.000000   \n",
       "mean              0.006771         -0.010048     0.000079     0.010023   \n",
       "std               1.003648          0.988894     1.000148     1.001568   \n",
       "min              -0.604448         -0.385024    -0.724670    -0.861788   \n",
       "25%              -0.604448         -0.385024    -0.724670    -0.861788   \n",
       "50%              -0.604448         -0.385024    -0.724670    -0.861788   \n",
       "75%               1.654403         -0.385024     1.379938     1.160378   \n",
       "max               1.654403          2.597241     1.379938     1.160378   \n",
       "\n",
       "               cld  ...          mch         mchc          mcv     platelet  \\\n",
       "count  4080.000000  ...  4080.000000  4080.000000  4080.000000  4080.000000   \n",
       "mean      0.001403  ...     0.013606     0.017951     0.006273     0.019893   \n",
       "std       1.001838  ...     1.030755     1.012629     1.033956     0.997434   \n",
       "min      -0.356554  ...    -5.979635    -4.703022    -5.123209    -1.979846   \n",
       "25%      -0.356554  ...    -0.457730    -0.572406    -0.562131    -0.547709   \n",
       "50%      -0.356554  ...     0.062417     0.028080    -0.073444    -0.139272   \n",
       "75%      -0.356554  ...     0.556239     0.628566     0.578138     0.413422   \n",
       "max       2.804625  ...     5.596123     4.176893     6.523829     7.707935   \n",
       "\n",
       "               rbc          rdw          wbc          inr           pt  \\\n",
       "count  4080.000000  4080.000000  4080.000000  4080.000000  4080.000000   \n",
       "mean      0.016688    -0.020015     0.030381     0.035645     0.029947   \n",
       "std       1.020633     1.008651     1.106378     1.093530     1.060812   \n",
       "min      -3.006764    -2.074617    -1.409709    -1.153236    -1.182793   \n",
       "25%      -0.633266    -0.655743    -0.412774    -0.364711    -0.385762   \n",
       "50%      -0.089458    -0.198420    -0.145097    -0.207006    -0.234564   \n",
       "75%       0.590303     0.399310     0.247197    -0.049301    -0.022886   \n",
       "max       5.127706     9.009762    44.840118    14.774972    15.059149   \n",
       "\n",
       "               ptt  \n",
       "count  4080.000000  \n",
       "mean      0.026353  \n",
       "std       1.046085  \n",
       "min      -1.436924  \n",
       "25%      -0.436420  \n",
       "50%      -0.251711  \n",
       "75%       0.009959  \n",
       "max       8.714347  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e31edb-a114-4d8b-a8d6-b7863ca32205",
   "metadata": {},
   "source": [
    "# Deep Neural Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cec2c6-26a2-4d14-a2a0-8dbabf25280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de05a587-c45f-4315-86eb-c52aea178cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender                                                             float64\n",
      "admission_age                                                      float64\n",
      "weight_admit                                                       float64\n",
      "height                                                             float64\n",
      "charlson_score                                                     float64\n",
      "atrial_fibrillation                                                float64\n",
      "malignant_cancer                                                   float64\n",
      "chf                                                                float64\n",
      "ckd                                                                float64\n",
      "cld                                                                float64\n",
      "copd                                                               float64\n",
      "diabetes                                                           float64\n",
      "hypertension                                                       float64\n",
      "ihd                                                                float64\n",
      "stroke                                                             float64\n",
      "race_encode_African                                                float64\n",
      "race_encode_Asian                                                  float64\n",
      "race_encode_Caucasian                                              float64\n",
      "race_encode_Hispanic                                               float64\n",
      "race_encode_Not Specified                                          float64\n",
      "race_encode_South American                                         float64\n",
      "admission_type_DIRECT EMER.                                        float64\n",
      "admission_type_DIRECT OBSERVATION                                  float64\n",
      "admission_type_ELECTIVE                                            float64\n",
      "admission_type_EU OBSERVATION                                      float64\n",
      "admission_type_EW EMER.                                            float64\n",
      "admission_type_OBSERVATION ADMIT                                   float64\n",
      "admission_type_SURGICAL SAME DAY ADMISSION                         float64\n",
      "admission_type_URGENT                                              float64\n",
      "first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)        float64\n",
      "first_careunit_Coronary Care Unit (CCU)                            float64\n",
      "first_careunit_Medical Intensive Care Unit (MICU)                  float64\n",
      "first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)    float64\n",
      "first_careunit_Neuro Intermediate                                  float64\n",
      "first_careunit_Neuro Stepdown                                      float64\n",
      "first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)     float64\n",
      "first_careunit_Surgical Intensive Care Unit (SICU)                 float64\n",
      "first_careunit_Trauma SICU (TSICU)                                 float64\n",
      "aniongap                                                           float64\n",
      "bicarbonate                                                        float64\n",
      "bun                                                                float64\n",
      "calcium                                                            float64\n",
      "chloride                                                           float64\n",
      "creatinine                                                         float64\n",
      "glucose                                                            float64\n",
      "sodium                                                             float64\n",
      "potassium                                                          float64\n",
      "hematocrit                                                         float64\n",
      "hemoglobin                                                         float64\n",
      "mch                                                                float64\n",
      "mchc                                                               float64\n",
      "mcv                                                                float64\n",
      "platelet                                                           float64\n",
      "rbc                                                                float64\n",
      "rdw                                                                float64\n",
      "wbc                                                                float64\n",
      "inr                                                                float64\n",
      "pt                                                                 float64\n",
      "ptt                                                                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# X_train['gender'] = X_train['gender'].astype(int)\n",
    "# X_test['gender'] = X_test['gender'].astype(int)\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb0f20e-fe6d-4111-86ef-f7e652c66f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.to_numpy(),dtype=torch.float32)\n",
    "m,n = X_train_tensor.shape\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(),dtype=torch.long).reshape(m).squeeze()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(),dtype=torch.float32)\n",
    "m,n = X_test_tensor.shape\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(),dtype=torch.long).reshape(m).squeeze()\n",
    "\n",
    "X_holdout_tensor = torch.tensor(X_holdout.to_numpy(),dtype=torch.float32)\n",
    "m,n = X_holdout_tensor.shape\n",
    "y_holdout_tensor = torch.tensor(y_holdout.to_numpy(),dtype=torch.long).reshape(m).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d45576fa-101e-4aa6-a619-6da5438cfb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14283, 59])\n",
      "torch.Size([14283])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "# y_train_tensor = y_train.view(-1).long()\n",
    "print(y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f44a41-29f4-4015-a7db-850ab57592b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t1.1023120880126953\n",
      "    1\t1.0973256826400757\n",
      "    2\t1.07455313205719\n",
      "    3\t1.0748486518859863\n",
      "    4\t1.0666764974594116\n",
      "    5\t1.0627646446228027\n",
      "    6\t1.0598446130752563\n",
      "    7\t1.0562636852264404\n",
      "    8\t1.0540813207626343\n",
      "    9\t1.0521713495254517\n",
      "   10\t1.0497864484786987\n",
      "   11\t1.0478484630584717\n",
      "   12\t1.046120285987854\n",
      "   13\t1.0444294214248657\n",
      "   14\t1.0421719551086426\n",
      "   15\t1.0405839681625366\n",
      "   16\t1.0371439456939697\n",
      "   17\t1.034985065460205\n",
      "   18\t1.031877040863037\n",
      "   19\t1.030208706855774\n",
      "   20\t1.0284759998321533\n",
      "   21\t1.0252312421798706\n",
      "   22\t1.0220063924789429\n",
      "   23\t1.0182706117630005\n",
      "   24\t1.0166207551956177\n",
      "   25\t1.0136144161224365\n",
      "   26\t1.0098512172698975\n",
      "   27\t1.0089178085327148\n",
      "   28\t1.0043741464614868\n",
      "   29\t1.0017871856689453\n",
      "   30\t1.0019153356552124\n",
      "   31\t0.9976661801338196\n",
      "   32\t0.9962183833122253\n",
      "   33\t0.9945583343505859\n",
      "   34\t0.9923444986343384\n",
      "   35\t0.9895961880683899\n",
      "   36\t0.9906918406486511\n",
      "   37\t0.9854230284690857\n",
      "   38\t0.9834398627281189\n",
      "   39\t0.981504499912262\n",
      "   40\t0.9824061989784241\n",
      "   41\t0.978141188621521\n",
      "   42\t0.9768304228782654\n",
      "   43\t0.972873866558075\n",
      "   44\t0.9737282991409302\n",
      "   45\t0.9684963822364807\n",
      "   46\t0.968951404094696\n",
      "   47\t0.9677822589874268\n",
      "   48\t0.9651257395744324\n",
      "   49\t0.9623165130615234\n",
      "   50\t0.9625433087348938\n",
      "   51\t0.9601567387580872\n",
      "   52\t0.9595263004302979\n",
      "   53\t0.9566054940223694\n",
      "   54\t0.9536591172218323\n",
      "   55\t0.9546713829040527\n",
      "   56\t0.9514400362968445\n",
      "   57\t0.9516393542289734\n",
      "   58\t0.9486749768257141\n",
      "   59\t0.9445400238037109\n",
      "   60\t0.9488303661346436\n",
      "   61\t0.9415201544761658\n",
      "   62\t0.9423302412033081\n",
      "   63\t0.9405231475830078\n",
      "   64\t0.9365352392196655\n",
      "   65\t0.9373948574066162\n",
      "   66\t0.935663640499115\n",
      "   67\t0.9340798258781433\n",
      "   68\t0.9308062195777893\n",
      "   69\t0.9303061366081238\n",
      "   70\t0.9301255345344543\n",
      "   71\t0.924944281578064\n",
      "   72\t0.9196808338165283\n",
      "   73\t0.9243249893188477\n",
      "   74\t0.9206351637840271\n",
      "   75\t0.9191544651985168\n",
      "   76\t0.91492760181427\n",
      "   77\t0.9169941544532776\n",
      "   78\t0.9135004281997681\n",
      "   79\t0.9157975912094116\n",
      "   80\t0.9081680774688721\n",
      "   81\t0.9098960757255554\n",
      "   82\t0.9089139699935913\n",
      "   83\t0.9057878851890564\n",
      "   84\t0.9101470708847046\n",
      "   85\t0.9014760851860046\n",
      "   86\t0.9005840420722961\n",
      "   87\t0.9018155336380005\n",
      "   88\t0.9007323384284973\n",
      "   89\t0.8972289562225342\n",
      "   90\t0.8938380479812622\n",
      "   91\t0.8964940309524536\n",
      "   92\t0.8929787278175354\n",
      "   93\t0.8914909958839417\n",
      "   94\t0.8898726105690002\n",
      "   95\t0.8923341035842896\n",
      "   96\t0.8886244297027588\n",
      "   97\t0.8891500234603882\n",
      "   98\t0.8850933909416199\n",
      "   99\t0.8846608996391296\n",
      "  100\t0.8818061947822571\n",
      "  101\t0.8785417079925537\n",
      "  102\t0.8805662393569946\n",
      "  103\t0.878350019454956\n",
      "  104\t0.8736997842788696\n",
      "  105\t0.8702510595321655\n",
      "  106\t0.8711459040641785\n",
      "  107\t0.8726730346679688\n",
      "  108\t0.8697779774665833\n",
      "  109\t0.8740121126174927\n",
      "  110\t0.8710803389549255\n",
      "  111\t0.8697078824043274\n",
      "  112\t0.8675971031188965\n",
      "  113\t0.8665345907211304\n",
      "  114\t0.863964319229126\n",
      "  115\t0.8659379482269287\n",
      "  116\t0.8652507066726685\n",
      "  117\t0.8617587089538574\n",
      "  118\t0.8622666597366333\n",
      "  119\t0.8603309988975525\n",
      "  120\t0.8541887998580933\n",
      "  121\t0.8538398742675781\n",
      "  122\t0.8570941090583801\n",
      "  123\t0.8564899563789368\n",
      "  124\t0.8544423580169678\n",
      "  125\t0.8548301458358765\n",
      "  126\t0.8548974990844727\n",
      "  127\t0.8548259139060974\n",
      "  128\t0.8520495891571045\n",
      "  129\t0.8477641940116882\n",
      "  130\t0.8491334915161133\n",
      "  131\t0.844675600528717\n",
      "  132\t0.8464664220809937\n",
      "  133\t0.8469077348709106\n",
      "  134\t0.84434974193573\n",
      "  135\t0.8455982804298401\n",
      "  136\t0.8437620997428894\n",
      "  137\t0.8449416756629944\n",
      "  138\t0.8394365310668945\n",
      "  139\t0.8386096358299255\n",
      "  140\t0.8426425457000732\n",
      "  141\t0.8369297981262207\n",
      "  142\t0.8372887969017029\n",
      "  143\t0.8380500078201294\n",
      "  144\t0.8359960317611694\n",
      "  145\t0.8374924063682556\n",
      "  146\t0.8346744179725647\n",
      "  147\t0.8309871554374695\n",
      "  148\t0.8352512717247009\n",
      "  149\t0.8288457989692688\n",
      "  150\t0.8311631679534912\n",
      "  151\t0.8341701030731201\n",
      "  152\t0.8275841474533081\n",
      "  153\t0.8325129151344299\n",
      "  154\t0.8229433298110962\n",
      "  155\t0.8259124159812927\n",
      "  156\t0.8263774514198303\n",
      "  157\t0.8263363838195801\n",
      "  158\t0.8260170817375183\n",
      "  159\t0.8265255093574524\n",
      "  160\t0.8235127925872803\n",
      "  161\t0.8245436549186707\n",
      "  162\t0.8252102732658386\n",
      "  163\t0.824728786945343\n",
      "  164\t0.8270383477210999\n",
      "  165\t0.824260950088501\n",
      "  166\t0.818926990032196\n",
      "  167\t0.8238049149513245\n",
      "  168\t0.8230065107345581\n",
      "  169\t0.8198167085647583\n",
      "  170\t0.8191577792167664\n",
      "  171\t0.8178865909576416\n",
      "  172\t0.8182013630867004\n",
      "  173\t0.8162606954574585\n",
      "  174\t0.8185360431671143\n",
      "  175\t0.8164253234863281\n",
      "  176\t0.81174635887146\n",
      "  177\t0.8162778615951538\n",
      "  178\t0.8157391548156738\n",
      "  179\t0.812521755695343\n",
      "  180\t0.8070315718650818\n",
      "  181\t0.8091021180152893\n",
      "  182\t0.8092803359031677\n",
      "  183\t0.8099040985107422\n",
      "  184\t0.8139224648475647\n",
      "  185\t0.8071587681770325\n",
      "  186\t0.8078630566596985\n",
      "  187\t0.8057247996330261\n",
      "  188\t0.8071449398994446\n",
      "  189\t0.8076954483985901\n",
      "  190\t0.8057757019996643\n",
      "  191\t0.8078948259353638\n",
      "  192\t0.806157648563385\n",
      "  193\t0.8031558394432068\n",
      "  194\t0.8036676645278931\n",
      "  195\t0.803997814655304\n",
      "  196\t0.804020881652832\n",
      "  197\t0.802574872970581\n",
      "  198\t0.8042670488357544\n",
      "  199\t0.8021185994148254\n",
      "  200\t0.8014459609985352\n",
      "  201\t0.800933837890625\n",
      "  202\t0.8035739660263062\n",
      "  203\t0.7984097599983215\n",
      "  204\t0.798828661441803\n",
      "  205\t0.7985921502113342\n",
      "  206\t0.8018132448196411\n",
      "  207\t0.7966371178627014\n",
      "  208\t0.8003992438316345\n",
      "  209\t0.7966771721839905\n",
      "  210\t0.7975879907608032\n",
      "  211\t0.7949738502502441\n",
      "  212\t0.8001899123191833\n",
      "  213\t0.8001410961151123\n",
      "  214\t0.7967330813407898\n",
      "  215\t0.7978693842887878\n",
      "  216\t0.7955814599990845\n",
      "  217\t0.7943893074989319\n",
      "  218\t0.7954179048538208\n",
      "  219\t0.7952514290809631\n",
      "  220\t0.7922030091285706\n",
      "  221\t0.7908800840377808\n",
      "  222\t0.7904923558235168\n",
      "  223\t0.7938534021377563\n",
      "  224\t0.7916470170021057\n",
      "  225\t0.7891511917114258\n",
      "  226\t0.7878327369689941\n",
      "  227\t0.7925775647163391\n",
      "  228\t0.7918775081634521\n",
      "  229\t0.7891212105751038\n",
      "  230\t0.7856085300445557\n",
      "  231\t0.7912629246711731\n",
      "  232\t0.7881555557250977\n",
      "  233\t0.7876023650169373\n",
      "  234\t0.7876101136207581\n",
      "  235\t0.7874218225479126\n",
      "  236\t0.783444881439209\n",
      "  237\t0.7880295515060425\n",
      "  238\t0.7883316874504089\n",
      "  239\t0.7860049605369568\n",
      "  240\t0.7847522497177124\n",
      "  241\t0.7830005288124084\n",
      "  242\t0.782162606716156\n",
      "  243\t0.7816832661628723\n",
      "  244\t0.7856889367103577\n",
      "  245\t0.7805575728416443\n",
      "  246\t0.7819646000862122\n",
      "  247\t0.780924916267395\n",
      "  248\t0.781184196472168\n",
      "  249\t0.7831767797470093\n",
      "  250\t0.7776827216148376\n",
      "  251\t0.7790473699569702\n",
      "  252\t0.7811296582221985\n",
      "  253\t0.7834281325340271\n",
      "  254\t0.776109516620636\n",
      "  255\t0.7764934301376343\n",
      "  256\t0.7796710729598999\n",
      "  257\t0.7798023223876953\n",
      "  258\t0.777849018573761\n",
      "  259\t0.7788337469100952\n",
      "  260\t0.7755662202835083\n",
      "  261\t0.7776992321014404\n",
      "  262\t0.7779555320739746\n",
      "  263\t0.776586651802063\n",
      "  264\t0.7763533592224121\n",
      "  265\t0.7728133797645569\n",
      "  266\t0.7729365229606628\n",
      "  267\t0.7714782953262329\n",
      "  268\t0.7754966616630554\n",
      "  269\t0.7752578258514404\n",
      "  270\t0.7711430788040161\n",
      "  271\t0.7700498104095459\n",
      "  272\t0.7716320157051086\n",
      "  273\t0.7747571468353271\n",
      "  274\t0.7719430327415466\n",
      "  275\t0.7696806192398071\n",
      "  276\t0.7727991938591003\n",
      "  277\t0.7705910801887512\n",
      "  278\t0.7753646969795227\n",
      "  279\t0.7705237865447998\n",
      "  280\t0.7736698389053345\n",
      "  281\t0.7688922882080078\n",
      "  282\t0.7709953188896179\n",
      "  283\t0.7707579731941223\n",
      "  284\t0.7713500261306763\n",
      "  285\t0.769042432308197\n",
      "  286\t0.7679123878479004\n",
      "  287\t0.7681487202644348\n",
      "  288\t0.7682517766952515\n",
      "  289\t0.7678156495094299\n",
      "  290\t0.7739551663398743\n",
      "  291\t0.7681363224983215\n",
      "  292\t0.7730331420898438\n",
      "  293\t0.7669176459312439\n",
      "  294\t0.7683340907096863\n",
      "  295\t0.766718327999115\n",
      "  296\t0.7671701312065125\n",
      "  297\t0.766821563243866\n",
      "  298\t0.7669217586517334\n",
      "  299\t0.7623075246810913\n",
      "  300\t0.7668091654777527\n",
      "  301\t0.7639268636703491\n",
      "  302\t0.7690964937210083\n",
      "  303\t0.76300448179245\n",
      "  304\t0.7650148868560791\n",
      "  305\t0.7669113874435425\n",
      "  306\t0.7609186768531799\n",
      "  307\t0.7621393203735352\n",
      "  308\t0.7595730423927307\n",
      "  309\t0.7609712481498718\n",
      "  310\t0.7632237672805786\n",
      "  311\t0.7653252482414246\n",
      "  312\t0.7593392133712769\n",
      "  313\t0.7619568109512329\n",
      "  314\t0.76015704870224\n",
      "  315\t0.7552811503410339\n",
      "  316\t0.7601508498191833\n",
      "  317\t0.7610683441162109\n",
      "  318\t0.761793851852417\n",
      "  319\t0.7619311213493347\n",
      "  320\t0.7596970200538635\n",
      "  321\t0.7641304731369019\n",
      "  322\t0.7611882090568542\n",
      "  323\t0.7595487236976624\n",
      "  324\t0.7595173120498657\n",
      "  325\t0.7627677321434021\n",
      "  326\t0.7612308263778687\n",
      "  327\t0.7620097994804382\n",
      "  328\t0.7567224502563477\n",
      "  329\t0.7541395425796509\n",
      "  330\t0.7581166625022888\n",
      "  331\t0.756022036075592\n",
      "  332\t0.7562357783317566\n",
      "  333\t0.7560380101203918\n",
      "  334\t0.7567023634910583\n",
      "  335\t0.7554963827133179\n",
      "  336\t0.7582128643989563\n",
      "  337\t0.7547798156738281\n",
      "  338\t0.7523841857910156\n",
      "  339\t0.7533225417137146\n",
      "  340\t0.7559593915939331\n",
      "  341\t0.7603739500045776\n",
      "  342\t0.7582609057426453\n",
      "  343\t0.7523666024208069\n",
      "  344\t0.7540163993835449\n",
      "  345\t0.7515220642089844\n",
      "  346\t0.755196213722229\n",
      "  347\t0.7549829483032227\n",
      "  348\t0.7544708847999573\n",
      "  349\t0.7527126669883728\n",
      "  350\t0.7529975175857544\n",
      "  351\t0.7501835823059082\n",
      "  352\t0.7544394731521606\n",
      "  353\t0.7525941133499146\n",
      "  354\t0.7494577169418335\n",
      "  355\t0.7499139904975891\n",
      "  356\t0.7533917427062988\n",
      "  357\t0.7507008910179138\n",
      "  358\t0.7521790862083435\n",
      "  359\t0.7493637800216675\n",
      "  360\t0.7519124150276184\n",
      "  361\t0.7541744709014893\n",
      "  362\t0.7530876398086548\n",
      "  363\t0.7489350438117981\n",
      "  364\t0.7499392032623291\n",
      "  365\t0.7489533424377441\n",
      "  366\t0.7493067979812622\n",
      "  367\t0.74720698595047\n",
      "  368\t0.748310387134552\n",
      "  369\t0.7508603930473328\n",
      "  370\t0.7499353885650635\n",
      "  371\t0.7480989098548889\n",
      "  372\t0.7504665851593018\n",
      "  373\t0.7496839165687561\n",
      "  374\t0.7474884986877441\n",
      "  375\t0.7507943511009216\n",
      "  376\t0.7442930936813354\n",
      "  377\t0.7519048452377319\n",
      "  378\t0.7464465498924255\n",
      "  379\t0.7447022199630737\n",
      "  380\t0.7477574348449707\n",
      "  381\t0.7510635256767273\n",
      "  382\t0.7483893632888794\n",
      "  383\t0.7482956647872925\n",
      "  384\t0.7426644563674927\n",
      "  385\t0.7457940578460693\n",
      "  386\t0.7466244101524353\n",
      "  387\t0.7451964020729065\n",
      "  388\t0.7457821369171143\n",
      "  389\t0.7450761198997498\n",
      "  390\t0.7477735877037048\n",
      "  391\t0.7458882331848145\n",
      "  392\t0.7447803020477295\n",
      "  393\t0.7438873052597046\n",
      "  394\t0.7442196607589722\n",
      "  395\t0.7445997595787048\n",
      "  396\t0.7395654320716858\n",
      "  397\t0.7429126501083374\n",
      "  398\t0.7417664527893066\n",
      "  399\t0.7398427128791809\n",
      "  400\t0.7412770390510559\n",
      "  401\t0.7436250448226929\n",
      "  402\t0.7412447929382324\n",
      "  403\t0.7394255995750427\n",
      "  404\t0.7408669590950012\n",
      "  405\t0.7407265305519104\n",
      "  406\t0.7464034557342529\n",
      "  407\t0.7386638522148132\n",
      "  408\t0.7404813766479492\n",
      "  409\t0.7389612197875977\n",
      "  410\t0.7388105988502502\n",
      "  411\t0.73970627784729\n",
      "  412\t0.7415348887443542\n",
      "  413\t0.7418612241744995\n",
      "  414\t0.7402041554450989\n",
      "  415\t0.7395026683807373\n",
      "  416\t0.7433416843414307\n",
      "  417\t0.7367786765098572\n",
      "  418\t0.7405353784561157\n",
      "  419\t0.7378210425376892\n",
      "  420\t0.7393141984939575\n",
      "  421\t0.7367119193077087\n",
      "  422\t0.7412949204444885\n",
      "  423\t0.7355295419692993\n",
      "  424\t0.7378705143928528\n",
      "  425\t0.742529034614563\n",
      "  426\t0.7357866764068604\n",
      "  427\t0.7384823560714722\n",
      "  428\t0.7368597388267517\n",
      "  429\t0.736327052116394\n",
      "  430\t0.7345435619354248\n",
      "  431\t0.7323529720306396\n",
      "  432\t0.7368961572647095\n",
      "  433\t0.740927517414093\n",
      "  434\t0.7407101988792419\n",
      "  435\t0.7304916381835938\n",
      "  436\t0.7362641096115112\n",
      "  437\t0.7342730760574341\n",
      "  438\t0.733707070350647\n",
      "  439\t0.7303079962730408\n",
      "  440\t0.7363834977149963\n",
      "  441\t0.7374449372291565\n",
      "  442\t0.7333633899688721\n",
      "  443\t0.7316532731056213\n",
      "  444\t0.7305557727813721\n",
      "  445\t0.7334784269332886\n",
      "  446\t0.7326913475990295\n",
      "  447\t0.7332110404968262\n",
      "  448\t0.7371464967727661\n",
      "  449\t0.7309078574180603\n",
      "  450\t0.7314923405647278\n",
      "  451\t0.7336434721946716\n",
      "  452\t0.7284834980964661\n",
      "  453\t0.7339510321617126\n",
      "  454\t0.7341858744621277\n",
      "  455\t0.7317845225334167\n",
      "  456\t0.7311539053916931\n",
      "  457\t0.7337731719017029\n",
      "  458\t0.7306413054466248\n",
      "  459\t0.7307993173599243\n",
      "  460\t0.731049120426178\n",
      "  461\t0.7313089966773987\n",
      "  462\t0.7328321933746338\n",
      "  463\t0.7309103012084961\n",
      "  464\t0.7325189709663391\n",
      "  465\t0.7293194532394409\n",
      "  466\t0.7286699414253235\n",
      "  467\t0.7281774282455444\n",
      "  468\t0.7296802997589111\n",
      "  469\t0.7276133298873901\n",
      "  470\t0.7298762202262878\n",
      "  471\t0.7320475578308105\n",
      "  472\t0.7290371060371399\n",
      "  473\t0.7251943945884705\n",
      "  474\t0.7299574017524719\n",
      "  475\t0.7329698801040649\n",
      "  476\t0.7303982377052307\n",
      "  477\t0.7292770743370056\n",
      "  478\t0.7253767848014832\n",
      "  479\t0.7282394170761108\n",
      "  480\t0.7291257977485657\n",
      "  481\t0.7294052839279175\n",
      "  482\t0.7298404574394226\n",
      "  483\t0.729293167591095\n",
      "  484\t0.7242040634155273\n",
      "  485\t0.7300736308097839\n",
      "  486\t0.7271617650985718\n",
      "  487\t0.7300249934196472\n",
      "  488\t0.7236886620521545\n",
      "  489\t0.7294986248016357\n",
      "  490\t0.7288665771484375\n",
      "  491\t0.724966287612915\n",
      "  492\t0.7258052825927734\n",
      "  493\t0.7252472043037415\n",
      "  494\t0.7284048199653625\n",
      "  495\t0.726172924041748\n",
      "  496\t0.7261859178543091\n",
      "  497\t0.7259421348571777\n",
      "  498\t0.7229461073875427\n",
      "  499\t0.7272513508796692\n",
      "  500\t0.7230724096298218\n",
      "  501\t0.7232964634895325\n",
      "  502\t0.7259218692779541\n",
      "  503\t0.7266345620155334\n",
      "  504\t0.721856415271759\n",
      "  505\t0.7261428236961365\n",
      "  506\t0.7262003421783447\n",
      "  507\t0.7236143946647644\n",
      "  508\t0.7210698127746582\n",
      "  509\t0.7216370105743408\n",
      "  510\t0.7248249053955078\n",
      "  511\t0.7258474826812744\n",
      "  512\t0.726013720035553\n",
      "  513\t0.728853166103363\n",
      "  514\t0.7222933769226074\n",
      "  515\t0.7259254455566406\n",
      "  516\t0.7198138236999512\n",
      "  517\t0.7231660485267639\n",
      "  518\t0.7236217856407166\n",
      "  519\t0.7245790958404541\n",
      "  520\t0.7251268625259399\n",
      "  521\t0.7177903056144714\n",
      "  522\t0.7251944541931152\n",
      "  523\t0.7207306623458862\n",
      "  524\t0.7205679416656494\n",
      "  525\t0.7227752804756165\n",
      "  526\t0.7240713238716125\n",
      "  527\t0.7221733331680298\n",
      "  528\t0.721678614616394\n",
      "  529\t0.7237791419029236\n",
      "  530\t0.7239366769790649\n",
      "  531\t0.7258930206298828\n",
      "  532\t0.7181652784347534\n",
      "  533\t0.7230384349822998\n",
      "  534\t0.7208527326583862\n",
      "  535\t0.7251068949699402\n",
      "  536\t0.7245242595672607\n",
      "  537\t0.7196476459503174\n",
      "  538\t0.7205654978752136\n",
      "  539\t0.7230098247528076\n",
      "  540\t0.7254748344421387\n",
      "  541\t0.7231243848800659\n",
      "  542\t0.7214514017105103\n",
      "  543\t0.725483238697052\n",
      "  544\t0.7211753129959106\n",
      "  545\t0.720309853553772\n",
      "  546\t0.7189444303512573\n",
      "  547\t0.7202309370040894\n",
      "  548\t0.7219379544258118\n",
      "  549\t0.7188726663589478\n",
      "  550\t0.7196664214134216\n",
      "  551\t0.7161856293678284\n",
      "  552\t0.7213180065155029\n",
      "  553\t0.721488356590271\n",
      "  554\t0.7203106880187988\n",
      "  555\t0.721631646156311\n",
      "  556\t0.7183873057365417\n",
      "  557\t0.7199265956878662\n",
      "  558\t0.7170267701148987\n",
      "  559\t0.719632089138031\n",
      "  560\t0.7203183174133301\n",
      "  561\t0.7178919911384583\n",
      "  562\t0.7179297208786011\n",
      "  563\t0.7218431830406189\n",
      "  564\t0.7181127667427063\n",
      "  565\t0.7226916551589966\n",
      "  566\t0.7204601764678955\n",
      "  567\t0.7188513278961182\n",
      "  568\t0.7183224558830261\n",
      "  569\t0.7180283069610596\n",
      "  570\t0.718285322189331\n",
      "  571\t0.7147958874702454\n",
      "  572\t0.7183281183242798\n",
      "  573\t0.7194243669509888\n",
      "  574\t0.7158359289169312\n",
      "  575\t0.714187741279602\n",
      "  576\t0.7177166938781738\n",
      "  577\t0.7193066477775574\n",
      "  578\t0.7164163589477539\n",
      "  579\t0.7172375917434692\n",
      "  580\t0.7180325984954834\n",
      "  581\t0.7150479555130005\n",
      "  582\t0.7201303243637085\n",
      "  583\t0.7148824334144592\n",
      "  584\t0.716107189655304\n",
      "  585\t0.7143301963806152\n",
      "  586\t0.7156990170478821\n",
      "  587\t0.7122446894645691\n",
      "  588\t0.7150138020515442\n",
      "  589\t0.7168986797332764\n",
      "  590\t0.7145394682884216\n",
      "  591\t0.7154960036277771\n",
      "  592\t0.7144162058830261\n",
      "  593\t0.7139264941215515\n",
      "  594\t0.7152799367904663\n",
      "  595\t0.7156632542610168\n",
      "  596\t0.7142449617385864\n",
      "  597\t0.7158218622207642\n",
      "  598\t0.7138844132423401\n",
      "  599\t0.7173503041267395\n",
      "  600\t0.7143109440803528\n",
      "  601\t0.7153652906417847\n",
      "  602\t0.7139492630958557\n",
      "  603\t0.713005006313324\n",
      "  604\t0.7160648107528687\n",
      "  605\t0.7148745059967041\n",
      "  606\t0.7177732586860657\n",
      "  607\t0.7118934392929077\n",
      "  608\t0.7145825028419495\n",
      "  609\t0.7130964398384094\n",
      "  610\t0.7118847966194153\n",
      "  611\t0.7129300832748413\n",
      "  612\t0.7141715288162231\n",
      "  613\t0.7088978886604309\n",
      "  614\t0.7109991312026978\n",
      "  615\t0.7152006030082703\n",
      "  616\t0.7150364518165588\n",
      "  617\t0.7102310061454773\n",
      "  618\t0.7086414694786072\n",
      "  619\t0.7130695581436157\n",
      "  620\t0.7100658416748047\n",
      "  621\t0.7124108076095581\n",
      "  622\t0.7101340293884277\n",
      "  623\t0.7087714672088623\n",
      "  624\t0.7105489373207092\n",
      "  625\t0.7102612257003784\n",
      "  626\t0.7107172608375549\n",
      "  627\t0.7116379737854004\n",
      "  628\t0.7106756567955017\n",
      "  629\t0.7107270956039429\n",
      "  630\t0.7122082114219666\n",
      "  631\t0.711499810218811\n",
      "  632\t0.7064213752746582\n",
      "  633\t0.7100639939308167\n",
      "  634\t0.7107956409454346\n",
      "  635\t0.7124369740486145\n",
      "  636\t0.7098007202148438\n",
      "  637\t0.7077475190162659\n",
      "  638\t0.7069033980369568\n",
      "  639\t0.7098594307899475\n",
      "  640\t0.7091864347457886\n",
      "  641\t0.7084954977035522\n",
      "  642\t0.7103676199913025\n",
      "  643\t0.7073239088058472\n",
      "  644\t0.7110301852226257\n",
      "  645\t0.7123956084251404\n",
      "  646\t0.7081088423728943\n",
      "  647\t0.7065792679786682\n",
      "  648\t0.7120107412338257\n",
      "  649\t0.7075241208076477\n",
      "  650\t0.7084623575210571\n",
      "  651\t0.7057991027832031\n",
      "  652\t0.7080662846565247\n",
      "  653\t0.7066395878791809\n",
      "  654\t0.7090270519256592\n",
      "  655\t0.7097877264022827\n",
      "  656\t0.7088286280632019\n",
      "  657\t0.7057306170463562\n",
      "  658\t0.707474946975708\n",
      "  659\t0.7087579369544983\n",
      "  660\t0.7103908658027649\n",
      "  661\t0.7065469622612\n",
      "  662\t0.7078747153282166\n",
      "  663\t0.7051497101783752\n",
      "  664\t0.7098610997200012\n",
      "  665\t0.704625129699707\n",
      "  666\t0.707112193107605\n",
      "  667\t0.712212860584259\n",
      "  668\t0.7046934366226196\n",
      "  669\t0.7111406922340393\n",
      "  670\t0.7063455581665039\n",
      "  671\t0.7053067088127136\n",
      "  672\t0.7046073079109192\n",
      "  673\t0.705279529094696\n",
      "  674\t0.7007136344909668\n",
      "  675\t0.7098428010940552\n",
      "  676\t0.7086682319641113\n",
      "  677\t0.7069259285926819\n",
      "  678\t0.7092313766479492\n",
      "  679\t0.7088689208030701\n",
      "  680\t0.7085331678390503\n",
      "  681\t0.7002267241477966\n",
      "  682\t0.7072080373764038\n",
      "  683\t0.7052268385887146\n",
      "  684\t0.7075366973876953\n",
      "  685\t0.7069043517112732\n",
      "  686\t0.7077410817146301\n",
      "  687\t0.7037506699562073\n",
      "  688\t0.7086975574493408\n",
      "  689\t0.7054196000099182\n",
      "  690\t0.7037972807884216\n",
      "  691\t0.7087345719337463\n",
      "  692\t0.7073468565940857\n",
      "  693\t0.7043494582176208\n",
      "  694\t0.7046037316322327\n",
      "  695\t0.7067705988883972\n",
      "  696\t0.7072812914848328\n",
      "  697\t0.707504391670227\n",
      "  698\t0.7039378881454468\n",
      "  699\t0.7050101161003113\n",
      "  700\t0.703296422958374\n",
      "  701\t0.7046760320663452\n",
      "  702\t0.7009047865867615\n",
      "  703\t0.7051700353622437\n",
      "  704\t0.7038435339927673\n",
      "  705\t0.710707426071167\n",
      "  706\t0.7071152925491333\n",
      "  707\t0.7024401426315308\n",
      "  708\t0.7061344385147095\n",
      "  709\t0.7032350301742554\n",
      "  710\t0.7078094482421875\n",
      "  711\t0.7030951380729675\n",
      "  712\t0.7039247751235962\n",
      "  713\t0.7042889595031738\n",
      "  714\t0.70387864112854\n",
      "  715\t0.7061963081359863\n",
      "  716\t0.7040902972221375\n",
      "  717\t0.7016568183898926\n",
      "  718\t0.7065205574035645\n",
      "  719\t0.7030576467514038\n",
      "  720\t0.7015786170959473\n",
      "  721\t0.7005170583724976\n",
      "  722\t0.7065658569335938\n",
      "  723\t0.7015150189399719\n",
      "  724\t0.7002158761024475\n",
      "  725\t0.7028157711029053\n",
      "  726\t0.702214777469635\n",
      "  727\t0.7017934322357178\n",
      "  728\t0.7028439044952393\n",
      "  729\t0.6995865106582642\n",
      "  730\t0.7031983733177185\n",
      "  731\t0.6992707848548889\n",
      "  732\t0.7053120136260986\n",
      "  733\t0.7039738297462463\n",
      "  734\t0.7003901600837708\n",
      "  735\t0.7046270966529846\n",
      "  736\t0.6998798251152039\n",
      "  737\t0.7001707553863525\n",
      "  738\t0.6977279782295227\n",
      "  739\t0.7040250301361084\n",
      "  740\t0.7006811499595642\n",
      "  741\t0.7013688683509827\n",
      "  742\t0.7020155191421509\n",
      "  743\t0.6984854340553284\n",
      "  744\t0.6998026967048645\n",
      "  745\t0.6967523694038391\n",
      "  746\t0.6975714564323425\n",
      "  747\t0.6992834210395813\n",
      "  748\t0.696494996547699\n",
      "  749\t0.6991953253746033\n",
      "  750\t0.6976122260093689\n",
      "  751\t0.698760449886322\n",
      "  752\t0.7037398815155029\n",
      "  753\t0.7023531198501587\n",
      "  754\t0.6975986957550049\n",
      "  755\t0.6977220177650452\n",
      "  756\t0.6980069279670715\n",
      "  757\t0.6963563561439514\n",
      "  758\t0.6972157955169678\n",
      "  759\t0.7026287317276001\n",
      "  760\t0.7005702257156372\n",
      "  761\t0.6982153058052063\n",
      "  762\t0.6955106854438782\n",
      "  763\t0.6987959742546082\n",
      "  764\t0.6971739530563354\n",
      "  765\t0.7017497420310974\n",
      "  766\t0.697934627532959\n",
      "  767\t0.697023868560791\n",
      "  768\t0.698276698589325\n",
      "  769\t0.6969746947288513\n",
      "  770\t0.6993427276611328\n",
      "  771\t0.7028258442878723\n",
      "  772\t0.6988064050674438\n",
      "  773\t0.6977313160896301\n",
      "  774\t0.6981021165847778\n",
      "  775\t0.6960404515266418\n",
      "  776\t0.6988975405693054\n",
      "  777\t0.6995802521705627\n",
      "  778\t0.6992406845092773\n",
      "  779\t0.7017800211906433\n",
      "  780\t0.7015538811683655\n",
      "  781\t0.7039250135421753\n",
      "  782\t0.6972615122795105\n",
      "  783\t0.6977917551994324\n",
      "  784\t0.6998612880706787\n",
      "  785\t0.7017661929130554\n",
      "  786\t0.6993386149406433\n",
      "  787\t0.6985790729522705\n",
      "  788\t0.6992982625961304\n",
      "  789\t0.6972710490226746\n",
      "  790\t0.6963411569595337\n",
      "  791\t0.6947475671768188\n",
      "  792\t0.6996798515319824\n",
      "  793\t0.6952406167984009\n",
      "  794\t0.6988329291343689\n",
      "  795\t0.6978413462638855\n",
      "  796\t0.6979454755783081\n",
      "  797\t0.7004161477088928\n",
      "  798\t0.6986462473869324\n",
      "  799\t0.6969931721687317\n",
      "  800\t0.6972429752349854\n",
      "  801\t0.698108434677124\n",
      "  802\t0.6964266896247864\n",
      "  803\t0.695196807384491\n",
      "  804\t0.6982550621032715\n",
      "  805\t0.6948356628417969\n",
      "  806\t0.6920509934425354\n",
      "  807\t0.6937687397003174\n",
      "  808\t0.6975043416023254\n",
      "  809\t0.6942006349563599\n",
      "  810\t0.6978772878646851\n",
      "  811\t0.6938986778259277\n",
      "  812\t0.6955596804618835\n",
      "  813\t0.6946856379508972\n",
      "  814\t0.6975565552711487\n",
      "  815\t0.695943295955658\n",
      "  816\t0.6946678757667542\n",
      "  817\t0.6928611993789673\n",
      "  818\t0.6949740648269653\n",
      "  819\t0.6951256990432739\n",
      "  820\t0.6933152079582214\n",
      "  821\t0.6928623914718628\n",
      "  822\t0.6963854432106018\n",
      "  823\t0.694413959980011\n",
      "  824\t0.6965375542640686\n",
      "  825\t0.6960397958755493\n",
      "  826\t0.6952029466629028\n",
      "  827\t0.6985272169113159\n",
      "  828\t0.6942142248153687\n",
      "  829\t0.6979886889457703\n",
      "  830\t0.6950907707214355\n",
      "  831\t0.6927855014801025\n",
      "  832\t0.6918067932128906\n",
      "  833\t0.693875253200531\n",
      "  834\t0.690966010093689\n",
      "  835\t0.6929168701171875\n",
      "  836\t0.6943930387496948\n",
      "  837\t0.6945953369140625\n",
      "  838\t0.6967965960502625\n",
      "  839\t0.6956582069396973\n",
      "  840\t0.6951665878295898\n",
      "  841\t0.6934826970100403\n",
      "  842\t0.6937918066978455\n",
      "  843\t0.6922168731689453\n",
      "  844\t0.6960682272911072\n",
      "  845\t0.6943649053573608\n",
      "  846\t0.6915329694747925\n",
      "  847\t0.6915591955184937\n",
      "  848\t0.6938427686691284\n",
      "  849\t0.6937772035598755\n",
      "  850\t0.6880106925964355\n",
      "  851\t0.691118061542511\n",
      "  852\t0.6946622133255005\n",
      "  853\t0.6927052736282349\n",
      "  854\t0.6889867782592773\n",
      "  855\t0.6903285384178162\n",
      "  856\t0.6936314105987549\n",
      "  857\t0.6920968890190125\n",
      "  858\t0.6937429904937744\n",
      "  859\t0.6902526617050171\n",
      "  860\t0.6889545321464539\n",
      "  861\t0.6953539848327637\n",
      "  862\t0.6909292340278625\n",
      "  863\t0.6935383677482605\n",
      "  864\t0.6915963888168335\n",
      "  865\t0.690296471118927\n",
      "  866\t0.6885061264038086\n",
      "  867\t0.6907514929771423\n",
      "  868\t0.6925700902938843\n",
      "  869\t0.6894623041152954\n",
      "  870\t0.6924805045127869\n",
      "  871\t0.6935346722602844\n",
      "  872\t0.6960918307304382\n",
      "  873\t0.691280722618103\n",
      "  874\t0.6919485330581665\n",
      "  875\t0.691135823726654\n",
      "  876\t0.6952428221702576\n",
      "  877\t0.6916873455047607\n",
      "  878\t0.6870004534721375\n",
      "  879\t0.6879708170890808\n",
      "  880\t0.6898152232170105\n",
      "  881\t0.6865034699440002\n",
      "  882\t0.6917601227760315\n",
      "  883\t0.6892759799957275\n",
      "  884\t0.6910901069641113\n",
      "  885\t0.6920989155769348\n",
      "  886\t0.6920047998428345\n",
      "  887\t0.6921826601028442\n",
      "  888\t0.6871919631958008\n",
      "  889\t0.6886847615242004\n",
      "  890\t0.6864351034164429\n",
      "  891\t0.6916995644569397\n",
      "  892\t0.6902122497558594\n",
      "  893\t0.685723602771759\n",
      "  894\t0.6908429265022278\n",
      "  895\t0.6873841285705566\n",
      "  896\t0.6880156397819519\n",
      "  897\t0.691994845867157\n",
      "  898\t0.6923058032989502\n",
      "  899\t0.6907646656036377\n",
      "  900\t0.6921234130859375\n",
      "  901\t0.6892687678337097\n",
      "  902\t0.6878237724304199\n",
      "  903\t0.6899259686470032\n",
      "  904\t0.6901058554649353\n",
      "  905\t0.6899808645248413\n",
      "  906\t0.6873623728752136\n",
      "  907\t0.689826250076294\n",
      "  908\t0.6861969828605652\n",
      "  909\t0.6931656002998352\n",
      "  910\t0.6894168257713318\n",
      "  911\t0.6902207136154175\n",
      "  912\t0.6905379891395569\n",
      "  913\t0.6864522099494934\n",
      "  914\t0.6849588751792908\n",
      "  915\t0.6881857514381409\n",
      "  916\t0.6930122375488281\n",
      "  917\t0.6904270648956299\n",
      "  918\t0.6907173991203308\n",
      "  919\t0.6880578398704529\n",
      "  920\t0.6874513030052185\n",
      "  921\t0.6894074082374573\n",
      "  922\t0.6901363730430603\n",
      "  923\t0.6881691813468933\n",
      "  924\t0.6833098530769348\n",
      "  925\t0.6865566372871399\n",
      "  926\t0.683135449886322\n",
      "  927\t0.6849471926689148\n",
      "  928\t0.6885407567024231\n",
      "  929\t0.6871340870857239\n",
      "  930\t0.686955988407135\n",
      "  931\t0.6861799955368042\n",
      "  932\t0.6892523765563965\n",
      "  933\t0.686713695526123\n",
      "  934\t0.6877623200416565\n",
      "  935\t0.6889727115631104\n",
      "  936\t0.6871760487556458\n",
      "  937\t0.6826580166816711\n",
      "  938\t0.6854098439216614\n",
      "  939\t0.6840654611587524\n",
      "  940\t0.6885545253753662\n",
      "  941\t0.6871696710586548\n",
      "  942\t0.685710072517395\n",
      "  943\t0.6872578859329224\n",
      "  944\t0.6873897910118103\n",
      "  945\t0.6908345818519592\n",
      "  946\t0.6866350769996643\n",
      "  947\t0.6867482662200928\n",
      "  948\t0.6872493028640747\n",
      "  949\t0.6853075623512268\n",
      "  950\t0.6847054958343506\n",
      "  951\t0.684865415096283\n",
      "  952\t0.6860573887825012\n",
      "  953\t0.68400639295578\n",
      "  954\t0.6835704445838928\n",
      "  955\t0.684467077255249\n",
      "  956\t0.6900161504745483\n",
      "  957\t0.6901655197143555\n",
      "  958\t0.6887567639350891\n",
      "  959\t0.6878359317779541\n",
      "  960\t0.6807736158370972\n",
      "  961\t0.6874577403068542\n",
      "  962\t0.6832435727119446\n",
      "  963\t0.6850285530090332\n",
      "  964\t0.6849778294563293\n",
      "  965\t0.6831415891647339\n",
      "  966\t0.6856119632720947\n",
      "  967\t0.686881422996521\n",
      "  968\t0.6859074234962463\n",
      "  969\t0.6856574416160583\n",
      "  970\t0.685855507850647\n",
      "  971\t0.6864447593688965\n",
      "  972\t0.6852312088012695\n",
      "  973\t0.6839273571968079\n",
      "  974\t0.6827676892280579\n",
      "  975\t0.6861860156059265\n",
      "  976\t0.6791479587554932\n",
      "  977\t0.6832951903343201\n",
      "  978\t0.6870183348655701\n",
      "  979\t0.6856504082679749\n",
      "  980\t0.6814522743225098\n",
      "  981\t0.6844959855079651\n",
      "  982\t0.6851135492324829\n",
      "  983\t0.6846849322319031\n",
      "  984\t0.6833824515342712\n",
      "  985\t0.6866978406906128\n",
      "  986\t0.6823479533195496\n",
      "  987\t0.6815522313117981\n",
      "  988\t0.6858139634132385\n",
      "  989\t0.68686443567276\n",
      "  990\t0.6834104657173157\n",
      "  991\t0.6865243315696716\n",
      "  992\t0.681341826915741\n",
      "  993\t0.6814939379692078\n",
      "  994\t0.6831472516059875\n",
      "  995\t0.6827748417854309\n",
      "  996\t0.6823732852935791\n",
      "  997\t0.680351197719574\n",
      "  998\t0.6825070977210999\n",
      "  999\t0.6799941658973694\n"
     ]
    }
   ],
   "source": [
    "class NN_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_prob): # set the arguments you'd need, including activation function\n",
    "        super(NN_Classifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layer1 = nn.Linear(self.input_size, 512)\n",
    "        self.hidden2 = nn.Linear(512, 512)\n",
    "        self.hidden3 = nn.Linear(512, 128)\n",
    "        # self.hidden4 = nn.Linear(128, 64)\n",
    "        # self.hidden5 = nn.Linear(64, 32)\n",
    "        #self.hidden6 = nn.Linear(16, 8)\n",
    "        self.output4 = nn.Linear(128, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.hidden3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x = self.hidden4(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.hidden5(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = self.hidden6(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.output4(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, epochs=1000):\n",
    "    model = NN_Classifier(59, 3, 0.3)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=0.005)\n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "    for i in range(epochs):\n",
    "        # reset gradients to 0\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # get predictions\n",
    "        y_pred = model.forward(x_train)\n",
    "        \n",
    "        # compute loss (uncomment the next line and fill right hand side)\n",
    "        abs_loss = loss_fn(y_pred, y_train) \n",
    "\n",
    "        # backpropagate\n",
    "        abs_loss.backward()\n",
    "    \n",
    "        # update the model weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        print (f\"{i:5d}\", abs_loss.item(), sep='\\t')\n",
    "        \n",
    "    return model\n",
    "                \n",
    "NNClassifier = train_model(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8100071-4c27-4dd0-9e6c-c2dd862a8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer1.weight\n",
      "Feature Importance: [0.01775549 0.01878838 0.01877016 0.02162337 0.01999416 0.01750987\n",
      " 0.01538098 0.01739327 0.01686682 0.01480698 0.0159712  0.01767061\n",
      " 0.01562246 0.0167145  0.015358   0.01387335 0.01155199 0.01033696\n",
      " 0.01293942 0.01212627 0.02641577 0.01211676 0.04556694 0.01165832\n",
      " 0.06181817 0.01072241 0.01363614 0.01369777 0.0125476  0.01351092\n",
      " 0.01302378 0.01225249 0.01495396 0.01008663 0.03347662 0.01100742\n",
      " 0.01256576 0.01276261 0.01777695 0.0189276  0.0184885  0.01875623\n",
      " 0.01635609 0.01745308 0.01968682 0.01641028 0.01796969 0.01135249\n",
      " 0.01056141 0.01178804 0.01571593 0.01348253 0.01700652 0.01062824\n",
      " 0.01801188 0.01987842 0.01492241 0.01447268 0.01747986]\n",
      "Layer: hidden2.weight\n",
      "Feature Importance: [0.00192537 0.00174971 0.00186473 0.0019858  0.00195068 0.0018897\n",
      " 0.0025501  0.00212835 0.00199513 0.00179695 0.00185376 0.00177313\n",
      " 0.00226564 0.00201186 0.00199151 0.00188444 0.00189672 0.0019127\n",
      " 0.00209557 0.00208834 0.0021147  0.00190056 0.00214013 0.00176258\n",
      " 0.00197654 0.00200645 0.00175955 0.00184396 0.00191521 0.00190652\n",
      " 0.0021781  0.00190887 0.0017846  0.00199972 0.00200045 0.00193385\n",
      " 0.00218748 0.00192842 0.00183299 0.00176826 0.00186013 0.00207577\n",
      " 0.00198019 0.00181268 0.00194879 0.00210359 0.00216547 0.00191158\n",
      " 0.00200711 0.00178305 0.00216788 0.00180702 0.00183794 0.00185894\n",
      " 0.00234762 0.00203155 0.0020209  0.00201844 0.00188941 0.00197791\n",
      " 0.00180719 0.00186842 0.00196551 0.00203527 0.00205218 0.00201065\n",
      " 0.0021045  0.00201131 0.00178937 0.0019633  0.00218179 0.00196079\n",
      " 0.00194229 0.00190489 0.00193397 0.00185836 0.00186122 0.00205501\n",
      " 0.00202237 0.00201351 0.00221049 0.00203597 0.00208732 0.00242979\n",
      " 0.00188462 0.00196094 0.00169853 0.00190242 0.00204695 0.00199822\n",
      " 0.00204429 0.00211119 0.00200339 0.00187444 0.0020085  0.00185361\n",
      " 0.00197371 0.0022464  0.00193381 0.00195718 0.00206504 0.00202376\n",
      " 0.00188043 0.00216027 0.00176328 0.00195702 0.00196043 0.00185779\n",
      " 0.00209848 0.00192179 0.00189299 0.00198361 0.00186772 0.0017613\n",
      " 0.00206076 0.00187328 0.00209519 0.00180605 0.0017522  0.00197392\n",
      " 0.00212125 0.00176889 0.00187514 0.00191691 0.00218862 0.00205722\n",
      " 0.00183048 0.00209078 0.00180827 0.00197284 0.00198662 0.00212205\n",
      " 0.00189007 0.00212766 0.00182991 0.00206115 0.00195603 0.00198227\n",
      " 0.00190107 0.00192029 0.00203058 0.00187032 0.00178295 0.00188712\n",
      " 0.00195526 0.00213465 0.0019818  0.00199108 0.00186224 0.00217375\n",
      " 0.00187732 0.00197971 0.00181877 0.00191407 0.0020255  0.00219564\n",
      " 0.00176791 0.00181354 0.00201903 0.00207766 0.0017849  0.00163892\n",
      " 0.00212295 0.00180594 0.00193125 0.00205729 0.00190923 0.00193869\n",
      " 0.00209808 0.0019547  0.00185028 0.00172914 0.00186332 0.00208639\n",
      " 0.00186085 0.0021132  0.00193926 0.00184415 0.00209768 0.0023989\n",
      " 0.0020539  0.00183363 0.00175672 0.00193653 0.00181697 0.00193702\n",
      " 0.00237641 0.0019859  0.00193904 0.00198662 0.00196365 0.00180024\n",
      " 0.00183087 0.00190235 0.00189936 0.00233301 0.00186191 0.00194125\n",
      " 0.00198671 0.00191468 0.00202804 0.00187434 0.00220987 0.00192047\n",
      " 0.00203876 0.00191772 0.00181708 0.00192488 0.00195039 0.00191497\n",
      " 0.00209001 0.00201237 0.00198146 0.00205533 0.00198785 0.00188764\n",
      " 0.00204702 0.00188442 0.00193719 0.00181646 0.00197354 0.00196072\n",
      " 0.0018024  0.00192436 0.00195449 0.00168034 0.00175847 0.0019746\n",
      " 0.00202629 0.00190156 0.00201754 0.00200617 0.00191059 0.0017803\n",
      " 0.00191492 0.00190068 0.00175088 0.00197203 0.00178762 0.00204098\n",
      " 0.00184522 0.00205617 0.00207037 0.00176118 0.00201284 0.00181255\n",
      " 0.00174065 0.00191321 0.00205366 0.00207564 0.00197871 0.00193294\n",
      " 0.00176972 0.00198279 0.00212293 0.00197372 0.00200769 0.00185758\n",
      " 0.00175898 0.00181909 0.00188074 0.00171834 0.00209962 0.00221819\n",
      " 0.00181028 0.00208208 0.00194902 0.00207718 0.00184412 0.00211118\n",
      " 0.00194036 0.00207225 0.00210105 0.00213307 0.00206503 0.0021292\n",
      " 0.00191389 0.00189841 0.00192944 0.00183049 0.00179937 0.00186405\n",
      " 0.00186243 0.0018457  0.00191218 0.00204666 0.00178023 0.00204415\n",
      " 0.00198092 0.00206166 0.00174474 0.00201027 0.00185751 0.00175204\n",
      " 0.0021875  0.00188184 0.0018916  0.00187786 0.00208628 0.00176894\n",
      " 0.00205626 0.00187336 0.00184555 0.00192603 0.00186209 0.00177656\n",
      " 0.00187334 0.00191774 0.00181073 0.00195915 0.00181195 0.00192937\n",
      " 0.00188801 0.00197085 0.001823   0.00188942 0.00223768 0.00212289\n",
      " 0.00199275 0.00187678 0.00198167 0.00222545 0.00184934 0.00175646\n",
      " 0.00194197 0.00192309 0.00213939 0.00232007 0.00192236 0.0018896\n",
      " 0.00179794 0.00190153 0.00177198 0.00174421 0.00200463 0.00195294\n",
      " 0.00203208 0.00202715 0.001961   0.00196223 0.00174266 0.00215677\n",
      " 0.00179212 0.00204608 0.00180871 0.00201428 0.00190699 0.00193724\n",
      " 0.00200098 0.00207754 0.0018225  0.00198103 0.00177051 0.00183\n",
      " 0.00186571 0.00212387 0.00217367 0.00189635 0.00195217 0.00195646\n",
      " 0.00181232 0.00194046 0.00205131 0.00187797 0.00177799 0.00205339\n",
      " 0.0016979  0.00204017 0.00180757 0.00185234 0.00207485 0.00185089\n",
      " 0.00190158 0.00200309 0.00208959 0.00210796 0.00202306 0.0018954\n",
      " 0.00209826 0.00193238 0.00189017 0.00202706 0.00218152 0.00186429\n",
      " 0.00196874 0.00200715 0.00180343 0.00180427 0.00209291 0.00186144\n",
      " 0.00199184 0.00183521 0.00198823 0.00203729 0.00205951 0.00181683\n",
      " 0.00192847 0.00181928 0.00207811 0.00192884 0.00166253 0.00181035\n",
      " 0.00189726 0.00184192 0.00221417 0.0017804  0.00215109 0.00218335\n",
      " 0.00188653 0.00199596 0.00178398 0.00170876 0.00187037 0.00218511\n",
      " 0.00181478 0.0020943  0.001978   0.00203682 0.00187061 0.00215647\n",
      " 0.00197275 0.00178264 0.00179765 0.00188934 0.00192718 0.00210145\n",
      " 0.00217008 0.00215842 0.00203222 0.00196155 0.00195538 0.00208097\n",
      " 0.00190251 0.0020995  0.00184012 0.002191   0.00196843 0.00188154\n",
      " 0.00183289 0.00178071 0.00187319 0.00182008 0.00219943 0.00225201\n",
      " 0.00201666 0.00191132 0.00226548 0.00201016 0.00174265 0.00177559\n",
      " 0.00195946 0.00181235 0.00186766 0.00207333 0.00195696 0.00192641\n",
      " 0.00176237 0.00178103 0.00188217 0.00208924 0.00172817 0.00181592\n",
      " 0.0020168  0.00182547 0.0019993  0.00190681 0.00205137 0.00185413\n",
      " 0.00195807 0.00183321 0.00175718 0.00197719 0.00190457 0.00187433\n",
      " 0.00196989 0.00188513 0.00188392 0.00186726 0.00212221 0.00190924\n",
      " 0.00201187 0.00185626 0.00177985 0.00199553 0.00222951 0.00202693\n",
      " 0.0020373  0.00182018 0.00206533 0.00198441 0.00173008 0.00194964\n",
      " 0.00198003 0.00240866 0.0017674  0.0018195  0.00208332 0.00195796\n",
      " 0.00201022 0.00192582 0.00214703 0.00198798 0.00199849 0.00180406\n",
      " 0.00200737 0.0019607  0.00184353 0.00212738 0.00207831 0.00219611\n",
      " 0.00182848 0.00192408]\n",
      "Layer: hidden3.weight\n",
      "Feature Importance: [0.00337257 0.00072181 0.00075754 0.00069996 0.00351688 0.00317396\n",
      " 0.0007292  0.00068794 0.00078249 0.00363528 0.00266409 0.00416416\n",
      " 0.00074848 0.00072874 0.00068426 0.00391509 0.00067001 0.00084185\n",
      " 0.00071792 0.00417746 0.00065133 0.00375495 0.00454002 0.00071115\n",
      " 0.00078003 0.00458646 0.00385796 0.00404225 0.00067197 0.00066917\n",
      " 0.00301141 0.00071542 0.00069999 0.00324922 0.00355856 0.0006699\n",
      " 0.00292212 0.00077756 0.00072791 0.00063743 0.00375225 0.00074617\n",
      " 0.00076721 0.00342756 0.00071786 0.00077526 0.00068998 0.00070127\n",
      " 0.00073584 0.00080526 0.00070268 0.00071258 0.00071632 0.00276818\n",
      " 0.00073097 0.0009385  0.00067109 0.00073825 0.00333628 0.00161888\n",
      " 0.00077701 0.00378988 0.00316841 0.00072642 0.0039066  0.00070788\n",
      " 0.00072401 0.00075313 0.00375255 0.00069898 0.00339529 0.00347209\n",
      " 0.00332763 0.00352749 0.00354024 0.00066684 0.00071343 0.00070681\n",
      " 0.00076542 0.00069558 0.00312476 0.00340613 0.00068927 0.00359251\n",
      " 0.00075917 0.00287717 0.00332552 0.00070366 0.0007302  0.00074504\n",
      " 0.00417056 0.00069153 0.00070858 0.0007705  0.00075523 0.00319731\n",
      " 0.00076052 0.004007   0.00079017 0.00066771 0.00070272 0.00076094\n",
      " 0.00071063 0.00293995 0.00070118 0.00283497 0.00066082 0.00072611\n",
      " 0.0006912  0.00068949 0.00067693 0.00305697 0.00080082 0.0033003\n",
      " 0.00076856 0.00269957 0.00071282 0.00067994 0.00069229 0.00070874\n",
      " 0.00431778 0.00072945 0.00386075 0.00366645 0.00364097 0.00358603\n",
      " 0.00071792 0.00075535 0.00486577 0.00061998 0.00335548 0.00073134\n",
      " 0.00290631 0.00069358 0.00307168 0.00068552 0.0035648  0.00068529\n",
      " 0.00077405 0.00079349 0.00393732 0.00070344 0.00351193 0.00072525\n",
      " 0.00323204 0.00071124 0.00340827 0.0032621  0.00072318 0.00070669\n",
      " 0.00257349 0.00072794 0.00066096 0.00069237 0.000804   0.00076627\n",
      " 0.00073483 0.00395308 0.00351696 0.00087853 0.00240863 0.00343867\n",
      " 0.00393243 0.00395175 0.00068374 0.00073709 0.00075881 0.00358686\n",
      " 0.00073125 0.00071096 0.00359745 0.00067361 0.00380343 0.00077707\n",
      " 0.00072483 0.00347335 0.00373134 0.00073029 0.00074523 0.00073252\n",
      " 0.00240781 0.00073745 0.00296809 0.00388948 0.00072845 0.00423394\n",
      " 0.00308056 0.00065752 0.0016793  0.00073071 0.00250738 0.00074758\n",
      " 0.00491869 0.0042183  0.00374247 0.00069557 0.00068738 0.00340747\n",
      " 0.00070409 0.00406755 0.00351551 0.00072915 0.00076293 0.00065454\n",
      " 0.0007209  0.00070409 0.00311508 0.0007034  0.00388356 0.00073273\n",
      " 0.00077274 0.00502755 0.00405996 0.00381723 0.00073867 0.00074571\n",
      " 0.00090507 0.00070477 0.00074493 0.00071438 0.00075012 0.00073262\n",
      " 0.00063985 0.00386608 0.00068443 0.00308538 0.00069776 0.00363321\n",
      " 0.00072399 0.00363938 0.00076033 0.00267567 0.00068852 0.00399783\n",
      " 0.00070904 0.00320507 0.00076273 0.0007779  0.00072823 0.00384484\n",
      " 0.0007156  0.00074589 0.00069988 0.00372089 0.00406718 0.00478422\n",
      " 0.00189704 0.00077712 0.00078008 0.00426745 0.00077037 0.00296811\n",
      " 0.00066125 0.00069953 0.00075768 0.00434836 0.00080897 0.00069962\n",
      " 0.000771   0.00074842 0.00362851 0.00404141 0.00068806 0.00293487\n",
      " 0.00345902 0.00081152 0.00501793 0.00413804 0.00283861 0.00337257\n",
      " 0.00070817 0.00071664 0.00075937 0.00353412 0.00070003 0.00066556\n",
      " 0.00070436 0.00075948 0.00361693 0.00072109 0.00363293 0.0006315\n",
      " 0.00371783 0.00072023 0.0032957  0.00084713 0.00071833 0.00066775\n",
      " 0.00418542 0.00067641 0.00073992 0.00349327 0.0034448  0.00272624\n",
      " 0.00068832 0.00074198 0.00074788 0.00084985 0.00075691 0.00318986\n",
      " 0.00064588 0.00074404 0.00072282 0.00070026 0.00385199 0.00062868\n",
      " 0.00385629 0.00355253 0.00068354 0.00073726 0.00416036 0.00077922\n",
      " 0.00071023 0.00291929 0.00313702 0.00072937 0.00428109 0.00068541\n",
      " 0.00412472 0.0007109  0.00432222 0.00063575 0.00393298 0.00430835\n",
      " 0.00078722 0.00080045 0.00337405 0.00075572 0.00360958 0.00392165\n",
      " 0.00410436 0.00352778 0.00069756 0.00238554 0.0033875  0.00364162\n",
      " 0.00073241 0.00332993 0.00395033 0.00073214 0.000715   0.00328666\n",
      " 0.00069899 0.00365545 0.00387068 0.00403456 0.00071429 0.00072451\n",
      " 0.00071283 0.0059815  0.00352894 0.00061699 0.00068691 0.00071187\n",
      " 0.00435271 0.00069603 0.00368471 0.00289023 0.00084937 0.00073807\n",
      " 0.00065913 0.00272421 0.00363767 0.00072384 0.00078775 0.00068306\n",
      " 0.00070805 0.00070958 0.00292059 0.00408508 0.00368705 0.00069866\n",
      " 0.00076331 0.00069222 0.00129077 0.00071236 0.00353093 0.00242717\n",
      " 0.00072736 0.00071877 0.0051643  0.00379718 0.00066765 0.00503409\n",
      " 0.00064308 0.00071781 0.00064599 0.0007344  0.00068775 0.00079774\n",
      " 0.00079053 0.00381043 0.00071831 0.00334183 0.00436089 0.00343841\n",
      " 0.0033709  0.0006557  0.00075422 0.00411055 0.00074681 0.00077365\n",
      " 0.0006626  0.0007452  0.00401693 0.00076639 0.00080551 0.00068826\n",
      " 0.00068549 0.00073821 0.00075655 0.00061551 0.00064135 0.00373164\n",
      " 0.00281178 0.00079239 0.00300435 0.00080289 0.00069945 0.00071376\n",
      " 0.00068776 0.00378207 0.00320258 0.003137   0.000696   0.00063639\n",
      " 0.00440297 0.00347311 0.00660564 0.00296806 0.00071812 0.00072273\n",
      " 0.00072881 0.00316484 0.00077584 0.00321327 0.000736   0.00354437\n",
      " 0.00362263 0.00310099 0.004489   0.00076173 0.00354814 0.00077809\n",
      " 0.00068025 0.00087153 0.0040567  0.00418946 0.00361805 0.00075119\n",
      " 0.00078638 0.00539349 0.0006755  0.00081434 0.00066582 0.00076648\n",
      " 0.00063834 0.00071299 0.00174877 0.00314883 0.00078044 0.00397699\n",
      " 0.00076661 0.00399226 0.0041687  0.00370347 0.00072236 0.00321417\n",
      " 0.00072847 0.00068283 0.00340814 0.00344611 0.00699914 0.00400498\n",
      " 0.00064771 0.00355026 0.00077806 0.00065877 0.00076255 0.00065472\n",
      " 0.0007244  0.00068503 0.00300629 0.00073186 0.00074525 0.00369586\n",
      " 0.00071683 0.00077885 0.00221813 0.00079611 0.00072419 0.00072132\n",
      " 0.00075453 0.0034961  0.00343356 0.00076545 0.00378709 0.00067154\n",
      " 0.00073145 0.00428846 0.00314914 0.00333288 0.00338031 0.00426372\n",
      " 0.00357499 0.00073885 0.00487226 0.00071347 0.00425559 0.00074644\n",
      " 0.00066025 0.00359829]\n",
      "Layer: output4.weight\n",
      "Feature Importance: [0.00465376 0.00631525 0.00555101 0.01604612 0.00497284 0.00995461\n",
      " 0.0110552  0.00057103 0.00700757 0.00552111 0.00741995 0.00661452\n",
      " 0.00776265 0.01527134 0.00330495 0.00975127 0.00111746 0.01007139\n",
      " 0.01100567 0.00587314 0.00933938 0.00666409 0.01044285 0.00626336\n",
      " 0.01134923 0.00044222 0.00719009 0.00117272 0.00111823 0.00772823\n",
      " 0.01314929 0.0016586  0.00891649 0.00609437 0.00382493 0.00411092\n",
      " 0.00236113 0.00670554 0.01231415 0.00426319 0.00873839 0.0171356\n",
      " 0.00695596 0.00779984 0.00734283 0.00733629 0.00688376 0.00091513\n",
      " 0.00678846 0.00638354 0.01286884 0.0048367  0.0075555  0.0053231\n",
      " 0.01529301 0.00405455 0.00925149 0.0135212  0.00560338 0.00078348\n",
      " 0.00789876 0.00345936 0.0091941  0.00608784 0.00849667 0.00767443\n",
      " 0.01004853 0.00896368 0.0087657  0.01299427 0.00132257 0.00714228\n",
      " 0.01320507 0.00072055 0.0077948  0.00548556 0.00848773 0.01344899\n",
      " 0.00110063 0.00150817 0.00871465 0.00197493 0.00535865 0.00700415\n",
      " 0.01001978 0.00573347 0.00934872 0.0010932  0.01512858 0.00912472\n",
      " 0.00716396 0.00944527 0.00770853 0.00984626 0.01023733 0.01709378\n",
      " 0.01517996 0.01540908 0.00676967 0.01527189 0.0091189  0.00803704\n",
      " 0.00758504 0.00108462 0.00480862 0.01538911 0.00684533 0.00742189\n",
      " 0.00576475 0.00848288 0.01202347 0.00606641 0.00834263 0.00078345\n",
      " 0.01607557 0.00673266 0.01449578 0.01464165 0.00113687 0.01391381\n",
      " 0.00791434 0.00630564 0.01033644 0.00725319 0.01333144 0.0062631\n",
      " 0.00596433 0.01213384]\n"
     ]
    }
   ],
   "source": [
    "def get_feature_importance(model):\n",
    "    feature_importance = dict()\n",
    "\n",
    "    # Loop through each linear layer\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter is a weight matrix of a linear layer\n",
    "        if 'weight' in name:\n",
    "            # Calculate the absolute sum of weights for each feature\n",
    "            importance = torch.abs(param).sum(dim=0).detach().numpy()\n",
    "            # Normalize the importance scores\n",
    "            importance /= importance.sum()\n",
    "            # Store the importance scores for the layer\n",
    "            feature_importance[name] = importance\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "# Get feature importance for the trained model\n",
    "feature_importance = get_feature_importance(NNClassifier)\n",
    "\n",
    "# Print feature importance for each layer\n",
    "for layer, importance in feature_importance.items():\n",
    "    print(f'Layer: {layer}')\n",
    "    print('Feature Importance:', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ffb7831-a078-4e7d-a430-b17366de4ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: admission_type_EU OBSERVATION, Importance: 0.06181817\n",
      "Feature: admission_type_DIRECT OBSERVATION, Importance: 0.04556694\n",
      "Feature: first_careunit_Neuro Stepdown, Importance: 0.03347662\n",
      "Feature: race_encode_South American, Importance: 0.02641577\n",
      "Feature: height, Importance: 0.02162337\n",
      "Feature: charlson_score, Importance: 0.01999416\n",
      "Feature: wbc, Importance: 0.01987842\n",
      "Feature: glucose, Importance: 0.01968682\n",
      "Feature: bicarbonate, Importance: 0.0189276\n",
      "Feature: admission_age, Importance: 0.01878838\n",
      "Feature: weight_admit, Importance: 0.01877016\n",
      "Feature: calcium, Importance: 0.01875623\n",
      "Feature: bun, Importance: 0.0184885\n",
      "Feature: rdw, Importance: 0.01801188\n",
      "Feature: potassium, Importance: 0.01796969\n",
      "Feature: aniongap, Importance: 0.01777695\n",
      "Feature: gender, Importance: 0.01775549\n",
      "Feature: diabetes, Importance: 0.01767061\n",
      "Feature: atrial_fibrillation, Importance: 0.01750987\n",
      "Feature: ptt, Importance: 0.01747986\n",
      "Feature: creatinine, Importance: 0.01745308\n",
      "Feature: chf, Importance: 0.01739327\n",
      "Feature: platelet, Importance: 0.01700652\n",
      "Feature: ckd, Importance: 0.01686682\n",
      "Feature: ihd, Importance: 0.0167145\n",
      "Feature: sodium, Importance: 0.01641028\n",
      "Feature: chloride, Importance: 0.01635609\n",
      "Feature: copd, Importance: 0.0159712\n",
      "Feature: mchc, Importance: 0.01571593\n",
      "Feature: hypertension, Importance: 0.01562246\n",
      "Feature: malignant_cancer, Importance: 0.01538098\n",
      "Feature: stroke, Importance: 0.015358\n",
      "Feature: first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU), Importance: 0.01495396\n",
      "Feature: inr, Importance: 0.01492241\n",
      "Feature: cld, Importance: 0.01480698\n",
      "Feature: pt, Importance: 0.01447268\n",
      "Feature: race_encode_African, Importance: 0.01387335\n",
      "Feature: admission_type_SURGICAL SAME DAY ADMISSION, Importance: 0.01369777\n",
      "Feature: admission_type_OBSERVATION ADMIT, Importance: 0.01363614\n",
      "Feature: first_careunit_Cardiac Vascular Intensive Care Unit (CVICU), Importance: 0.01351092\n",
      "Feature: mcv, Importance: 0.01348253\n",
      "Feature: first_careunit_Coronary Care Unit (CCU), Importance: 0.01302378\n",
      "Feature: race_encode_Hispanic, Importance: 0.01293942\n",
      "Feature: first_careunit_Trauma SICU (TSICU), Importance: 0.01276261\n",
      "Feature: first_careunit_Surgical Intensive Care Unit (SICU), Importance: 0.01256576\n",
      "Feature: admission_type_URGENT, Importance: 0.0125476\n",
      "Feature: first_careunit_Medical Intensive Care Unit (MICU), Importance: 0.01225249\n",
      "Feature: race_encode_Not Specified, Importance: 0.01212627\n",
      "Feature: admission_type_DIRECT EMER., Importance: 0.01211676\n",
      "Feature: mch, Importance: 0.01178804\n",
      "Feature: admission_type_ELECTIVE, Importance: 0.01165832\n",
      "Feature: race_encode_Asian, Importance: 0.01155199\n",
      "Feature: hematocrit, Importance: 0.01135249\n",
      "Feature: first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU), Importance: 0.01100742\n",
      "Feature: admission_type_EW EMER., Importance: 0.01072241\n",
      "Feature: rbc, Importance: 0.01062824\n",
      "Feature: hemoglobin, Importance: 0.01056141\n",
      "Feature: race_encode_Caucasian, Importance: 0.01033696\n",
      "Feature: first_careunit_Neuro Intermediate, Importance: 0.01008663\n",
      "['admission_type_EU OBSERVATION', 'admission_type_DIRECT OBSERVATION', 'first_careunit_Neuro Stepdown', 'race_encode_South American', 'height', 'charlson_score', 'wbc', 'glucose', 'bicarbonate', 'admission_age', 'weight_admit', 'calcium', 'bun', 'rdw', 'potassium', 'aniongap', 'gender', 'diabetes', 'atrial_fibrillation', 'ptt', 'creatinine', 'chf', 'platelet', 'ckd', 'ihd', 'sodium', 'chloride', 'copd', 'mchc', 'hypertension', 'malignant_cancer', 'stroke', 'first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)', 'inr', 'cld', 'pt', 'race_encode_African', 'admission_type_SURGICAL SAME DAY ADMISSION', 'admission_type_OBSERVATION ADMIT', 'first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)', 'mcv', 'first_careunit_Coronary Care Unit (CCU)', 'race_encode_Hispanic', 'first_careunit_Trauma SICU (TSICU)', 'first_careunit_Surgical Intensive Care Unit (SICU)', 'admission_type_URGENT', 'first_careunit_Medical Intensive Care Unit (MICU)', 'race_encode_Not Specified', 'admission_type_DIRECT EMER.', 'mch', 'admission_type_ELECTIVE', 'race_encode_Asian', 'hematocrit', 'first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)', 'admission_type_EW EMER.', 'rbc', 'hemoglobin', 'race_encode_Caucasian', 'first_careunit_Neuro Intermediate']\n"
     ]
    }
   ],
   "source": [
    "Feature_Importance = [0.01775549, 0.01878838, 0.01877016, 0.02162337, 0.01999416, 0.01750987,\n",
    " 0.01538098, 0.01739327, 0.01686682, 0.01480698, 0.0159712,  0.01767061,\n",
    " 0.01562246, 0.0167145,  0.015358,   0.01387335, 0.01155199, 0.01033696,\n",
    " 0.01293942, 0.01212627, 0.02641577, 0.01211676, 0.04556694, 0.01165832,\n",
    " 0.06181817, 0.01072241, 0.01363614, 0.01369777, 0.0125476,  0.01351092,\n",
    " 0.01302378, 0.01225249, 0.01495396, 0.01008663, 0.03347662, 0.01100742,\n",
    " 0.01256576, 0.01276261, 0.01777695, 0.0189276,  0.0184885,  0.01875623,\n",
    " 0.01635609, 0.01745308, 0.01968682, 0.01641028, 0.01796969, 0.01135249,\n",
    " 0.01056141, 0.01178804, 0.01571593, 0.01348253, 0.01700652, 0.01062824,\n",
    " 0.01801188, 0.01987842, 0.01492241, 0.01447268, 0.01747986]\n",
    "feature_names = X_train.columns\n",
    "importance_mapping = dict(zip(feature_names, Feature_Importance))\n",
    "\n",
    "sorted_importance = sorted(importance_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "feature_ranking = []\n",
    "feature_ranking_total = []\n",
    "\n",
    "# Print or display the sorted feature importance scores\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"Feature: {feature}, Importance: {score}\")\n",
    "    feature_ranking.append(feature)\n",
    "    feature_ranking_total.append(feature)\n",
    "\n",
    "print(feature_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37e2ddb0-d3eb-4502-af1a-b7d140d8743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3958333333333333\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn MLPclassifier for feature selection\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "MLP_NN = MLPClassifier(hidden_layer_sizes=(128,32), activation='relu', solver='adam', max_iter=500, random_state=26)\n",
    "MLP_NN.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = MLP_NN.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f6d2e4e-5c9d-408a-ab32-352fd2008a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection_f1_test(X, y, X_test, y_test, feature_ranking, model, metric, max_features=None):\n",
    "    selected_features = []\n",
    "    best_performance = float('-inf')\n",
    "    \n",
    "    if max_features is None:\n",
    "        max_features = len(feature_ranking)\n",
    "    \n",
    "    for feature in feature_ranking:\n",
    "        selected_features.append(feature)\n",
    "        X_subset = X[selected_features]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_subset, y)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        y_pred = model.predict(X_test[selected_features])\n",
    "        performance = metric(y_test, model.predict(X_test[selected_features]), average='macro')\n",
    "        \n",
    "        # Check if performance improved\n",
    "        if performance - best_performance >= -0.01:\n",
    "            best_performance = performance\n",
    "            if len(selected_features) == max_features:\n",
    "                break\n",
    "        elif performance - best_performance < -0.01:\n",
    "            selected_features.pop()\n",
    "            break\n",
    "        # if performance > best_performance:\n",
    "        #     best_performance = performance\n",
    "        #     if len(selected_features) == max_features:\n",
    "        #         break\n",
    "        # elif performance <= best_performance:\n",
    "        #     selected_features.pop()\n",
    "        #     break\n",
    "    \n",
    "    return selected_features, best_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "853d1ba3-a2ee-45eb-94de-bd4eb84714e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['admission_type_EU OBSERVATION',\n",
       "  'admission_type_DIRECT OBSERVATION',\n",
       "  'first_careunit_Neuro Stepdown',\n",
       "  'race_encode_South American',\n",
       "  'height',\n",
       "  'charlson_score',\n",
       "  'wbc',\n",
       "  'glucose',\n",
       "  'bicarbonate',\n",
       "  'admission_age',\n",
       "  'weight_admit'],\n",
       " 0.4173707516201612)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_feature_selection_f1_test(X_train, y_train, X_test, y_test, feature_ranking, MLP_NN, f1_score, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970a8746-8e24-4109-8dba-ac197cbd31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_feature_selection_roc_test(X, y, X_test, y_test, feature_ranking, model, metric):\n",
    "    \n",
    "    best_performance = float('-inf')\n",
    "    selected_features = feature_ranking.copy()\n",
    "    model.fit(X, y) \n",
    "    y_pred = model.predict(X_test)\n",
    "    best_performance = metric(y_test, model.predict(X_test), average='macro')\n",
    "    for i in range(len(feature_ranking)-1):\n",
    "        last_feature = selected_features.pop()\n",
    "        model.fit(X[selected_features], y) \n",
    "        y_pred = model.predict(X_test[selected_features])\n",
    "        performance = metric(y_test, model.predict(X_test[selected_features]), average='macro')\n",
    "        if best_performance - performance < 0.01:\n",
    "            best_performance = performance\n",
    "        else:\n",
    "            selected_features.append(last_feature)\n",
    "            break\n",
    "\n",
    "    \n",
    "    return selected_features, best_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b2c062d-71fb-4651-94d0-958083df4975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['admission_type_EU OBSERVATION',\n",
       "  'admission_type_DIRECT OBSERVATION',\n",
       "  'first_careunit_Neuro Stepdown',\n",
       "  'race_encode_South American',\n",
       "  'height',\n",
       "  'charlson_score',\n",
       "  'wbc',\n",
       "  'glucose',\n",
       "  'bicarbonate',\n",
       "  'admission_age',\n",
       "  'weight_admit',\n",
       "  'calcium',\n",
       "  'bun',\n",
       "  'rdw',\n",
       "  'potassium',\n",
       "  'aniongap',\n",
       "  'gender',\n",
       "  'diabetes',\n",
       "  'atrial_fibrillation',\n",
       "  'ptt',\n",
       "  'creatinine',\n",
       "  'chf',\n",
       "  'platelet',\n",
       "  'ckd',\n",
       "  'ihd',\n",
       "  'sodium',\n",
       "  'chloride',\n",
       "  'copd',\n",
       "  'mchc',\n",
       "  'hypertension',\n",
       "  'malignant_cancer',\n",
       "  'stroke',\n",
       "  'first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)',\n",
       "  'inr',\n",
       "  'cld',\n",
       "  'pt',\n",
       "  'race_encode_African',\n",
       "  'admission_type_SURGICAL SAME DAY ADMISSION',\n",
       "  'admission_type_OBSERVATION ADMIT',\n",
       "  'first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)',\n",
       "  'mcv',\n",
       "  'first_careunit_Coronary Care Unit (CCU)',\n",
       "  'race_encode_Hispanic',\n",
       "  'first_careunit_Trauma SICU (TSICU)',\n",
       "  'first_careunit_Surgical Intensive Care Unit (SICU)',\n",
       "  'admission_type_URGENT',\n",
       "  'first_careunit_Medical Intensive Care Unit (MICU)',\n",
       "  'race_encode_Not Specified',\n",
       "  'admission_type_DIRECT EMER.',\n",
       "  'mch',\n",
       "  'admission_type_ELECTIVE',\n",
       "  'race_encode_Asian',\n",
       "  'hematocrit',\n",
       "  'first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)',\n",
       "  'admission_type_EW EMER.'],\n",
       " 0.4008410468071499)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_feature_selection_roc_test(X_train, y_train, X_test, y_test, feature_ranking, MLP_NN, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6108c1a5-8b77-41b6-9a14-e781136d326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# generating the first 2 parent chromosomes for genetic feature selection algorithm\n",
    "\n",
    "forward_features = ['admission_type_EU OBSERVATION',\n",
    "  'admission_type_DIRECT OBSERVATION',\n",
    "  'first_careunit_Neuro Stepdown',\n",
    "  'race_encode_South American',\n",
    "  'height',\n",
    "  'charlson_score',\n",
    "  'wbc',\n",
    "  'glucose',\n",
    "  'bicarbonate',\n",
    "  'admission_age',\n",
    "  'weight_admit']\n",
    "\n",
    "backward_features = ['admission_type_EU OBSERVATION',\n",
    "  'admission_type_DIRECT OBSERVATION',\n",
    "  'first_careunit_Neuro Stepdown',\n",
    "  'race_encode_South American',\n",
    "  'height',\n",
    "  'charlson_score',\n",
    "  'wbc',\n",
    "  'glucose',\n",
    "  'bicarbonate',\n",
    "  'admission_age',\n",
    "  'weight_admit',\n",
    "  'calcium',\n",
    "  'bun',\n",
    "  'rdw',\n",
    "  'potassium',\n",
    "  'aniongap',\n",
    "  'gender',\n",
    "  'diabetes',\n",
    "  'atrial_fibrillation',\n",
    "  'ptt',\n",
    "  'creatinine',\n",
    "  'chf',\n",
    "  'platelet',\n",
    "  'ckd',\n",
    "  'ihd',\n",
    "  'sodium',\n",
    "  'chloride',\n",
    "  'copd',\n",
    "  'mchc',\n",
    "  'hypertension',\n",
    "  'malignant_cancer',\n",
    "  'stroke',\n",
    "  'first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)',\n",
    "  'inr',\n",
    "  'cld',\n",
    "  'pt',\n",
    "  'race_encode_African',\n",
    "  'admission_type_SURGICAL SAME DAY ADMISSION',\n",
    "  'admission_type_OBSERVATION ADMIT',\n",
    "  'first_careunit_Cardiac Vascular Intensive Care Unit (CVICU)',\n",
    "  'mcv',\n",
    "  'first_careunit_Coronary Care Unit (CCU)',\n",
    "  'race_encode_Hispanic',\n",
    "  'first_careunit_Trauma SICU (TSICU)',\n",
    "  'first_careunit_Surgical Intensive Care Unit (SICU)',\n",
    "  'admission_type_URGENT',\n",
    "  'first_careunit_Medical Intensive Care Unit (MICU)',\n",
    "  'race_encode_Not Specified',\n",
    "  'admission_type_DIRECT EMER.',\n",
    "  'mch',\n",
    "  'admission_type_ELECTIVE',\n",
    "  'race_encode_Asian',\n",
    "  'hematocrit',\n",
    "  'first_careunit_Neuro Surgical Intensive Care Unit (Neuro SICU)',\n",
    "  'admission_type_EW EMER.']\n",
    "\n",
    "forward_parent_chromosome = [1 if feature in forward_features else 0 for feature in feature_ranking_total]\n",
    "backward_parent_chromosome = [1 if feature in backward_features else 0 for feature in feature_ranking_total]\n",
    "print(forward_parent_chromosome)\n",
    "print(backward_parent_chromosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9696ddc-140b-4e38-b640-7f64359de1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33361424-c4ec-4df4-ba3b-f409a79b213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneal.genetic_algorithms import BinaryGenAlgSolver\n",
    "\n",
    "class MyBinaryGenAlgSolver(BinaryGenAlgSolver):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def fitness_function(self, chromosome):\n",
    "        selected_features = np.where(chromosome)[0]\n",
    "        selected_features_list = []\n",
    "        for i in selected_features:\n",
    "            selected_features_list.append(column_list[i])\n",
    "        X_train_subset = X_train[selected_features_list]\n",
    "    \n",
    "        MLP_NN.fit(X_train_subset, y_train)\n",
    "\n",
    "        X_test_subset = X_test[selected_features_list]\n",
    "\n",
    "        y_pred =  MLP_NN.predict(X_test_subset)\n",
    "\n",
    "        # Calculate AUROC as the fitness score\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        return f1\n",
    "        \"\"\"\n",
    "        Example fitness function.\n",
    "        \"\"\"\n",
    "        # This is just a placeholder. You should implement your own fitness function logic here.\n",
    "        # For example, you could compute a score based on the number of ones in the chromosome.\n",
    "        #return np.sum(chromosome)\n",
    "\n",
    "    def initialize_population(self):\n",
    "        \"\"\"\n",
    "        Initializes the population of the problem.\n",
    "        \"\"\"\n",
    "        # Randomly initialize the population using numpy's random.randint method.\n",
    "        forward_parent_chromosome = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        backward_parent_chromosome = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "        bits_array = [forward_parent_chromosome, backward_parent_chromosome]\n",
    "\n",
    "        for _ in range(self.pop_size-2):\n",
    "            bits = np.zeros(self.n_genes)\n",
    "            bits[: np.random.randint(0, self.n_genes)] = 1\n",
    "            np.random.shuffle(bits)\n",
    "\n",
    "            bits_array.append(bits)\n",
    "\n",
    "        return np.array(bits_array)\n",
    "\n",
    "    def create_offspring(self, first_parent, sec_parent, crossover_pt, offspring_number):\n",
    "        \"\"\"\n",
    "        Creates an offspring from 2 parents with a random crossover point based on a probability.\n",
    "\n",
    "        :param first_parent: The chromosome of the first parent.\n",
    "        :param sec_parent: The chromosome of the second parent.\n",
    "        :param crossover_prob: The probability of crossover.\n",
    "        :return: The resulting offspring chromosome.\n",
    "        \"\"\"\n",
    "        n_genes = len(first_parent)\n",
    "        crossover_mask = np.random.rand(n_genes) < 0.5\n",
    "\n",
    "    # Select a random crossover point\n",
    "        crossover_pt = np.random.randint(1, n_genes)\n",
    "\n",
    "    # Perform crossover based on the crossover mask\n",
    "        offspring = np.where(crossover_mask, sec_parent, first_parent)\n",
    "\n",
    "        return offspring\n",
    "\n",
    "    def mutate_population(self, population, n_mutations):\n",
    "        \"\"\"\n",
    "        Mutates the population by flipping bits randomly.\n",
    "        \"\"\"\n",
    "        mutation_rows, mutation_cols = super(\n",
    "            BinaryGenAlgSolver, self\n",
    "        ).mutate_population(population, n_mutations)\n",
    "\n",
    "        population[mutation_rows, mutation_cols] = np.abs(population - 1)[\n",
    "            mutation_rows, mutation_cols\n",
    "        ]\n",
    "\n",
    "        return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f096efc2-ce52-479d-bee5-177055255ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Best fitness: 0.3949844159489386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\n",
      "Best fitness: 0.3949844159489386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\n",
      "Best fitness: 0.40602788775852416\n",
      "Iteration: 4\n",
      "Best fitness: 0.4070059879570936\n",
      "Iteration: 5\n",
      "Best fitness: 0.4144928827810541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6\n",
      "Best fitness: 0.4144928827810541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7\n",
      "Best fitness: 0.4144928827810541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8\n",
      "Best fitness: 0.4144928827810541\n",
      "Iteration: 9\n",
      "Best fitness: 0.41904184413646867\n",
      "Iteration: 10\n",
      "Best fitness: 0.41904184413646867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############################\n",
      "#\t\t\tSTATS\t\t\t#\n",
      "#############################\n",
      "\n",
      "\n",
      "Total running time: 1 hours, 23 minutes, 38 seconds\n",
      "\n",
      "\n",
      "Population size: 10\n",
      "Number variables: 59\n",
      "Selection rate: 0.3\n",
      "Mutation rate: 0.025\n",
      "Number Generations: 10\n",
      "\n",
      "Best fitness: 0.41904184413646867\n",
      "Best individual: [1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "solver = MyBinaryGenAlgSolver(pop_size=10, n_genes=59, mutation_rate=0.025, max_gen=10, selection_rate=0.3)\n",
    "best_solution = solver.solve()\n",
    "print(best_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39108085-2efc-4a8c-b157-1b7ddc6bc96a",
   "metadata": {},
   "source": [
    "# Deep Neural Network after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5ce407f-4c68-4441-be29-939acf346df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admission_type_EU OBSERVATION', 'race_encode_South American', 'height', 'charlson_score', 'wbc', 'glucose', 'bicarbonate', 'admission_age', 'weight_admit', 'ckd', 'ihd', 'first_careunit_Medical/Surgical Intensive Care Unit (MICU/SICU)', 'pt', 'race_encode_African', 'first_careunit_Coronary Care Unit (CCU)', 'mch']\n"
     ]
    }
   ],
   "source": [
    "Best_individual = [1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    " 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    " 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "\n",
    "\n",
    "selected_features_NN = [feature_ranking_total[i] for i, value in enumerate(Best_individual) if value == 1]\n",
    "print(selected_features_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a7b3b93-270c-4d06-93b6-f633c244a9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14283, 16)\n",
      "(4080, 16)\n",
      "(2041, 16)\n"
     ]
    }
   ],
   "source": [
    "# Reducing feature dimension of train and test data after genetic algorithm feature selection\n",
    "\n",
    "X_train_selected_NN = X_train[selected_features_NN]\n",
    "X_test_selected_NN = X_test[selected_features_NN]\n",
    "X_holdout_selected_NN = X_holdout[selected_features_NN]\n",
    "\n",
    "print(X_train_selected_NN.shape)\n",
    "print(X_test_selected_NN.shape)\n",
    "print(X_holdout_selected_NN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6daa2-1f0d-4b5e-a5eb-2b13bd9a8205",
   "metadata": {},
   "source": [
    "# Kfold cross validation + GridSearchCV for Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bdb1924-add0-4c2f-a04b-b658830db5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72e82b60-2994-45c0-aab1-08944ad16062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining X_train and X_test together for this GridSearch\n",
    "\n",
    "X_combined = pd.concat([X_train_selected_NN, X_test_selected_NN], axis=0)\n",
    "\n",
    "y_combined = pd.concat([y_train, y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f25c28d-0491-4f2e-9a88-a965d0a2c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(max_iter=500, random_state=26),\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(128, 32), (64, 16)],\n",
       "                         &#x27;solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;]},\n",
       "             scoring=make_scorer(f1_score, response_method=&#x27;predict&#x27;, average=macro))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(max_iter=500, random_state=26),\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(128, 32), (64, 16)],\n",
       "                         &#x27;solver&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;]},\n",
       "             scoring=make_scorer(f1_score, response_method=&#x27;predict&#x27;, average=macro))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=500, random_state=26)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(max_iter=500, random_state=26)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=MLPClassifier(max_iter=500, random_state=26),\n",
       "             param_grid={'activation': ['relu', 'logistic'],\n",
       "                         'hidden_layer_sizes': [(128, 32), (64, 16)],\n",
       "                         'solver': ['adam', 'sgd']},\n",
       "             scoring=make_scorer(f1_score, response_method='predict', average=macro))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stratifiedCV = StratifiedKFold(n_splits=5)\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "\n",
    "MLP_NN = MLPClassifier(max_iter=500, random_state=26)\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(128,32), (64,16)],\n",
    "    'activation': ['relu', 'logistic'],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "best_NN = GridSearchCV(\n",
    "    MLP_NN,\n",
    "    param_grid=params,\n",
    "    scoring=scorer,\n",
    "    cv=stratifiedCV)\n",
    "\n",
    "best_NN.fit(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ce962c4-67c6-4802-b716-e3df724733ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic', 'hidden_layer_sizes': (64, 16), 'solver': 'adam'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_NN.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a5cf535-f246-430f-aa79-a95b10bc17aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41447434164992414"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_NN.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1021c729-f5f9-4fe3-9f61-87372d54da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_NN_selected = MLPClassifier(hidden_layer_sizes=(64, 16), \n",
    "                                 activation='logistic', \n",
    "                                 solver='adam',\n",
    "                                 max_iter=500, \n",
    "                                 random_state=26)\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'accuracy', 'f1_macro']\n",
    "\n",
    "# scoring = {\n",
    "#     'precision_1': make_scorer(precision_score, pos_label=1),\n",
    "#     'recall_1': make_scorer(recall_score, pos_label=1),\n",
    "#     'f1_1': make_scorer(f1_score, pos_label=1),\n",
    "#     'accuracy': make_scorer(accuracy_score)\n",
    "# }\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validate(best_NN_selected, X_combined, y_combined, cv=StratifiedKFold(), scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e300c03f-aa91-4665-b5e8-c656dbd1e375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_precision_macro</th>\n",
       "      <th>train_precision_macro</th>\n",
       "      <th>test_recall_macro</th>\n",
       "      <th>train_recall_macro</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>train_f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.595628</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.432739</td>\n",
       "      <td>0.505726</td>\n",
       "      <td>0.441118</td>\n",
       "      <td>0.501597</td>\n",
       "      <td>0.475361</td>\n",
       "      <td>0.539142</td>\n",
       "      <td>0.414964</td>\n",
       "      <td>0.477906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.515707</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.429803</td>\n",
       "      <td>0.510897</td>\n",
       "      <td>0.442586</td>\n",
       "      <td>0.504551</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.539415</td>\n",
       "      <td>0.419484</td>\n",
       "      <td>0.487744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.781329</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.424791</td>\n",
       "      <td>0.505616</td>\n",
       "      <td>0.433465</td>\n",
       "      <td>0.497588</td>\n",
       "      <td>0.473727</td>\n",
       "      <td>0.536964</td>\n",
       "      <td>0.407527</td>\n",
       "      <td>0.474511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.512424</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.419368</td>\n",
       "      <td>0.508590</td>\n",
       "      <td>0.431941</td>\n",
       "      <td>0.507202</td>\n",
       "      <td>0.460512</td>\n",
       "      <td>0.538016</td>\n",
       "      <td>0.414730</td>\n",
       "      <td>0.492470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.304402</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.429961</td>\n",
       "      <td>0.519229</td>\n",
       "      <td>0.437034</td>\n",
       "      <td>0.506502</td>\n",
       "      <td>0.473312</td>\n",
       "      <td>0.542849</td>\n",
       "      <td>0.415667</td>\n",
       "      <td>0.489008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_precision_macro  train_precision_macro  \\\n",
       "0  33.595628    0.019419              0.432739               0.505726   \n",
       "1  32.515707    0.017892              0.429803               0.510897   \n",
       "2  32.781329    0.017822              0.424791               0.505616   \n",
       "3  32.512424    0.019671              0.419368               0.508590   \n",
       "4  32.304402    0.017964              0.429961               0.519229   \n",
       "\n",
       "   test_recall_macro  train_recall_macro  test_accuracy  train_accuracy  \\\n",
       "0           0.441118            0.501597       0.475361        0.539142   \n",
       "1           0.442586            0.504551       0.479445        0.539415   \n",
       "2           0.433465            0.497588       0.473727        0.536964   \n",
       "3           0.431941            0.507202       0.460512        0.538016   \n",
       "4           0.437034            0.506502       0.473312        0.542849   \n",
       "\n",
       "   test_f1_macro  train_f1_macro  \n",
       "0       0.414964        0.477906  \n",
       "1       0.419484        0.487744  \n",
       "2       0.407527        0.474511  \n",
       "3       0.414730        0.492470  \n",
       "4       0.415667        0.489008  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores_NN_df = pd.DataFrame(scores)\n",
    "cv_scores_NN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5a8cafe-35b2-4e3b-b814-35d6d3e492c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision_macro</th>\n",
       "      <th>test_precision_macro</th>\n",
       "      <th>train_recall_macro</th>\n",
       "      <th>test_recall_macro</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>accuracy_diff</th>\n",
       "      <th>precision_diff</th>\n",
       "      <th>recall_diff</th>\n",
       "      <th>f1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.595628</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.539142</td>\n",
       "      <td>0.475361</td>\n",
       "      <td>0.505726</td>\n",
       "      <td>0.432739</td>\n",
       "      <td>0.501597</td>\n",
       "      <td>0.441118</td>\n",
       "      <td>0.477906</td>\n",
       "      <td>0.414964</td>\n",
       "      <td>0.063782</td>\n",
       "      <td>0.072987</td>\n",
       "      <td>0.060479</td>\n",
       "      <td>0.062942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.515707</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.539415</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.510897</td>\n",
       "      <td>0.429803</td>\n",
       "      <td>0.504551</td>\n",
       "      <td>0.442586</td>\n",
       "      <td>0.487744</td>\n",
       "      <td>0.419484</td>\n",
       "      <td>0.059970</td>\n",
       "      <td>0.081094</td>\n",
       "      <td>0.061965</td>\n",
       "      <td>0.068260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.781329</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.536964</td>\n",
       "      <td>0.473727</td>\n",
       "      <td>0.505616</td>\n",
       "      <td>0.424791</td>\n",
       "      <td>0.497588</td>\n",
       "      <td>0.433465</td>\n",
       "      <td>0.474511</td>\n",
       "      <td>0.407527</td>\n",
       "      <td>0.063237</td>\n",
       "      <td>0.080825</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.066984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.512424</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.538016</td>\n",
       "      <td>0.460512</td>\n",
       "      <td>0.508590</td>\n",
       "      <td>0.419368</td>\n",
       "      <td>0.507202</td>\n",
       "      <td>0.431941</td>\n",
       "      <td>0.492470</td>\n",
       "      <td>0.414730</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.089223</td>\n",
       "      <td>0.075261</td>\n",
       "      <td>0.077741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.304402</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.542849</td>\n",
       "      <td>0.473312</td>\n",
       "      <td>0.519229</td>\n",
       "      <td>0.429961</td>\n",
       "      <td>0.506502</td>\n",
       "      <td>0.437034</td>\n",
       "      <td>0.489008</td>\n",
       "      <td>0.415667</td>\n",
       "      <td>0.069538</td>\n",
       "      <td>0.089268</td>\n",
       "      <td>0.069468</td>\n",
       "      <td>0.073341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  train_accuracy  test_accuracy  \\\n",
       "0  33.595628    0.019419        0.539142       0.475361   \n",
       "1  32.515707    0.017892        0.539415       0.479445   \n",
       "2  32.781329    0.017822        0.536964       0.473727   \n",
       "3  32.512424    0.019671        0.538016       0.460512   \n",
       "4  32.304402    0.017964        0.542849       0.473312   \n",
       "\n",
       "   train_precision_macro  test_precision_macro  train_recall_macro  \\\n",
       "0               0.505726              0.432739            0.501597   \n",
       "1               0.510897              0.429803            0.504551   \n",
       "2               0.505616              0.424791            0.497588   \n",
       "3               0.508590              0.419368            0.507202   \n",
       "4               0.519229              0.429961            0.506502   \n",
       "\n",
       "   test_recall_macro  train_f1_macro  test_f1_macro  accuracy_diff  \\\n",
       "0           0.441118        0.477906       0.414964       0.063782   \n",
       "1           0.442586        0.487744       0.419484       0.059970   \n",
       "2           0.433465        0.474511       0.407527       0.063237   \n",
       "3           0.431941        0.492470       0.414730       0.077504   \n",
       "4           0.437034        0.489008       0.415667       0.069538   \n",
       "\n",
       "   precision_diff  recall_diff   f1_diff  \n",
       "0        0.072987     0.060479  0.062942  \n",
       "1        0.081094     0.061965  0.068260  \n",
       "2        0.080825     0.064123  0.066984  \n",
       "3        0.089223     0.075261  0.077741  \n",
       "4        0.089268     0.069468  0.073341  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores_NN_df['precision_diff'] = cv_scores_NN_df['train_precision_macro'] - cv_scores_NN_df['test_precision_macro']\n",
    "cv_scores_NN_df['recall_diff'] = cv_scores_NN_df['train_recall_macro'] - cv_scores_NN_df['test_recall_macro']\n",
    "cv_scores_NN_df['f1_diff'] = cv_scores_NN_df['train_f1_macro'] - cv_scores_NN_df['test_f1_macro']\n",
    "cv_scores_NN_df['accuracy_diff'] = cv_scores_NN_df['train_accuracy'] - cv_scores_NN_df['test_accuracy']\n",
    "cv_order = ['fit_time', 'score_time', 'train_accuracy', 'test_accuracy', 'train_precision_macro', 'test_precision_macro', 'train_recall_macro', 'test_recall_macro', 'train_f1_macro', 'test_f1_macro', 'accuracy_diff', 'precision_diff', 'recall_diff', 'f1_diff']\n",
    "cv_scores_NN_df = cv_scores_NN_df[cv_order]\n",
    "cv_scores_NN_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "154be41e-dd55-4c25-ad81-ac64057625ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "holdout Accuracy:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.77      0.62       883\n",
      "           1       0.38      0.08      0.13       614\n",
      "           2       0.44      0.47      0.45       544\n",
      "\n",
      "    accuracy                           0.48      2041\n",
      "   macro avg       0.45      0.44      0.40      2041\n",
      "weighted avg       0.45      0.48      0.43      2041\n",
      "\n",
      "Holdout dataset icu_cat prediction Confusion Matrix:\n",
      "        Predicted_0  Predicted_1  Predicted_2\n",
      "True_0          683           50          150\n",
      "True_1          387           50          177\n",
      "True_2          259           30          255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngqin\\anaconda3\\envs\\python3.10\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_NN_selected.fit(X_combined, y_combined)\n",
    "\n",
    "y_holdout_cat_pred = best_NN_selected.predict(X_holdout_selected_NN)\n",
    "X_holdout_df['icu_cat_pred'] = best_NN_selected.predict(X_holdout_selected_NN)\n",
    "\n",
    "print(\"\\n\\nholdout Accuracy:\\n\", classification_report(y_holdout, y_holdout_cat_pred))\n",
    "\n",
    "conf_matrix_holdout = confusion_matrix(y_holdout, y_holdout_cat_pred)\n",
    "\n",
    "conf_matrix_holdout_df = pd.DataFrame(conf_matrix_holdout, columns=['Predicted_0', 'Predicted_1', 'Predicted_2'],\n",
    "                               index=['True_0', 'True_1', 'True_2'])\n",
    "\n",
    "\n",
    "print(\"Holdout dataset icu_cat prediction Confusion Matrix:\")\n",
    "print(conf_matrix_holdout_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9815ad11-0455-44ae-8944-7e0307ec2cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hosp_admittime</th>\n",
       "      <th>hosp_dischtime</th>\n",
       "      <th>icu_intime</th>\n",
       "      <th>icu_outtime</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>icu_death</th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>...</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "      <th>icu_cat</th>\n",
       "      <th>icu_cat_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20017191</td>\n",
       "      <td>2032-12-03 17:42:00</td>\n",
       "      <td>2032-12-08 15:46:00</td>\n",
       "      <td>2032-12-03 17:42:00</td>\n",
       "      <td>2032-12-06 18:02:00</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.924695</td>\n",
       "      <td>52.3</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>5.495000</td>\n",
       "      <td>14.450000</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>12.600</td>\n",
       "      <td>27.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20036035</td>\n",
       "      <td>2022-10-05 19:51:00</td>\n",
       "      <td>2022-10-09 18:00:00</td>\n",
       "      <td>2022-10-05 21:56:00</td>\n",
       "      <td>2022-10-09 19:49:00</td>\n",
       "      <td>3.91</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84.760666</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>217.500000</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>13.450000</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>36.850000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20037205</td>\n",
       "      <td>2022-03-05 19:44:00</td>\n",
       "      <td>2022-03-13 14:28:00</td>\n",
       "      <td>2022-03-05 19:45:00</td>\n",
       "      <td>2022-03-08 22:18:00</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.174740</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>4.355000</td>\n",
       "      <td>13.150000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>14.900</td>\n",
       "      <td>27.550000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20039772</td>\n",
       "      <td>2065-01-19 00:40:00</td>\n",
       "      <td>2065-02-04 16:57:00</td>\n",
       "      <td>2065-01-19 04:35:00</td>\n",
       "      <td>2065-01-22 19:03:00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58.051521</td>\n",
       "      <td>76.8</td>\n",
       "      <td>...</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>255.750000</td>\n",
       "      <td>2.895000</td>\n",
       "      <td>16.175000</td>\n",
       "      <td>8.775000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>22.775</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20042202</td>\n",
       "      <td>2048-03-27 01:42:00</td>\n",
       "      <td>2048-04-10 13:21:00</td>\n",
       "      <td>2048-03-27 03:48:00</td>\n",
       "      <td>2048-03-31 22:28:00</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63.235742</td>\n",
       "      <td>79.6</td>\n",
       "      <td>...</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>162.750000</td>\n",
       "      <td>3.437500</td>\n",
       "      <td>15.525000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>14.550</td>\n",
       "      <td>33.950000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20047616</td>\n",
       "      <td>1987-09-28 16:55:00</td>\n",
       "      <td>1987-10-08 16:30:00</td>\n",
       "      <td>1987-09-28 18:05:00</td>\n",
       "      <td>1987-09-30 20:50:00</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73.741166</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>14.066667</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>13.900</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20048682</td>\n",
       "      <td>2051-04-05 06:16:00</td>\n",
       "      <td>2051-04-09 12:05:00</td>\n",
       "      <td>2051-04-05 08:22:00</td>\n",
       "      <td>2051-04-06 12:20:00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.256753</td>\n",
       "      <td>58.9</td>\n",
       "      <td>...</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20065759</td>\n",
       "      <td>2001-11-02 09:57:00</td>\n",
       "      <td>2001-11-10 01:45:00</td>\n",
       "      <td>2001-11-02 11:00:00</td>\n",
       "      <td>2001-11-10 05:20:00</td>\n",
       "      <td>7.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88.833548</td>\n",
       "      <td>72.3</td>\n",
       "      <td>...</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>3.870000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>7.525000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>16.475</td>\n",
       "      <td>32.375000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20075994</td>\n",
       "      <td>2062-10-27 22:15:00</td>\n",
       "      <td>2062-10-30 14:40:00</td>\n",
       "      <td>2062-10-27 23:07:00</td>\n",
       "      <td>2062-10-30 16:51:00</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73.821174</td>\n",
       "      <td>81.2</td>\n",
       "      <td>...</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>358.666667</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>16.266667</td>\n",
       "      <td>29.566667</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20076204</td>\n",
       "      <td>2050-03-31 16:06:00</td>\n",
       "      <td>2050-04-06 18:18:00</td>\n",
       "      <td>2050-03-31 18:25:00</td>\n",
       "      <td>2050-04-03 10:21:00</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.246261</td>\n",
       "      <td>72.2</td>\n",
       "      <td>...</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>16.775000</td>\n",
       "      <td>8.825000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>15.100</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       hosp_admittime       hosp_dischtime           icu_intime  \\\n",
       "0  20017191  2032-12-03 17:42:00  2032-12-08 15:46:00  2032-12-03 17:42:00   \n",
       "1  20036035  2022-10-05 19:51:00  2022-10-09 18:00:00  2022-10-05 21:56:00   \n",
       "2  20037205  2022-03-05 19:44:00  2022-03-13 14:28:00  2022-03-05 19:45:00   \n",
       "3  20039772  2065-01-19 00:40:00  2065-02-04 16:57:00  2065-01-19 04:35:00   \n",
       "4  20042202  2048-03-27 01:42:00  2048-04-10 13:21:00  2048-03-27 03:48:00   \n",
       "5  20047616  1987-09-28 16:55:00  1987-10-08 16:30:00  1987-09-28 18:05:00   \n",
       "6  20048682  2051-04-05 06:16:00  2051-04-09 12:05:00  2051-04-05 08:22:00   \n",
       "7  20065759  2001-11-02 09:57:00  2001-11-10 01:45:00  2001-11-02 11:00:00   \n",
       "8  20075994  2062-10-27 22:15:00  2062-10-30 14:40:00  2062-10-27 23:07:00   \n",
       "9  20076204  2050-03-31 16:06:00  2050-04-06 18:18:00  2050-03-31 18:25:00   \n",
       "\n",
       "           icu_outtime  los_icu  icu_death  gender  admission_age  \\\n",
       "0  2032-12-06 18:02:00     3.01          0       0      84.924695   \n",
       "1  2022-10-09 19:49:00     3.91          1       0      84.760666   \n",
       "2  2022-03-08 22:18:00     3.11          0       0      47.174740   \n",
       "3  2065-01-22 19:03:00     3.60          0       1      58.051521   \n",
       "4  2048-03-31 22:28:00     4.78          0       1      63.235742   \n",
       "5  1987-09-30 20:50:00     2.11          0       0      73.741166   \n",
       "6  2051-04-06 12:20:00     1.17          0       0      67.256753   \n",
       "7  2001-11-10 05:20:00     7.76          1       1      88.833548   \n",
       "8  2062-10-30 16:51:00     2.74          1       1      73.821174   \n",
       "9  2050-04-03 10:21:00     2.66          0       0      78.246261   \n",
       "\n",
       "   weight_admit  ...        mcv    platelet       rbc        rdw        wbc  \\\n",
       "0          52.3  ...  92.000000  106.000000  5.495000  14.450000  25.100000   \n",
       "1          60.0  ...  91.000000  217.500000  4.560000  13.450000  18.300000   \n",
       "2          99.0  ...  87.500000  412.000000  4.355000  13.150000  18.400000   \n",
       "3          76.8  ...  88.250000  255.750000  2.895000  16.175000   8.775000   \n",
       "4          79.6  ...  91.750000  162.750000  3.437500  15.525000   9.200000   \n",
       "5          70.0  ...  87.666667  278.000000  4.133333  14.066667  11.700000   \n",
       "6          58.9  ...  87.500000  359.000000  3.475000  17.500000  17.650000   \n",
       "7          72.3  ...  91.750000  127.000000  3.870000  17.350000   7.525000   \n",
       "8          81.2  ...  79.000000  358.666667  4.250000  16.266667  29.566667   \n",
       "9          72.2  ...  93.250000  143.500000  2.937500  16.775000   8.825000   \n",
       "\n",
       "        inr      pt        ptt  icu_cat  icu_cat_pred  \n",
       "0  1.150000  12.600  27.950000        1             0  \n",
       "1  1.350000  15.000  36.850000        1             2  \n",
       "2  1.400000  14.900  27.550000        1             2  \n",
       "3  2.100000  22.775  62.625000        1             2  \n",
       "4  1.350000  14.550  33.950000        1             2  \n",
       "5  1.233333  13.900  26.666667        0             1  \n",
       "6  1.400000  15.000  33.500000        0             0  \n",
       "7  1.550000  16.475  32.375000        2             2  \n",
       "8  1.400000  15.000  33.500000        1             0  \n",
       "9  1.400000  15.100  33.300000        1             0  \n",
       "\n",
       "[10 rows x 69 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb6cbc-ccc5-4042-93fb-898ebaff1ab9",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f41ff097-090f-49af-9299-74ef0a81fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6243, 68)\n",
      "(4197, 68)\n",
      "(3843, 68)\n",
      "(1793, 68)\n",
      "(1148, 68)\n",
      "(1139, 68)\n"
     ]
    }
   ],
   "source": [
    "Cat0_X_train = X_train_df[X_train_df['icu_cat'] == 0]\n",
    "Cat1_X_train = X_train_df[X_train_df['icu_cat'] == 1]\n",
    "Cat2_X_train = X_train_df[X_train_df['icu_cat'] == 2]\n",
    "\n",
    "Cat0_X_test = X_test_df[X_test_df['icu_cat'] == 0]\n",
    "Cat1_X_test = X_test_df[X_test_df['icu_cat'] == 1]\n",
    "Cat2_X_test = X_test_df[X_test_df['icu_cat'] == 2]\n",
    "\n",
    "print(Cat0_X_train.shape)\n",
    "print(Cat1_X_train.shape)\n",
    "print(Cat2_X_train.shape)\n",
    "print(Cat0_X_test.shape)\n",
    "print(Cat1_X_test.shape)\n",
    "print(Cat2_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8769112-0fc3-42d5-a57f-2fbf1a85ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6243, 59)\n",
      "(1793, 59)\n",
      "(4197, 59)\n",
      "(1148, 59)\n",
      "(6243,)\n",
      "(1793,)\n",
      "(4197,)\n",
      "(1148,)\n"
     ]
    }
   ],
   "source": [
    "# splitting into X and Y\n",
    "\n",
    "X_features_to_drop = ['id', 'los_icu', 'icu_death', 'hosp_admittime', 'hosp_dischtime', 'icu_intime', 'icu_outtime', 'icu_cat', 'icu_outcome']\n",
    "\n",
    "y_train_cat0 = Cat0_X_train.los_icu\n",
    "X_train_cat0 = Cat0_X_train.drop(columns=X_features_to_drop)\n",
    "\n",
    "y_test_cat0 = Cat0_X_test.icu_cat\n",
    "X_test_cat0 = Cat0_X_test.drop(columns=X_features_to_drop)\n",
    "\n",
    "y_train_cat1 = Cat1_X_train.los_icu\n",
    "X_train_cat1 = Cat1_X_train.drop(columns=X_features_to_drop)\n",
    "\n",
    "y_test_cat1 = Cat1_X_test.los_icu\n",
    "X_test_cat1 = Cat1_X_test.drop(columns=X_features_to_drop)\n",
    "\n",
    "print(X_train_cat0.shape)\n",
    "print(X_test_cat0.shape)\n",
    "print(X_train_cat1.shape)\n",
    "print(X_test_cat1.shape)\n",
    "print(y_train_cat0.shape)\n",
    "print(y_test_cat0.shape)\n",
    "print(y_train_cat1.shape)\n",
    "print(y_test_cat1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbe1c5c7-d79f-4975-9593-8046a67eaffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1329, 69)\n",
      "(130, 69)\n"
     ]
    }
   ],
   "source": [
    "# predicting MSE of cat0 icu_cat_pred of holdout data from first NN model\n",
    "\n",
    "holdout_pred_cat0_df = X_holdout_df[X_holdout_df['icu_cat_pred'] == 0]\n",
    "holdout_pred_cat1_df = X_holdout_df[X_holdout_df['icu_cat_pred'] == 1]\n",
    "\n",
    "print(holdout_pred_cat0_df.shape)\n",
    "print(holdout_pred_cat1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cfbe0f3-9536-47f9-a475-042f6439dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1329, 59)\n",
      "(130, 59)\n",
      "(1329,)\n",
      "(130,)\n"
     ]
    }
   ],
   "source": [
    "X_holdout_features_to_drop = ['id', 'los_icu', 'icu_death', 'hosp_admittime', 'hosp_dischtime', 'icu_intime', 'icu_outtime', 'icu_cat', 'icu_outcome', 'icu_cat_pred']\n",
    "\n",
    "X_holdout_pred_cat0 = holdout_pred_cat0_df.drop(columns=X_holdout_features_to_drop)\n",
    "y_holdout_reg_cat0 = holdout_pred_cat0_df.los_icu\n",
    "\n",
    "X_holdout_pred_cat1 = holdout_pred_cat1_df.drop(columns=X_holdout_features_to_drop)\n",
    "y_holdout_reg_cat1 = holdout_pred_cat1_df.los_icu\n",
    "\n",
    "print(X_holdout_pred_cat0.shape)\n",
    "print(X_holdout_pred_cat1.shape)\n",
    "print(y_holdout_reg_cat0.shape)\n",
    "print(y_holdout_reg_cat1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa3cd3fe-2827-4c21-a65c-b5fef83456f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization using data from X_train to transform X_test for CAT0\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = X_train_cat0.columns[X_train_cat0.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "# print(num_cols)\n",
    "scaler_cat0 = StandardScaler()\n",
    "X_train_cat0[num_cols] = scaler_cat0.fit_transform(X_train_cat0[num_cols])\n",
    "X_test_cat0[num_cols] = scaler_cat0.transform(X_test_cat0[num_cols])\n",
    "X_holdout_pred_cat0[num_cols] = scaler_cat0.transform(X_holdout_pred_cat0[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "797dc9c5-e2ad-42d5-b28e-c1643f2ee9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>-0.008097</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>-0.058628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>-0.013268</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>-0.014115</td>\n",
       "      <td>-0.017866</td>\n",
       "      <td>0.014924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999631</td>\n",
       "      <td>1.008558</td>\n",
       "      <td>0.789125</td>\n",
       "      <td>0.907918</td>\n",
       "      <td>1.020459</td>\n",
       "      <td>1.018478</td>\n",
       "      <td>0.991817</td>\n",
       "      <td>1.018034</td>\n",
       "      <td>1.004870</td>\n",
       "      <td>0.924653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>1.030059</td>\n",
       "      <td>0.982395</td>\n",
       "      <td>1.019990</td>\n",
       "      <td>0.971712</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>1.084689</td>\n",
       "      <td>1.124364</td>\n",
       "      <td>1.145289</td>\n",
       "      <td>1.058137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-3.117846</td>\n",
       "      <td>-2.746396</td>\n",
       "      <td>-5.550409</td>\n",
       "      <td>-1.946083</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.357416</td>\n",
       "      <td>-4.217825</td>\n",
       "      <td>-3.741144</td>\n",
       "      <td>-1.985078</td>\n",
       "      <td>-2.987631</td>\n",
       "      <td>-1.947009</td>\n",
       "      <td>-1.546576</td>\n",
       "      <td>-1.117834</td>\n",
       "      <td>-1.181794</td>\n",
       "      <td>-1.128259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-0.613069</td>\n",
       "      <td>-0.549268</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>-0.591937</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446956</td>\n",
       "      <td>-0.581999</td>\n",
       "      <td>-0.535620</td>\n",
       "      <td>-0.544374</td>\n",
       "      <td>-0.638809</td>\n",
       "      <td>-0.611290</td>\n",
       "      <td>-0.434428</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.421161</td>\n",
       "      <td>-0.428530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.100986</td>\n",
       "      <td>-0.114420</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.085136</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070916</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>-0.054792</td>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.101609</td>\n",
       "      <td>-0.138395</td>\n",
       "      <td>-0.163284</td>\n",
       "      <td>-0.252415</td>\n",
       "      <td>-0.249589</td>\n",
       "      <td>-0.215454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.766109</td>\n",
       "      <td>0.362932</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.762208</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546513</td>\n",
       "      <td>0.584321</td>\n",
       "      <td>0.506175</td>\n",
       "      <td>0.337108</td>\n",
       "      <td>0.561318</td>\n",
       "      <td>0.458945</td>\n",
       "      <td>0.204368</td>\n",
       "      <td>-0.138390</td>\n",
       "      <td>-0.129489</td>\n",
       "      <td>-0.065512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>1.940691</td>\n",
       "      <td>4.544667</td>\n",
       "      <td>4.126044</td>\n",
       "      <td>4.486108</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>2.496811</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>2.784953</td>\n",
       "      <td>...</td>\n",
       "      <td>4.478110</td>\n",
       "      <td>6.249306</td>\n",
       "      <td>4.539793</td>\n",
       "      <td>10.149745</td>\n",
       "      <td>3.878815</td>\n",
       "      <td>5.262558</td>\n",
       "      <td>27.406054</td>\n",
       "      <td>20.342228</td>\n",
       "      <td>22.946859</td>\n",
       "      <td>8.978364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender  admission_age  weight_admit       height  charlson_score  \\\n",
       "count  1329.000000    1329.000000   1329.000000  1329.000000     1329.000000   \n",
       "mean      0.005951       0.005059     -0.015768     0.001135        0.014066   \n",
       "std       0.999631       1.008558      0.789125     0.907918        1.020459   \n",
       "min      -1.129591      -3.117846     -2.746396    -5.550409       -1.946083   \n",
       "25%      -1.129591      -0.613069     -0.549268     0.020882       -0.591937   \n",
       "50%       0.885276       0.100986     -0.114420     0.020882        0.085136   \n",
       "75%       0.885276       0.766109      0.362932     0.020882        0.762208   \n",
       "max       0.885276       1.940691      4.544667     4.126044        4.486108   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer          chf          ckd  \\\n",
       "count          1329.000000       1329.000000  1329.000000  1329.000000   \n",
       "mean              0.033572         -0.008097     0.053214     0.038860   \n",
       "std               1.018478          0.991817     1.018034     1.004870   \n",
       "min              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "25%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "50%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "75%               1.707075         -0.400511     1.424469     1.144402   \n",
       "max               1.707075          2.496811     1.424469     1.144402   \n",
       "\n",
       "               cld  ...          mch         mchc          mcv     platelet  \\\n",
       "count  1329.000000  ...  1329.000000  1329.000000  1329.000000  1329.000000   \n",
       "mean     -0.058628  ...     0.026565     0.001754     0.031395     0.001223   \n",
       "std       0.924653  ...     0.989780     1.030059     0.982395     1.019990   \n",
       "min      -0.359073  ...    -4.357416    -4.217825    -3.741144    -1.985078   \n",
       "25%      -0.359073  ...    -0.446956    -0.581999    -0.535620    -0.544374   \n",
       "50%      -0.359073  ...     0.070916     0.013062    -0.054792    -0.141115   \n",
       "75%      -0.359073  ...     0.546513     0.584321     0.506175     0.337108   \n",
       "max       2.784953  ...     4.478110     6.249306     4.539793    10.149745   \n",
       "\n",
       "               rbc          rdw          wbc          inr           pt  \\\n",
       "count  1329.000000  1329.000000  1329.000000  1329.000000  1329.000000   \n",
       "mean     -0.013268     0.011843     0.003802    -0.014115    -0.017866   \n",
       "std       0.971712     0.961903     1.084689     1.124364     1.145289   \n",
       "min      -2.987631    -1.947009    -1.546576    -1.117834    -1.181794   \n",
       "25%      -0.638809    -0.611290    -0.434428    -0.445380    -0.421161   \n",
       "50%      -0.101609    -0.138395    -0.163284    -0.252415    -0.249589   \n",
       "75%       0.561318     0.458945     0.204368    -0.138390    -0.129489   \n",
       "max       3.878815     5.262558    27.406054    20.342228    22.946859   \n",
       "\n",
       "               ptt  \n",
       "count  1329.000000  \n",
       "mean      0.014924  \n",
       "std       1.058137  \n",
       "min      -1.128259  \n",
       "25%      -0.428530  \n",
       "50%      -0.215454  \n",
       "75%      -0.065512  \n",
       "max       8.978364  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout_pred_cat0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae7ec37-4775-4f3c-88eb-b38e2f0532a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "      <td>1793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.041810</td>\n",
       "      <td>-0.018786</td>\n",
       "      <td>-0.032277</td>\n",
       "      <td>-0.018940</td>\n",
       "      <td>-0.055528</td>\n",
       "      <td>-0.010341</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>-0.044976</td>\n",
       "      <td>-0.012726</td>\n",
       "      <td>-0.034675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>-0.004929</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.030227</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>0.051787</td>\n",
       "      <td>0.045021</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>0.039264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004505</td>\n",
       "      <td>1.011321</td>\n",
       "      <td>0.866304</td>\n",
       "      <td>1.046400</td>\n",
       "      <td>0.978059</td>\n",
       "      <td>0.994409</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.982865</td>\n",
       "      <td>0.998474</td>\n",
       "      <td>0.956656</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027507</td>\n",
       "      <td>1.000718</td>\n",
       "      <td>1.024920</td>\n",
       "      <td>1.001410</td>\n",
       "      <td>1.031613</td>\n",
       "      <td>1.053923</td>\n",
       "      <td>1.531883</td>\n",
       "      <td>1.244556</td>\n",
       "      <td>1.214319</td>\n",
       "      <td>1.133619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-3.075017</td>\n",
       "      <td>-2.452137</td>\n",
       "      <td>-5.843635</td>\n",
       "      <td>-1.946083</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.794774</td>\n",
       "      <td>-4.628417</td>\n",
       "      <td>-5.023354</td>\n",
       "      <td>-1.981631</td>\n",
       "      <td>-3.033350</td>\n",
       "      <td>-1.930416</td>\n",
       "      <td>-1.546576</td>\n",
       "      <td>-1.234783</td>\n",
       "      <td>-1.291273</td>\n",
       "      <td>-1.241374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-0.668274</td>\n",
       "      <td>-0.608120</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>-0.591937</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446956</td>\n",
       "      <td>-0.522493</td>\n",
       "      <td>-0.535620</td>\n",
       "      <td>-0.554714</td>\n",
       "      <td>-0.630237</td>\n",
       "      <td>-0.669364</td>\n",
       "      <td>-0.429832</td>\n",
       "      <td>-0.401524</td>\n",
       "      <td>-0.398285</td>\n",
       "      <td>-0.407485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.077299</td>\n",
       "      <td>-0.137307</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>-0.253401</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081485</td>\n",
       "      <td>0.084470</td>\n",
       "      <td>-0.054792</td>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.067320</td>\n",
       "      <td>-0.198129</td>\n",
       "      <td>-0.140306</td>\n",
       "      <td>-0.217330</td>\n",
       "      <td>-0.215275</td>\n",
       "      <td>-0.215454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.747838</td>\n",
       "      <td>0.392358</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.762208</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525375</td>\n",
       "      <td>0.637877</td>\n",
       "      <td>0.506175</td>\n",
       "      <td>0.386223</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.409167</td>\n",
       "      <td>0.282494</td>\n",
       "      <td>-0.094534</td>\n",
       "      <td>-0.095175</td>\n",
       "      <td>-0.051701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>2.062524</td>\n",
       "      <td>13.568583</td>\n",
       "      <td>5.592174</td>\n",
       "      <td>3.470499</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>2.496811</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>2.784953</td>\n",
       "      <td>...</td>\n",
       "      <td>5.436349</td>\n",
       "      <td>4.083283</td>\n",
       "      <td>5.394599</td>\n",
       "      <td>7.624206</td>\n",
       "      <td>4.847489</td>\n",
       "      <td>9.120382</td>\n",
       "      <td>51.753837</td>\n",
       "      <td>16.482925</td>\n",
       "      <td>17.139169</td>\n",
       "      <td>8.978364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender  admission_age  weight_admit       height  charlson_score  \\\n",
       "count  1793.000000    1793.000000   1793.000000  1793.000000     1793.000000   \n",
       "mean     -0.041810      -0.018786     -0.032277    -0.018940       -0.055528   \n",
       "std       1.004505       1.011321      0.866304     1.046400        0.978059   \n",
       "min      -1.129591      -3.075017     -2.452137    -5.843635       -1.946083   \n",
       "25%      -1.129591      -0.668274     -0.608120     0.020882       -0.591937   \n",
       "50%       0.885276       0.077299     -0.137307     0.020882       -0.253401   \n",
       "75%       0.885276       0.747838      0.392358     0.020882        0.762208   \n",
       "max       0.885276       2.062524     13.568583     5.592174        3.470499   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer          chf          ckd  \\\n",
       "count          1793.000000       1793.000000  1793.000000  1793.000000   \n",
       "mean             -0.010341         -0.002998    -0.044976    -0.012726   \n",
       "std               0.994409          0.997127     0.982865     0.998474   \n",
       "min              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "25%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "50%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "75%               1.707075         -0.400511     1.424469     1.144402   \n",
       "max               1.707075          2.496811     1.424469     1.144402   \n",
       "\n",
       "               cld  ...          mch         mchc          mcv     platelet  \\\n",
       "count  1793.000000  ...  1793.000000  1793.000000  1793.000000  1793.000000   \n",
       "mean     -0.034675  ...     0.016154     0.037152    -0.004929     0.005217   \n",
       "std       0.956656  ...     1.027507     1.000718     1.024920     1.001410   \n",
       "min      -0.359073  ...    -5.794774    -4.628417    -5.023354    -1.981631   \n",
       "25%      -0.359073  ...    -0.446956    -0.522493    -0.535620    -0.554714   \n",
       "50%      -0.359073  ...     0.081485     0.084470    -0.054792    -0.141115   \n",
       "75%      -0.359073  ...     0.525375     0.637877     0.506175     0.386223   \n",
       "max       2.784953  ...     5.436349     4.083283     5.394599     7.624206   \n",
       "\n",
       "               rbc          rdw          wbc          inr           pt  \\\n",
       "count  1793.000000  1793.000000  1793.000000  1793.000000  1793.000000   \n",
       "mean      0.030227    -0.017661     0.051787     0.045021     0.038148   \n",
       "std       1.031613     1.053923     1.531883     1.244556     1.214319   \n",
       "min      -3.033350    -1.930416    -1.546576    -1.234783    -1.291273   \n",
       "25%      -0.630237    -0.669364    -0.429832    -0.401524    -0.398285   \n",
       "50%      -0.067320    -0.198129    -0.140306    -0.217330    -0.215275   \n",
       "75%       0.608466     0.409167     0.282494    -0.094534    -0.095175   \n",
       "max       4.847489     9.120382    51.753837    16.482925    17.139169   \n",
       "\n",
       "               ptt  \n",
       "count  1793.000000  \n",
       "mean      0.039264  \n",
       "std       1.133619  \n",
       "min      -1.241374  \n",
       "25%      -0.407485  \n",
       "50%      -0.215454  \n",
       "75%      -0.051701  \n",
       "max       8.978364  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cat0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5756ad4-c11f-4c84-89fd-665172b766f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization using data from X_train to transform X_test for CAT1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = X_train_cat1.columns[X_train_cat1.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "# print(num_cols)\n",
    "scaler_cat1 = StandardScaler()\n",
    "X_train_cat1[num_cols] = scaler_cat1.fit_transform(X_train_cat1[num_cols])\n",
    "X_test_cat1[num_cols] = scaler_cat1.transform(X_test_cat1[num_cols])\n",
    "X_holdout_pred_cat1[num_cols] = scaler_cat1.transform(X_holdout_pred_cat1[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "508bf8c0-b140-4cc6-8682-32df0acd2429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>130.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.069873</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.044927</td>\n",
       "      <td>0.052978</td>\n",
       "      <td>0.081674</td>\n",
       "      <td>0.077355</td>\n",
       "      <td>0.047297</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>-0.013612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072489</td>\n",
       "      <td>0.048902</td>\n",
       "      <td>-0.115907</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>-0.059183</td>\n",
       "      <td>0.103469</td>\n",
       "      <td>0.171287</td>\n",
       "      <td>0.089012</td>\n",
       "      <td>0.083240</td>\n",
       "      <td>0.130086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.009562</td>\n",
       "      <td>0.979461</td>\n",
       "      <td>1.074498</td>\n",
       "      <td>0.870416</td>\n",
       "      <td>1.026074</td>\n",
       "      <td>1.043630</td>\n",
       "      <td>1.080813</td>\n",
       "      <td>1.017122</td>\n",
       "      <td>1.005719</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924348</td>\n",
       "      <td>1.008294</td>\n",
       "      <td>0.854671</td>\n",
       "      <td>1.004993</td>\n",
       "      <td>0.937082</td>\n",
       "      <td>0.975035</td>\n",
       "      <td>0.719317</td>\n",
       "      <td>0.907385</td>\n",
       "      <td>0.859405</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.123083</td>\n",
       "      <td>-2.972423</td>\n",
       "      <td>-1.653201</td>\n",
       "      <td>-3.165219</td>\n",
       "      <td>-2.020151</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.159632</td>\n",
       "      <td>-2.817946</td>\n",
       "      <td>-3.207948</td>\n",
       "      <td>-1.984789</td>\n",
       "      <td>-2.162912</td>\n",
       "      <td>-1.944896</td>\n",
       "      <td>-1.007845</td>\n",
       "      <td>-0.844726</td>\n",
       "      <td>-0.882273</td>\n",
       "      <td>-1.166072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.123083</td>\n",
       "      <td>-0.545236</td>\n",
       "      <td>-0.622337</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>-0.659006</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475396</td>\n",
       "      <td>-0.449728</td>\n",
       "      <td>-0.606827</td>\n",
       "      <td>-0.542358</td>\n",
       "      <td>-0.690500</td>\n",
       "      <td>-0.499461</td>\n",
       "      <td>-0.279837</td>\n",
       "      <td>-0.367909</td>\n",
       "      <td>-0.337690</td>\n",
       "      <td>-0.362090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.166087</td>\n",
       "      <td>-0.210924</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.035178</td>\n",
       "      <td>-0.064500</td>\n",
       "      <td>-0.013814</td>\n",
       "      <td>-0.161687</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-0.035969</td>\n",
       "      <td>-0.208970</td>\n",
       "      <td>-0.227463</td>\n",
       "      <td>-0.246499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.806322</td>\n",
       "      <td>0.201421</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>1.669702</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>1.349986</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241575</td>\n",
       "      <td>0.532597</td>\n",
       "      <td>0.303873</td>\n",
       "      <td>0.489995</td>\n",
       "      <td>0.460489</td>\n",
       "      <td>0.707389</td>\n",
       "      <td>0.429722</td>\n",
       "      <td>0.029439</td>\n",
       "      <td>0.041433</td>\n",
       "      <td>0.372474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>1.684961</td>\n",
       "      <td>6.619831</td>\n",
       "      <td>2.344077</td>\n",
       "      <td>2.403572</td>\n",
       "      <td>1.669702</td>\n",
       "      <td>2.530240</td>\n",
       "      <td>1.349986</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>2.815988</td>\n",
       "      <td>...</td>\n",
       "      <td>2.390286</td>\n",
       "      <td>2.678071</td>\n",
       "      <td>2.282171</td>\n",
       "      <td>5.329949</td>\n",
       "      <td>3.267762</td>\n",
       "      <td>4.138270</td>\n",
       "      <td>2.777073</td>\n",
       "      <td>4.241321</td>\n",
       "      <td>3.999698</td>\n",
       "      <td>5.402304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gender  admission_age  weight_admit      height  charlson_score  \\\n",
       "count  130.000000     130.000000    130.000000  130.000000      130.000000   \n",
       "mean    -0.069873       0.058202     -0.030744   -0.044927        0.052978   \n",
       "std      1.009562       0.979461      1.074498    0.870416        1.026074   \n",
       "min     -1.123083      -2.972423     -1.653201   -3.165219       -2.020151   \n",
       "25%     -1.123083      -0.545236     -0.622337    0.037860       -0.659006   \n",
       "50%      0.890406       0.166087     -0.210924    0.037860        0.021567   \n",
       "75%      0.890406       0.806322      0.201421    0.037860        0.702140   \n",
       "max      0.890406       1.684961      6.619831    2.344077        2.403572   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer         chf         ckd  \\\n",
       "count           130.000000        130.000000  130.000000  130.000000   \n",
       "mean              0.081674          0.077355    0.047297    0.014058   \n",
       "std               1.043630          1.080813    1.017122    1.005719   \n",
       "min              -0.598909         -0.395219   -0.740749   -0.871212   \n",
       "25%              -0.598909         -0.395219   -0.740749   -0.871212   \n",
       "50%              -0.598909         -0.395219   -0.740749   -0.871212   \n",
       "75%               1.669702         -0.395219    1.349986    1.147826   \n",
       "max               1.669702          2.530240    1.349986    1.147826   \n",
       "\n",
       "              cld  ...         mch        mchc         mcv    platelet  \\\n",
       "count  130.000000  ...  130.000000  130.000000  130.000000  130.000000   \n",
       "mean    -0.013612  ...   -0.072489    0.048902   -0.115907    0.054766   \n",
       "std      0.986817  ...    0.924348    1.008294    0.854671    1.004993   \n",
       "min     -0.355115  ...   -3.159632   -2.817946   -3.207948   -1.984789   \n",
       "25%     -0.355115  ...   -0.475396   -0.449728   -0.606827   -0.542358   \n",
       "50%     -0.355115  ...    0.004666    0.035178   -0.064500   -0.013814   \n",
       "75%     -0.355115  ...    0.241575    0.532597    0.303873    0.489995   \n",
       "max      2.815988  ...    2.390286    2.678071    2.282171    5.329949   \n",
       "\n",
       "              rbc         rdw         wbc         inr          pt         ptt  \n",
       "count  130.000000  130.000000  130.000000  130.000000  130.000000  130.000000  \n",
       "mean    -0.059183    0.103469    0.171287    0.089012    0.083240    0.130086  \n",
       "std      0.937082    0.975035    0.719317    0.907385    0.859405    1.005400  \n",
       "min     -2.162912   -1.944896   -1.007845   -0.844726   -0.882273   -1.166072  \n",
       "25%     -0.690500   -0.499461   -0.279837   -0.367909   -0.337690   -0.362090  \n",
       "50%     -0.161687   -0.044678   -0.035969   -0.208970   -0.227463   -0.246499  \n",
       "75%      0.460489    0.707389    0.429722    0.029439    0.041433    0.372474  \n",
       "max      3.267762    4.138270    2.777073    4.241321    3.999698    5.402304  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout_pred_cat1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5773dc8-ae02-4250-8c5b-bcaa235ae038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.025135</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>-0.014159</td>\n",
       "      <td>-0.051536</td>\n",
       "      <td>-0.037124</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>-0.041004</td>\n",
       "      <td>-0.003164</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.017793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>0.050695</td>\n",
       "      <td>-0.008711</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>-0.019969</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.030379</td>\n",
       "      <td>0.039213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.003042</td>\n",
       "      <td>0.969885</td>\n",
       "      <td>0.933715</td>\n",
       "      <td>0.968411</td>\n",
       "      <td>0.959947</td>\n",
       "      <td>1.002470</td>\n",
       "      <td>0.954761</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>1.005965</td>\n",
       "      <td>1.021949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019373</td>\n",
       "      <td>1.063763</td>\n",
       "      <td>1.009521</td>\n",
       "      <td>0.955042</td>\n",
       "      <td>0.996068</td>\n",
       "      <td>0.993008</td>\n",
       "      <td>0.682750</td>\n",
       "      <td>1.032955</td>\n",
       "      <td>0.978090</td>\n",
       "      <td>1.013554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.123083</td>\n",
       "      <td>-3.150412</td>\n",
       "      <td>-3.108538</td>\n",
       "      <td>-5.471435</td>\n",
       "      <td>-2.020151</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.069597</td>\n",
       "      <td>-4.619919</td>\n",
       "      <td>-4.484973</td>\n",
       "      <td>-1.914490</td>\n",
       "      <td>-2.906957</td>\n",
       "      <td>-2.068287</td>\n",
       "      <td>-1.284289</td>\n",
       "      <td>-1.003664</td>\n",
       "      <td>-0.984132</td>\n",
       "      <td>-1.436534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.123083</td>\n",
       "      <td>-0.670759</td>\n",
       "      <td>-0.608343</td>\n",
       "      <td>-0.218386</td>\n",
       "      <td>-0.659006</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441605</td>\n",
       "      <td>-0.565480</td>\n",
       "      <td>-0.555664</td>\n",
       "      <td>-0.536283</td>\n",
       "      <td>-0.614956</td>\n",
       "      <td>-0.620502</td>\n",
       "      <td>-0.392323</td>\n",
       "      <td>-0.367909</td>\n",
       "      <td>-0.372977</td>\n",
       "      <td>-0.415055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.067344</td>\n",
       "      <td>-0.149352</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.598909</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>-0.740749</td>\n",
       "      <td>-0.871212</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.035178</td>\n",
       "      <td>-0.064500</td>\n",
       "      <td>-0.160140</td>\n",
       "      <td>-0.093269</td>\n",
       "      <td>-0.192747</td>\n",
       "      <td>-0.172751</td>\n",
       "      <td>-0.208970</td>\n",
       "      <td>-0.227463</td>\n",
       "      <td>-0.246499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.755564</td>\n",
       "      <td>0.399198</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>1.669702</td>\n",
       "      <td>-0.395219</td>\n",
       "      <td>1.349986</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>-0.355115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594184</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>0.535811</td>\n",
       "      <td>0.401904</td>\n",
       "      <td>0.548149</td>\n",
       "      <td>0.368681</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>-0.010296</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>0.016880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.890406</td>\n",
       "      <td>2.006952</td>\n",
       "      <td>5.436903</td>\n",
       "      <td>4.265924</td>\n",
       "      <td>3.424431</td>\n",
       "      <td>1.669702</td>\n",
       "      <td>2.530240</td>\n",
       "      <td>1.349986</td>\n",
       "      <td>1.147826</td>\n",
       "      <td>2.815988</td>\n",
       "      <td>...</td>\n",
       "      <td>5.640900</td>\n",
       "      <td>3.864370</td>\n",
       "      <td>6.566210</td>\n",
       "      <td>5.472978</td>\n",
       "      <td>5.183465</td>\n",
       "      <td>6.075505</td>\n",
       "      <td>6.972337</td>\n",
       "      <td>10.360470</td>\n",
       "      <td>9.834780</td>\n",
       "      <td>8.756040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender  admission_age  weight_admit       height  charlson_score  \\\n",
       "count  1148.000000    1148.000000   1148.000000  1148.000000     1148.000000   \n",
       "mean     -0.025135       0.004454     -0.014159    -0.051536       -0.037124   \n",
       "std       1.003042       0.969885      0.933715     0.968411        0.959947   \n",
       "min      -1.123083      -3.150412     -3.108538    -5.471435       -2.020151   \n",
       "25%      -1.123083      -0.670759     -0.608343    -0.218386       -0.659006   \n",
       "50%       0.890406       0.067344     -0.149352     0.037860        0.021567   \n",
       "75%       0.890406       0.755564      0.399198     0.037860        0.702140   \n",
       "max       0.890406       2.006952      5.436903     4.265924        3.424431   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer          chf          ckd  \\\n",
       "count          1148.000000       1148.000000  1148.000000  1148.000000   \n",
       "mean              0.003814         -0.041004    -0.003164     0.048611   \n",
       "std               1.002470          0.954761     0.999466     1.005965   \n",
       "min              -0.598909         -0.395219    -0.740749    -0.871212   \n",
       "25%              -0.598909         -0.395219    -0.740749    -0.871212   \n",
       "50%              -0.598909         -0.395219    -0.740749    -0.871212   \n",
       "75%               1.669702         -0.395219     1.349986     1.147826   \n",
       "max               1.669702          2.530240     1.349986     1.147826   \n",
       "\n",
       "               cld  ...          mch         mchc          mcv     platelet  \\\n",
       "count  1148.000000  ...  1148.000000  1148.000000  1148.000000  1148.000000   \n",
       "mean      0.017793  ...     0.015861     0.050695    -0.008711     0.008858   \n",
       "std       1.021949  ...     1.019373     1.063763     1.009521     0.955042   \n",
       "min      -0.355115  ...    -5.069597    -4.619919    -4.484973    -1.914490   \n",
       "25%      -0.355115  ...    -0.441605    -0.565480    -0.555664    -0.536283   \n",
       "50%      -0.355115  ...     0.087309     0.035178    -0.064500    -0.160140   \n",
       "75%      -0.355115  ...     0.594184     0.673376     0.535811     0.401904   \n",
       "max       2.815988  ...     5.640900     3.864370     6.566210     5.472978   \n",
       "\n",
       "               rbc          rdw          wbc          inr           pt  \\\n",
       "count  1148.000000  1148.000000  1148.000000  1148.000000  1148.000000   \n",
       "mean      0.000037    -0.016284    -0.019969     0.033902     0.030379   \n",
       "std       0.996068     0.993008     0.682750     1.032955     0.978090   \n",
       "min      -2.906957    -2.068287    -1.284289    -1.003664    -0.984132   \n",
       "25%      -0.614956    -0.620502    -0.392323    -0.367909    -0.372977   \n",
       "50%      -0.093269    -0.192747    -0.172751    -0.208970    -0.227463   \n",
       "75%       0.548149     0.368681     0.173255    -0.010296    -0.004343   \n",
       "max       5.183465     6.075505     6.972337    10.360470     9.834780   \n",
       "\n",
       "               ptt  \n",
       "count  1148.000000  \n",
       "mean      0.039213  \n",
       "std       1.013554  \n",
       "min      -1.436534  \n",
       "25%      -0.415055  \n",
       "50%      -0.246499  \n",
       "75%       0.016880  \n",
       "max       8.756040  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cat1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e66e1b12-7c4c-4d50-8ecd-dc1e2c86e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training RandomForestRegressor for Cat0\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_regressor_cat0 = RandomForestRegressor(n_estimators=100, max_depth=8, min_samples_split=10, random_state=26)\n",
    "\n",
    "rf_regressor_cat0.fit(X_train_cat0, y_train_cat0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a55cfe92-9a34-4648-8852-f0dd214d7b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=8, min_samples_split=10, random_state=26)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training RandomForestRegressor for Cat1\n",
    "\n",
    "rf_regressor_cat1 = RandomForestRegressor(n_estimators=100, max_depth=8, min_samples_split=10, random_state=26)\n",
    "\n",
    "rf_regressor_cat1.fit(X_train_cat1, y_train_cat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bca74d0c-6b4c-4768-8a73-78711a2cdb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>weight_admit</th>\n",
       "      <th>height</th>\n",
       "      <th>charlson_score</th>\n",
       "      <th>atrial_fibrillation</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>chf</th>\n",
       "      <th>ckd</th>\n",
       "      <th>cld</th>\n",
       "      <th>...</th>\n",
       "      <th>mch</th>\n",
       "      <th>mchc</th>\n",
       "      <th>mcv</th>\n",
       "      <th>platelet</th>\n",
       "      <th>rbc</th>\n",
       "      <th>rdw</th>\n",
       "      <th>wbc</th>\n",
       "      <th>inr</th>\n",
       "      <th>pt</th>\n",
       "      <th>ptt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "      <td>1329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.014066</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>-0.008097</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>-0.058628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>-0.013268</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>-0.014115</td>\n",
       "      <td>-0.017866</td>\n",
       "      <td>0.014924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999631</td>\n",
       "      <td>1.008558</td>\n",
       "      <td>0.789125</td>\n",
       "      <td>0.907918</td>\n",
       "      <td>1.020459</td>\n",
       "      <td>1.018478</td>\n",
       "      <td>0.991817</td>\n",
       "      <td>1.018034</td>\n",
       "      <td>1.004870</td>\n",
       "      <td>0.924653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989780</td>\n",
       "      <td>1.030059</td>\n",
       "      <td>0.982395</td>\n",
       "      <td>1.019990</td>\n",
       "      <td>0.971712</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>1.084689</td>\n",
       "      <td>1.124364</td>\n",
       "      <td>1.145289</td>\n",
       "      <td>1.058137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-3.117846</td>\n",
       "      <td>-2.746396</td>\n",
       "      <td>-5.550409</td>\n",
       "      <td>-1.946083</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.357416</td>\n",
       "      <td>-4.217825</td>\n",
       "      <td>-3.741144</td>\n",
       "      <td>-1.985078</td>\n",
       "      <td>-2.987631</td>\n",
       "      <td>-1.947009</td>\n",
       "      <td>-1.546576</td>\n",
       "      <td>-1.117834</td>\n",
       "      <td>-1.181794</td>\n",
       "      <td>-1.128259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.129591</td>\n",
       "      <td>-0.613069</td>\n",
       "      <td>-0.549268</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>-0.591937</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446956</td>\n",
       "      <td>-0.581999</td>\n",
       "      <td>-0.535620</td>\n",
       "      <td>-0.544374</td>\n",
       "      <td>-0.638809</td>\n",
       "      <td>-0.611290</td>\n",
       "      <td>-0.434428</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.421161</td>\n",
       "      <td>-0.428530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.100986</td>\n",
       "      <td>-0.114420</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.085136</td>\n",
       "      <td>-0.585797</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>-0.702016</td>\n",
       "      <td>-0.873819</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070916</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>-0.054792</td>\n",
       "      <td>-0.141115</td>\n",
       "      <td>-0.101609</td>\n",
       "      <td>-0.138395</td>\n",
       "      <td>-0.163284</td>\n",
       "      <td>-0.252415</td>\n",
       "      <td>-0.249589</td>\n",
       "      <td>-0.215454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>0.766109</td>\n",
       "      <td>0.362932</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.762208</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>-0.400511</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>-0.359073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546513</td>\n",
       "      <td>0.584321</td>\n",
       "      <td>0.506175</td>\n",
       "      <td>0.337108</td>\n",
       "      <td>0.561318</td>\n",
       "      <td>0.458945</td>\n",
       "      <td>0.204368</td>\n",
       "      <td>-0.138390</td>\n",
       "      <td>-0.129489</td>\n",
       "      <td>-0.065512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.885276</td>\n",
       "      <td>1.940691</td>\n",
       "      <td>4.544667</td>\n",
       "      <td>4.126044</td>\n",
       "      <td>4.486108</td>\n",
       "      <td>1.707075</td>\n",
       "      <td>2.496811</td>\n",
       "      <td>1.424469</td>\n",
       "      <td>1.144402</td>\n",
       "      <td>2.784953</td>\n",
       "      <td>...</td>\n",
       "      <td>4.478110</td>\n",
       "      <td>6.249306</td>\n",
       "      <td>4.539793</td>\n",
       "      <td>10.149745</td>\n",
       "      <td>3.878815</td>\n",
       "      <td>5.262558</td>\n",
       "      <td>27.406054</td>\n",
       "      <td>20.342228</td>\n",
       "      <td>22.946859</td>\n",
       "      <td>8.978364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender  admission_age  weight_admit       height  charlson_score  \\\n",
       "count  1329.000000    1329.000000   1329.000000  1329.000000     1329.000000   \n",
       "mean      0.005951       0.005059     -0.015768     0.001135        0.014066   \n",
       "std       0.999631       1.008558      0.789125     0.907918        1.020459   \n",
       "min      -1.129591      -3.117846     -2.746396    -5.550409       -1.946083   \n",
       "25%      -1.129591      -0.613069     -0.549268     0.020882       -0.591937   \n",
       "50%       0.885276       0.100986     -0.114420     0.020882        0.085136   \n",
       "75%       0.885276       0.766109      0.362932     0.020882        0.762208   \n",
       "max       0.885276       1.940691      4.544667     4.126044        4.486108   \n",
       "\n",
       "       atrial_fibrillation  malignant_cancer          chf          ckd  \\\n",
       "count          1329.000000       1329.000000  1329.000000  1329.000000   \n",
       "mean              0.033572         -0.008097     0.053214     0.038860   \n",
       "std               1.018478          0.991817     1.018034     1.004870   \n",
       "min              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "25%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "50%              -0.585797         -0.400511    -0.702016    -0.873819   \n",
       "75%               1.707075         -0.400511     1.424469     1.144402   \n",
       "max               1.707075          2.496811     1.424469     1.144402   \n",
       "\n",
       "               cld  ...          mch         mchc          mcv     platelet  \\\n",
       "count  1329.000000  ...  1329.000000  1329.000000  1329.000000  1329.000000   \n",
       "mean     -0.058628  ...     0.026565     0.001754     0.031395     0.001223   \n",
       "std       0.924653  ...     0.989780     1.030059     0.982395     1.019990   \n",
       "min      -0.359073  ...    -4.357416    -4.217825    -3.741144    -1.985078   \n",
       "25%      -0.359073  ...    -0.446956    -0.581999    -0.535620    -0.544374   \n",
       "50%      -0.359073  ...     0.070916     0.013062    -0.054792    -0.141115   \n",
       "75%      -0.359073  ...     0.546513     0.584321     0.506175     0.337108   \n",
       "max       2.784953  ...     4.478110     6.249306     4.539793    10.149745   \n",
       "\n",
       "               rbc          rdw          wbc          inr           pt  \\\n",
       "count  1329.000000  1329.000000  1329.000000  1329.000000  1329.000000   \n",
       "mean     -0.013268     0.011843     0.003802    -0.014115    -0.017866   \n",
       "std       0.971712     0.961903     1.084689     1.124364     1.145289   \n",
       "min      -2.987631    -1.947009    -1.546576    -1.117834    -1.181794   \n",
       "25%      -0.638809    -0.611290    -0.434428    -0.445380    -0.421161   \n",
       "50%      -0.101609    -0.138395    -0.163284    -0.252415    -0.249589   \n",
       "75%       0.561318     0.458945     0.204368    -0.138390    -0.129489   \n",
       "max       3.878815     5.262558    27.406054    20.342228    22.946859   \n",
       "\n",
       "               ptt  \n",
       "count  1329.000000  \n",
       "mean      0.014924  \n",
       "std       1.058137  \n",
       "min      -1.128259  \n",
       "25%      -0.428530  \n",
       "50%      -0.215454  \n",
       "75%      -0.065512  \n",
       "max       8.978364  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_holdout_pred_cat0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d810e892-4516-458a-83b4-e27b29d9826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF regressor Predicted Cat0 Holdout MSE: 26.928415720789392\n",
      "RF regressor Predicted Cat1 Holdout MSE: 21.008456688624808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_holdout_pred_cat0 = rf_regressor_cat0.predict(X_holdout_pred_cat0)\n",
    "mse_holdout_rfregressor_cat0 = mean_squared_error(y_holdout_reg_cat0, y_holdout_pred_cat0)\n",
    "print(\"RF regressor Predicted Cat0 Holdout MSE:\", mse_holdout_rfregressor_cat0)\n",
    "\n",
    "y_holdout_pred_cat1 = rf_regressor_cat1.predict(X_holdout_pred_cat1)\n",
    "mse_holdout_rfregressor_cat1 = mean_squared_error(y_holdout_reg_cat1, y_holdout_pred_cat1)\n",
    "print(\"RF regressor Predicted Cat1 Holdout MSE:\", mse_holdout_rfregressor_cat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "37f60beb-26ac-47bd-8ab0-63a5552b52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall combined Classification + Regression holdout data MSE: 26.400934792632164\n"
     ]
    }
   ],
   "source": [
    "Overall_holdout_MSE = (mse_holdout_rfregressor_cat0 * len(X_holdout_pred_cat0) + mse_holdout_rfregressor_cat1 * len(X_holdout_pred_cat1)) / (len(X_holdout_pred_cat0) + len(X_holdout_pred_cat1))\n",
    "\n",
    "print(\"Overall combined Classification + Regression holdout data MSE:\", Overall_holdout_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8fd2b-ab8c-4197-8ba7-59de29b2b5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
